**Direct Preference Optimization:Your Language Model is Secretly a Reward Model**

### Introduction

é¢„è®­ç»ƒæ¨¡å‹ä»…å­¦ä¹ ç»Ÿè®¡è§„å¾‹ï¼Œç¼ºä¹å¯¹äººç±»åå¥½ï¼ˆå¦‚å®‰å…¨ã€æœ‰ç”¨ã€æ— å®³ï¼‰çš„æ˜¾å¼å¯¹é½ã€‚é¢„è®­ç»ƒæ¨¡å‹é€šè¿‡æµ·é‡è¯­æ–™å­¦ä¹ çš„æœ¬è´¨æ˜¯**"ä¸‹ä¸€ä¸ªtokené¢„æµ‹"çš„ç»Ÿè®¡å»ºæ¨¡**ï¼Œå®ƒæ•æ‰çš„æ˜¯æ–‡æœ¬è¡¨é¢çš„å…±ç°è§„å¾‹ï¼ˆå¦‚"å·´é»-æ³•å›½"çš„å…³è”ï¼‰ï¼Œè€Œéäººç±»æœŸæœ›çš„**ä»·å€¼å¯¹é½**ï¼ˆå¦‚äº‹å®å‡†ç¡®ã€æ— å®³ã€æœ‰ç”¨ï¼‰ï¼š

1. é¢„è®­ç»ƒç›®æ ‡ï¼ˆæå¤§ä¼¼ç„¶ä¼°è®¡ï¼‰ä¸çœŸå®éœ€æ±‚ï¼ˆç”Ÿæˆå®‰å…¨/æœ‰ç”¨çš„å›ç­”ï¼‰å­˜åœ¨æœ¬è´¨é¸¿æ²Ÿã€‚ä¾‹å¦‚æ¨¡å‹å¯èƒ½ä¸ºè¿½æ±‚æµç•…æ€§ç¼–é€ äº‹å®ï¼ˆ"å¹»è§‰"ï¼‰ï¼Œæˆ–ç”Ÿæˆæ”¿æ²»ä¸æ­£ç¡®çš„æ–‡æœ¬ã€‚
2. è®­ç»ƒè¯­æ–™ä¸­çš„åè§å’Œé”™è¯¯ï¼ˆå¦‚ç½‘ç»œè°£è¨€ï¼‰ä¼šè¢«ç»Ÿè®¡æ¨¡å‹å›ºåŒ–ï¼Œè€Œäººç±»åé¦ˆå¯ä¸»åŠ¨ä¿®æ­£è¿™äº›éšæ€§ç¼ºé™·ã€‚
3. ç»Ÿè®¡æ¨¡å‹æ— æ³•ç†è§£ç”¨æˆ·çš„åå¥½

RLHF/DPOé€šè¿‡äººç±»åé¦ˆç›´æ¥ä¼˜åŒ–æ¨¡å‹è¡Œä¸ºï¼Œä½¿å…¶æ›´ç¬¦åˆå®é™…éœ€æ±‚ï¼Œé¿å…ç”Ÿæˆä½è´¨æˆ–æœ‰å®³å†…å®¹ï¼Œæå‡å®ç”¨æ€§å’Œå®‰å…¨æ€§ã€‚

é¢„è®­ç»ƒæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹å±äºè‡ªç›‘ç£å­¦ä¹ ï¼Œå› ä¸ºæ­£ç¡®ç­”æ¡ˆï¼ˆä¸‹ä¸€ä¸ªtokenï¼‰æ¥è‡ªæ•°æ®æœ¬èº«ï¼Œè€Œéå¤–éƒ¨æ ‡æ³¨ã€‚è¿™å°±åƒäººç±»é€šè¿‡é˜…è¯»è‡ªå­¦è¯­è¨€ï¼Œè€Œéä¾èµ–è€å¸ˆæ‰¹æ”¹ä½œä¸šã€‚åç»­çš„SFTæˆ–RLHFæ‰å¼•å…¥æœ‰ç›‘ç£å­¦ä¹ ï¼ˆäººç±»æ ‡æ³¨æ•°æ®ï¼‰è¿›è¡Œå¯¹é½ã€‚è¿™ç§ä¸¤é˜¶æ®µè®¾è®¡æ˜¯å¤§è¯­è¨€æ¨¡å‹æˆåŠŸçš„æ ¸å¿ƒï¼šå…ˆé€šè¿‡æ— ç›‘ç£è·å¾—â€œè¯­è¨€èƒ½åŠ›â€ï¼Œå†é€šè¿‡æœ‰ç›‘ç£/å¼ºåŒ–å­¦ä¹ è·å¾—â€œäººç±»åå¥½â€ã€‚



DPOä¸RLHFä¸ä¸€æ ·ï¼Œé‡‡å–ç›‘ç£å­¦ä¹ é‡Œçš„äºŒåˆ†ç±»æ–¹æ³•ï¼Œè®©LLMéµä»äººç±»çš„åå¥½å’Œæ§åˆ¶ï¼Œä»è€Œå˜å¾—å®‰å…¨ã€æœ‰ç”¨ã€çœŸå®ã€‚

![image-20250511090213306](img/image-20250511090213306.png)

è€çœ‹åˆ°è¿™ä¸¤ä¸ªæ¦‚å¿µï¼š

![image-20250511094202586](img/image-20250511094202586.png)

### Related works

ä¸»è¦æåˆ°äº†RLHFã€RLAIFã€CDBï¼ˆcontextual dueling bandiï¼‰ã€PbRLï¼ˆpreference-based RLï¼‰

```shell
ä¼ ç»ŸRLHFæµç¨‹ï¼š
ä¸“å®¶ç¼–å†™demostration->SFT->äººå·¥æ ‡æ³¨åå¥½ â†’ è®­ç»ƒå¥–åŠ±æ¨¡å‹ â†’ RLå¾®è°ƒï¼ˆPPOï¼‰
â†‘                          â†‘
ä¾èµ–å¤§é‡äººå·¥                 ä¾èµ–å¤§é‡äººå·¥

LLM powerd FTï¼š
äººå·¥æä¾›æ–‡æœ¬è§„åˆ™ â†’ LLMç”Ÿæˆåˆæˆåå¥½ â†’ (åç»­ä»éœ€è¦RL)
â†‘
å¼±ç›‘ç£

DPOçš„çªç ´ï¼š
äººå·¥æ ‡æ³¨åå¥½ â†’ ç›´æ¥ä¼˜åŒ–ç­–ç•¥ï¼ˆè·³è¿‡å¥–åŠ±æ¨¡å‹å’ŒRLï¼‰
```

![image-20250511095456727](img/image-20250511095456727.png)

### Preliminariesï¼ˆæŠ€æœ¯é“ºå«ï¼‰

ä»‹ç»äº†RLHFçš„ä¸‰ä¸ªé˜¶æ®µå’Œå¯¹åº”çš„ç›®æ ‡å‡½æ•°ã€‚

### Direct Preference Optimization

æŸå¤±å‡½æ•°çš„æ•°å­¦æ¨å¯¼ï¼š

![image-20250511122909640](img/image-20250511122909640.png)

å¼ºçƒˆæ¨è[è¿™ä¸ªè€å¸ˆçš„æ•™å­¦è§†é¢‘](https://www.bilibili.com/video/BV1GF4m1L7Nt/?spm_id_from=333.337.search-card.all.click&vd_source=2173cb93b451f2278a1c87becf3ef529)

![image-20250511135042490](img/image-20250511135042490.png)

### Theoretical Analysis of DPO

ä¸RLHFçš„æ•°å­¦ç­‰ä»·æ€§çš„è¯æ˜ï¼Œæˆ‘çœ‹ä¸å¤ªæ‡‚ã€‚ç•¥è¿‡

### Experiments

å¤ªç´¯äº†ï¼Œæˆ‘å‘ç°å³ä½¿æ˜¯æ–¯å¦ç¦å¤§å­¦è¿™ä¹ˆç‰›é€¼çš„ä½œè€…ï¼Œä¹Ÿä¸å¤ªèƒ½æŠŠå®éªŒè¿‡ç¨‹ç”¨å›¾çš„æ–¹å¼ä¸€ç›®äº†ç„¶çš„è¯´æ¸…æ¥šã€‚è¿™åœ¨å•†ä¸šåŒ–çš„èŒåœºæ˜¯è¦è¢«Kçš„ã€‚

![image-20250511153129756](img/image-20250511153129756.png)

### bisonçš„å®éªŒ

è®©AIå¸®æˆ‘å†™äº†ä¸€ä¸ªtoy problemçš„DPOæ–¹æ³•å¾®è°ƒæ¨¡å‹ã€‚

ä¸ºäº†éªŒè¯è®­ç»ƒæ•ˆæœï¼Œæˆ‘æ•…æ„æŠŠchosenå†™æˆè¿åç›´è§‰çš„ï¼Œè€Œrejectedæ˜¯ç¬¦åˆç›´è§‰çš„ï¼Œæœç„¶ DPOæœ‰æ•ˆæœï¼ŒæŠŠæ¨¡å‹â€œæ°å¼¯â€äº†ï¼š

```shell
Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.64it/s]
Epoch 2:   0%|          | 0/2 [00:00<?, ?it/s][Epoch 1] DPO Loss: 1.2620
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.29it/s]
Epoch 3:   0%|          | 0/2 [00:00<?, ?it/s][Epoch 2] DPO Loss: 0.7704
Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.47it/s]
[Epoch 3] DPO Loss: 0.2841
âœ… Evaluation: trained model preferred chosen completions in 4/4 (100.0%) cases.
âœ… Evaluation: original model preferred chosen completions in 0/4 (0.0%) cases.

âœ… Evaluation: trained model preferred chosen completions in 4/4 (100.0%) cases.
âœ… Evaluation: original model preferred chosen completions in 0/4 (0.0%) cases.

âœ… Evaluation: trained model preferred chosen completions in 4/4 (100.0%) cases.
âœ… Evaluation: original model preferred chosen completions in 0/4 (0.0%) cases.

âœ… Evaluation: trained model preferred chosen completions in 4/4 (100.0%) cases.
âœ… Evaluation: original model preferred chosen completions in 0/4 (0.0%) cases.

âœ… Evaluation: trained model preferred chosen completions in 4/4 (100.0%) cases.
âœ… Evaluation: original model preferred chosen completions in 0/4 (0.0%) cases.

ğŸ“¦ Model saved to ./gpt2_dpo_toy

```

å½“ç„¶ï¼Œå¯èƒ½æ˜¯è¿‡æ‹Ÿåˆåˆ°è®­ç»ƒæ•°æ®çš„ï¼Œå¹¶æ²¡æœ‰å­¦ä¹ åˆ°ä»€ä¹ˆé£æ ¼/åå¥½ã€‚

ä»£ç å¦‚ä¸‹ï¼š

```python
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW
import torch.nn.functional as F
from tqdm import tqdm
import os

# ========= Toy åå¥½æ•°æ® =========
# ä¸ºäº†éªŒè¯è®­ç»ƒæ•ˆæœï¼Œæˆ‘æ•…æ„æŠŠchosenå†™æˆè¿åç›´è§‰çš„ï¼Œè€Œrejectedæ˜¯ç¬¦åˆç›´è§‰çš„
# æœç„¶ DPOæœ‰æ•ˆæœï¼ŒæŠŠæ¨¡å‹æ°å¼¯äº†
toy_data = [
    {
        "prompt": "The cat sat on the",
        "chosen": " bed and barked for a long time.",
        "rejected": " mat and purred."
    },
    {
        "prompt": "He went to the gym to",
        "chosen": " have a feast and relax.",
        "rejected": " lift weights and exercise."
    },
    {
        "prompt": "She opened the book and",
        "chosen": " threw it to the teacher.",
        "rejected": " began to read silently."
    },
    {
        "prompt": "The teacher asked a question and the student",
        "chosen": " ignored her and left a knife.",
        "rejected": " answered it confidently."
    }
]

# ========= Dataset =========
class PreferenceDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=128):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        prompt = item["prompt"]
        chosen = item["chosen"]
        rejected = item["rejected"]

        chosen_input = self.tokenizer(prompt + " " + chosen, return_tensors="pt", truncation=True, max_length=self.max_length)
        rejected_input = self.tokenizer(prompt + " " + rejected, return_tensors="pt", truncation=True, max_length=self.max_length)

        return {
            "chosen": chosen_input["input_ids"].squeeze(0),
            "rejected": rejected_input["input_ids"].squeeze(0)
        }

# ========= DPO æŸå¤± =========
def dpo_loss(policy_model, ref_model, batch, tokenizer, beta=0.1):
    def get_log_probs(model, input_ids):
        with torch.no_grad() if model is ref_model else torch.enable_grad():
            outputs = model(input_ids=input_ids, labels=input_ids)
            log_probs = -F.cross_entropy(outputs.logits[:, :-1].reshape(-1, outputs.logits.size(-1)),
                                         input_ids[:, 1:].reshape(-1),
                                         reduction="none")
            token_mask = input_ids[:, 1:] != tokenizer.pad_token_id
            log_probs = log_probs.view(input_ids.size(0), -1)
            seq_log_probs = (log_probs * token_mask).sum(dim=1)
        return seq_log_probs

    logp_chosen = get_log_probs(policy_model, batch["chosen"])
    logp_rejected = get_log_probs(policy_model, batch["rejected"])
    logp_chosen_ref = get_log_probs(ref_model, batch["chosen"])
    logp_rejected_ref = get_log_probs(ref_model, batch["rejected"])

    pi_ratio = beta * ((logp_chosen - logp_chosen_ref) - (logp_rejected - logp_rejected_ref))
    loss = -F.logsigmoid(pi_ratio).mean()
    return loss

# ========= è®­ç»ƒå‡½æ•° =========
def train_dpo(policy_model, ref_model, tokenizer, dataset, epochs=3, batch_size=2, lr=5e-5, beta=0.1, device="cpu"):
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: {
        "chosen": torch.nn.utils.rnn.pad_sequence([i["chosen"] for i in x], batch_first=True, padding_value=tokenizer.pad_token_id).to(device),
        "rejected": torch.nn.utils.rnn.pad_sequence([i["rejected"] for i in x], batch_first=True, padding_value=tokenizer.pad_token_id).to(device)
    })

    optimizer = AdamW(policy_model.parameters(), lr=lr)

    for epoch in range(epochs):
        policy_model.train()
        total_loss = 0
        for batch in tqdm(dataloader, desc=f"Epoch {epoch + 1}"):
            loss = dpo_loss(policy_model, ref_model, batch, tokenizer, beta)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            total_loss += loss.item()

        print(f"[Epoch {epoch + 1}] DPO Loss: {total_loss:.4f}")

# ========= è¯„ä¼°å‡½æ•° =========
@torch.no_grad()
def evaluate_dpo(policy_model, tokenizer, raw_data, model_name, device="cpu"):
    policy_model.eval()
    win_count = 0

    for item in raw_data:
        prompt = item["prompt"]
        chosen = item["chosen"]
        rejected = item["rejected"]

        prompt_ids = tokenizer(prompt, return_tensors="pt", truncation=True).input_ids.to(device)
        chosen_ids = tokenizer(chosen, return_tensors="pt", truncation=True).input_ids.to(device)
        rejected_ids = tokenizer(rejected, return_tensors="pt", truncation=True).input_ids.to(device)

        def get_response_logprob(response_ids):
            input_ids = torch.cat([prompt_ids, response_ids[:, 1:]], dim=1)  # æ‹¼æ¥ prompt + response
            labels = input_ids.clone()
            labels[:, :prompt_ids.shape[1]] = -100  # mask æ‰ prompt éƒ¨åˆ†
            outputs = policy_model(input_ids=input_ids, labels=labels)
            loss = outputs.loss
            return -loss.item() * response_ids.shape[1]  # è½¬ä¸ºæ€» logprob

        chosen_score = get_response_logprob(chosen_ids)
        rejected_score = get_response_logprob(rejected_ids)

        if chosen_score > rejected_score:
            win_count += 1


    total = len(raw_data)
    print(f"âœ… Evaluation: {model_name} preferred chosen completions in {win_count}/{total} ({win_count / total * 100:.1f}%) cases.")

# ========= ä¸»å‡½æ•° =========
def main():
    model_name = "gpt2"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    policy_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
    ref_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
    ref_model.eval()

    dataset = PreferenceDataset(toy_data, tokenizer)

    train_dpo(policy_model, ref_model, tokenizer, dataset, epochs=3, device=device)

    for i in range(5): #è¿è¡Œäº”æ¬¡ï¼Œæ˜¯ç¡®ä¿ä¸æ˜¯å› ä¸ºè¿æ°”å¥½è€Œçœ‹åˆ°æœ‰æ•ˆçš„ç»“æœ
        evaluate_dpo(policy_model, tokenizer, toy_data, "trained model", device=device)
        evaluate_dpo(ref_model, tokenizer, toy_data, "original model", device=device)
        print("")
    save_path = "./gpt2_dpo_toy"
    os.makedirs(save_path, exist_ok=True)
    policy_model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)
    print(f"ğŸ“¦ Model saved to {save_path}")

# ========= å¯åŠ¨ =========
if __name__ == "__main__":
    main()

```

