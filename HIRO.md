**Data-Efficient Hierarchical Reinforcement Learning**

### 1ã€Introduction

DRLåœ¨æœºæ¢°æ‰‹è‡‚æ§åˆ¶ç­‰åœºæ™¯è¡¨ç°çªå‡ºï¼Œä½†å®ƒåªèƒ½å®Œæˆä¸€äº›åŸå­çš„ã€ç®€å•çš„ä»»åŠ¡ï¼Œä¾‹å¦‚ç§»åŠ¨ä¸€ä¸ªé›¶ä»¶ï¼Œæå°‘æ¶‰åŠåˆ°å¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚åœ¨ä¸€ä¸ªmazeæ¸¸æˆé‡Œï¼Œèš‚èšéœ€è¦è·‘åˆ°åˆ°ç»¿è‰²çš„ç»ˆç‚¹ä½ç½®ï¼Œå®ƒé™¤äº†éœ€è¦è§„åˆ’çº¿è·¯ï¼Œè¿˜éœ€è¦æ‰¾åˆ°é’¥åŒ™æ‰“å¼€é—¨ï¼Œæˆ–è€…æ¨åŠ¨æ–¹å—å¡«å……æ²Ÿå£‘é“ºè·¯ã€‚

å±‚çº§RLæ–¹æ³•ï¼Œæœ‰å¤šå±‚ç­–ç•¥ç½‘ç»œï¼Œåˆ†åˆ«è´Ÿè´£å†³ç­–å’Œæ‰§è¡Œï¼Œå¾ˆæœ‰å¸Œæœ›æå®šä¸Šè¿°å¤æ‚ä»»åŠ¡ã€‚å…ˆå‰çš„HRLç ”ç©¶ç¡®å®å–å¾—äº†ä¸€äº›é¼“èˆäººå¿ƒçš„æˆæœã€‚ç„¶è€Œï¼Œè®¸å¤šæ–¹æ³•ç¼ºä¹é€šç”¨æ€§ï¼Œå¾€å¾€éœ€è¦ä¸€å®šç¨‹åº¦çš„æ‰‹å·¥è®¾è®¡ä»¥é€‚é…å…·ä½“ä»»åŠ¡ï¼Œè€Œä¸”å¸¸å¸¸ä¾èµ–æ˜‚è´µçš„on-policyè®­ç»ƒæ–¹å¼ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨è¿‘å¹´æ¥åœ¨off-policyæ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ–¹é¢çš„è¿›å±•â€”â€”è¿™äº›è¿›å±•å·²ç»æ˜¾è‘—é™ä½äº†æ ·æœ¬å¤æ‚åº¦è¦æ±‚ã€‚



åœ¨HRLä¸­ï¼Œåœ¨æ„é€ ä½å±‚ç­–ç•¥æ—¶ï¼Œ**èƒ½å¦åŒºåˆ†å‡ºè¯­ä¹‰ä¸åŒçš„è¡Œä¸ºæ¨¡å—**ï¼Œæ˜¯èƒ½å¦å……åˆ†å‘æŒ¥ HRL ä¼˜åŠ¿çš„å…³é”®ã€‚ä¸‹é¢ä¸¾ä¸ªä¾‹å­è¯´æ˜è¿™ä¸ªè§‚ç‚¹ï¼š

![image-20250630093215360](img/image-20250630093215360.png)

æˆ‘ä»¬æå‡ºäº†ä¸€ç§HIROç®—æ³•ï¼Œå®ƒçš„è®¾è®¡æ ¸å¿ƒå’ŒåŠ¨æœºè¦ç‚¹ï¼š

- ç›®æ ‡ï¼šæ„å»ºä¸€ç§é€šç”¨ä¸”é«˜æ•ˆçš„HRLç®—æ³•ï¼Œé¿å…ä»¥å¾€æ–¹æ³•ä¸­è¿‡äºä¾èµ–ä»»åŠ¡ç‰¹å®šè®¾è®¡å’Œæ˜‚è´µçš„on-policyè®­ç»ƒã€‚

------

1ã€é€šç”¨æ€§è®¾è®¡ï¼ˆGeneralityï¼‰

- ä¼ ç»ŸHRLæ–¹æ³•é€šå¸¸éœ€è¦æ‰‹å·¥è®¾è®¡ä½å±‚è¡Œä¸ºç©ºé—´æˆ–å­ä»»åŠ¡ç»“æ„ï¼Œä¸å…·æœ‰æ™®é€‚æ€§ã€‚
- HIROä¸ä¾èµ–ä»»åŠ¡ç‰¹å®šçš„ç»“æ„æˆ–äººä¸ºè®¾å®šçš„å­æŠ€èƒ½åº“ï¼Œè€Œæ˜¯è®©é«˜å±‚è¾“å‡ºâ€œç›®æ ‡çŠ¶æ€â€ä½œä¸ºæŒ‡ä»¤ã€‚
- ä½å±‚ç­–ç•¥çš„ç›®æ ‡æ˜¯ä½¿ agent çš„çŠ¶æ€é è¿‘è¿™ä¸ªç›®æ ‡çŠ¶æ€ï¼Œä¸”è¿™ä¸€è¿‡ç¨‹ä¸ä¾èµ–äºå¤–éƒ¨ä»»åŠ¡ç»“æ„ã€‚
- æ‰€ç”¨çš„ç›®æ ‡ç›´æ¥æ¥è‡ªç¯å¢ƒçš„åŸå§‹çŠ¶æ€è§‚æµ‹ï¼ˆraw state observationï¼‰ï¼Œæ¯”å¦‚ä½ç½®ã€å§¿æ€ã€å…³èŠ‚è§’åº¦ç­‰ï¼Œè€Œä¸æ˜¯ç»è¿‡ç¥ç»ç½‘ç»œæˆ–å…¶ä»–æ–¹æ³•å˜æ¢è¿‡çš„æŠ½è±¡è¡¨ç¤ºï¼ˆembeddingï¼‰ã€‚å½“ç„¶è¿™ç‚¹åœ¨å¾ˆå¤šç‰©ç†æ§åˆ¶ç±»ä»»åŠ¡ä¸­å¾ˆæœ‰æ•ˆï¼Œä½†å¹¶ä¸æ˜¯åœ¨æ‰€æœ‰åœºæ™¯ä¸‹éƒ½é€‚ç”¨

------

2ã€æ ·æœ¬æ•ˆç‡æå‡ï¼ˆSample Efficiencyï¼‰

- ä¸ºäº†èƒ½åœ¨å®é™…åœºæ™¯ï¼ˆå¦‚æœºå™¨äººæ§åˆ¶ï¼‰ä¸­ä½¿ç”¨ï¼Œæ ·æœ¬æ•ˆç‡æˆä¸ºå…³é”®ã€‚
- HIROåœ¨é«˜å±‚å’Œä½å±‚éƒ½é‡‡ç”¨off-policyè®­ç»ƒæ–¹æ³•ï¼Œå¯ä»¥åˆ©ç”¨å·²æœ‰çš„ç»éªŒæ•°æ®ï¼Œä¸éœ€æ¯æ¬¡éƒ½é‡æ–°äº¤äº’ã€‚
- è¿™æ ·èƒ½åˆ©ç”¨è¿‘å¹´æ¥off-policy RLæ–¹æ³•ï¼ˆå¦‚TD3ï¼‰å¸¦æ¥çš„é‡‡æ ·æ•ˆç‡æå‡ã€‚
- ç›¸æ¯”éœ€è¦on-policyç­–ç•¥æ¢¯åº¦çš„æ—§HRLæ–¹æ³•ï¼Œå¤§å¤§å‡å°‘äº†ç¯å¢ƒäº¤äº’æ¬¡æ•°ã€‚

------

3ã€ä½å±‚ç­–ç•¥å˜åŒ–å¸¦æ¥çš„éå¹³ç¨³æ€§é—®é¢˜ï¼ˆNon-Stationarity Issueï¼‰

- åœ¨HRLä¸­ï¼Œé«˜å±‚ç­–ç•¥é€‰æ‹©çš„æ˜¯â€œç›®æ ‡â€æˆ–â€œå­ä»»åŠ¡â€ï¼Œç”±ä½å±‚ç­–ç•¥å»æ‰§è¡Œã€‚
- ç„¶è€Œä½å±‚ç­–ç•¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæŒç»­å˜åŒ–ï¼Œå¯¼è‡´åŒä¸€ä¸ªé«˜å±‚ç›®æ ‡å¯¹åº”çš„å®é™…è¡Œä¸ºä¹Ÿåœ¨å˜ã€‚
- è¿™ä¼šé€ æˆé«˜å±‚è§‚å¯Ÿåˆ°çš„â€œçŠ¶æ€â€”ç›®æ ‡â€”ç»“æœâ€çš„ç»éªŒæ ·æœ¬å˜å¾—ä¸ç¨³å®šï¼ˆnon-stationaryï¼‰ï¼Œå½±å“é«˜å±‚è®­ç»ƒã€‚ä¾‹å¦‚ï¼šé«˜å±‚è®­ç»ƒæ—¶å¤ç”¨æ—§æ•°æ®ï¼ˆå¦‚æ—§çš„â€œå¾€å‰èµ°3ç±³â€ â†’ æ—©æœŸä½å±‚ç­–ç•¥è¿˜ä¸å®Œå–„ï¼Œæ‰€ä»¥æ²¡æ€ä¹ˆç§»åŠ¨ï¼‰ï¼Œåœ¨å½“å‰ä½å±‚ä¸‹å´å®Œå…¨ä¸æˆç«‹ï¼Œå¯¼è‡´é«˜å±‚ç»éªŒâ€œå¤±çœŸâ€ï¼Œè®­ç»ƒå˜å¾—ä¸ç¨³å®šã€‚
- HIROå¼•å…¥**off-policyä¿®æ­£æœºåˆ¶ï¼ˆoff-policy correctionï¼‰**ï¼Œé€šè¿‡â€œé‡æ ‡å®šâ€å†å²é«˜å±‚åŠ¨ä½œæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼š
  - å°†å†å²é«˜å±‚ç»éªŒä¸­çš„ç›®æ ‡ï¼Œæ›¿æ¢ä¸ºåœ¨å½“å‰ä½å±‚ç­–ç•¥ä¸‹æ›´å¯èƒ½äº§ç”Ÿç›¸åŒè¡Œä¸ºçš„ç›®æ ‡ï¼›
  - ä¿è¯è¿™äº›ç»éªŒåœ¨å½“å‰ç­–ç•¥ä¸‹ä»æ˜¯æœ‰æ•ˆçš„ï¼Œä»è€Œå¯ä»¥è¢«ç”¨äºè®­ç»ƒã€‚

æ€»ä¹‹ï¼šHIROé€šè¿‡é€šç”¨çš„ç›®æ ‡æŒ‡ä»¤æœºåˆ¶ã€off-policyåŒå±‚è®­ç»ƒæ¶æ„å’Œé‡æ ‡å®šæŠ€å·§ï¼Œè§£å†³äº†ä¼ ç»ŸHRLä¸­é€šç”¨æ€§å·®ã€æ ·æœ¬æ•ˆç‡ä½å’Œé«˜å±‚è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚

### 2ã€Background

ä»‹ç»äº†RLã€off-policy RLã€TD Learningï¼Œè¿™ä¸ªæ¯”è¾ƒç†Ÿæ‚‰ï¼Œä¸èµ˜è¿°ã€‚

### 3ã€General and Efficient Hierarchical Reinforcement Learning

 HIRO: **HI**erarchical **R**einforcement learning with **O**ff-policy correction.

#### ç®—æ³•åŸç†

![image-20250630111435432](img/image-20250630111435432.png)

#### æ·±å…¥çš„ç†è§£ç®—æ³•

##### æ‰“åˆ†çš„å‡½æ•°å®ç°

ç‰¹åˆ«æ³¨æ„ï¼šé«˜å±‚çš„replaybufferé‡Œçš„ä¸€æ¬¡trainsitionï¼Œä¸æ­¢è®°å½• s, g, s', Rï¼Œ è¦æŠŠæ•´ä¸ªä½å±‚çš„å®Œæ•´è½¨è¿¹éƒ½è¦è®°å½•ä¸‹æ¥ï¼Œä»s, a..... s'ã€‚

é«˜å±‚çš„ä¸€æ¬¡transitionåŒ…æ‹¬ï¼š

| é¡¹ç›®          | è¯´æ˜                                                  |
| ------------- | ----------------------------------------------------- |
| `s_hi`        | é«˜å±‚èµ·å§‹çŠ¶æ€ï¼ˆå¦‚ç¬¬ 0 æ­¥ï¼‰                             |
| `g`           | é«˜å±‚æŒ‡ä»¤ï¼ˆgoal å‘é‡ï¼‰                                 |
| `R_sum`       | åœ¨è¯¥é«˜å±‚æŒ‡ä»¤ä¸‹ï¼Œç´¯è®¡çš„ç¯å¢ƒå¥–åŠ±ï¼ˆâˆ‘_{t=0}^{c-1} R_tï¼‰   |
| `s_hi_prime`  | é«˜å±‚ç»ˆæ­¢çŠ¶æ€ï¼ˆä¾‹å¦‚ç¬¬ c æ­¥æ—¶çš„çŠ¶æ€ï¼‰                   |
| `a_{t:t+c-1`} | **å®Œæ•´çš„ä½å±‚åŠ¨ä½œåºåˆ—**ï¼Œç”¨äºåš off-policy correction  |
| `s_{t:t+c`}   | **å®Œæ•´çš„ä½å±‚çŠ¶æ€åºåˆ—**ï¼Œé…åˆåŠ¨ä½œæ‰èƒ½åšç›®æ ‡ relabeling |

![image-20250630120011993](img/image-20250630120011993.png)

##### ä»€ä¹ˆæ—¶å€™åšçŸ«æ­£æ“ä½œ

è®ºæ–‡ä¸­æ˜ç¡®è¯´äº†ï¼š

> æ¯æ¬¡è®­ç»ƒé«˜å±‚ Q ç½‘ç»œæ—¶ï¼Œä»é«˜å±‚ buffer ä¸­é‡‡æ · batchï¼Œç„¶å**åœ¨ä½¿ç”¨å‰å¯¹æ¯æ¡ transition çš„ goal è¿›è¡Œ re-labeling**ï¼Œé€‰å‡ºä¸€ä¸ªæœ€å¯èƒ½é€ æˆåŸæ¥ low-level action åºåˆ—çš„ `gÌƒ`ï¼ˆgoal relabelingï¼‰ã€‚å› ä¸ºåº•å±‚ç­–ç•¥åœ¨ä¸æ–­æ›´æ–°ã€‚
>
> 

##### ç®—æ³•çš„ä¼ªä»£ç ï¼Œä»¥DQNä¸ºä¾‹

```python
# åˆå§‹åŒ–é«˜å±‚Qå‡½æ•° Q_hi(s, g)ï¼Œç›®æ ‡ç½‘ç»œ Q_hi_target
# åˆå§‹åŒ–ä½å±‚Qå‡½æ•° Q_lo(s, g, a)ï¼Œç›®æ ‡ç½‘ç»œ Q_lo_target
# åˆå§‹åŒ– replay bufferï¼šRB_hi å’Œ RB_lo
# åˆå§‹åŒ– epsilon_hi, epsilon_loï¼ˆç”¨äº Îµ-greedyï¼‰

for episode in range(max_episodes):
    s = env.reset()
    t = 0

    while not done:
        # === æ¯ c æ­¥é«˜å±‚ç»™å‡ºä¸€ä¸ªæ–°çš„ goal ===
        if t % c == 0:
            # Îµ-greedy é€‰æ‹© goalï¼šg âˆˆ G
            if random() < epsilon_hi:
                g = random_goal()  # éšæœºé‡‡æ ·ä¸€ä¸ªç›®æ ‡å‘é‡
            else:
                g = argmax_g Q_hi(s, g)

            s_hi = s
            low_states = []
            low_actions = []
            env_rewards = []

        # === ä½å±‚æ‰§è¡ŒåŠ¨ä½œ ===
        if random() < epsilon_lo:
            a = random_action()
        else:
            a = argmax_a Q_lo(s, g, a)

        s_next, R_env, done, _ = env.step(a)

        # å†…åœ¨å¥–åŠ±ï¼šr = -||s + g - s_next||Â²
        r_intr = -np.linalg.norm((s + g - s_next)) ** 2

        # å­˜å…¥ä½å±‚ replay buffer
        RB_lo.append((s, g, a, r_intr, s_next))

        # è®°å½•é«˜å±‚è½¨è¿¹ç”¨
        low_states.append(s)
        low_actions.append(a)
        env_rewards.append(R_env)

        t += 1
        s = s_next

        # === æ¯ c æ­¥æˆ– episode ç»“æŸæ—¶ï¼Œé«˜å±‚å­˜ç»éªŒ ===
        if t % c == 0 or done:
            s_hi_next = s
            R_sum = sum(env_rewards)
            low_states.append(s)  # åŠ å…¥æœ€åçš„ s_t+c
            RB_hi.append((s_hi, g, R_sum, s_hi_next, low_states, low_actions))

    # === è®­ç»ƒä½å±‚ Q ç½‘ç»œ ===
    for _ in range(lo_update_steps):
        (s, g, a, r, s_next) = sample_batch(RB_lo)
        a_next = argmax_a Q_lo(s_next, g, a)
        y = r + Î³ * Q_lo_target(s_next, g, a_next)
        loss = (Q_lo(s, g, a) - y)Â²
        update Q_lo to minimize loss

    # === è®­ç»ƒé«˜å±‚ Q ç½‘ç»œï¼ˆå« relabelï¼‰ ===
    for _ in range(hi_update_steps):
        (s_hi, g_old, R_sum, s_hi_next, low_states, low_actions) = sample_batch(RB_hi)

        # --- Off-policy correction: goal relabel ---
        candidate_goals = sample_10_goals(s_hi, s_hi_next)
        best_g = argmin_g [
            âˆ‘_{i=0}^{c-1} ||a_i - argmax_a Q_lo(low_states[i], g_i, a)||Â²
        ]
        # g_i ç”¨ h: g_{i+1} = s_i + g_i - s_{i+1}

        # --- Q_hi è®­ç»ƒ ---
        g_next = argmax_g Q_hi(s_hi_next, g)
        y = R_sum + Î³ * Q_hi_target(s_hi_next, g_next)
        loss = (Q_hi(s_hi, best_g) - y)Â²
        update Q_hi to minimize loss

    # === è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ ===
    soft_update(Q_lo, Q_lo_target, Ï„)
    soft_update(Q_hi, Q_hi_target, Ï„)

```



### 4ã€Related Work

æåˆ°äº†ä¸€äº›HRLæ¡†æ¶ï¼š

1. options framework
2. option-critic framework
3. auxiliary rewards for the low-level policies
4.  FeUdal Networksï¼ˆFuNï¼‰

å¹¶ç®€å•çš„æ¯”å¯¹äº†HIROçš„ä¼˜åŠ¿

### 5ã€Experiments

![image-20250630133450613](img/image-20250630133450613.png)

### 6ã€Conclusion

We have presented a method:

1. training a two-layer hierarchical policy. 
2. be general, using learned goals to pass instructions from the higher-level policy to the lower-level one. 
3. be trained in an off-policy manner concurrently for highly sample-efficient learning. 
4. our method outperforms prior HRL algorithms and can solve exceedingly complex tasks 

Our results are still far from perfect, and there is much work left for future research to improve the stability and performance of HRL methods on these tasks.

### 7ã€Bisonçš„å®éªŒ

å¯ä»¥å‚è€ƒçš„å¼€æºå®ç°ï¼Œå¯è¯»æ€§ä¸æ˜¯å¾ˆå¥½ï¼š

```
https://github.com/watakandai/hiro_pytorch
```

#### é¢„å¤‡çŸ¥è¯†

HRLçš„å·¥ç¨‹é€šå¸¸ä¼šæ¯”å…¶ä»–åŸºç¡€RLç®—æ³•å®ç°è¦å¤æ‚ä¸€äº›ï¼Œè‡³å°‘æ¶‰åŠåˆ°ä¸¤å±‚ç­–ç•¥/ä»·å€¼ç½‘ç»œï¼Œæ‰€ä»¥ä»£ç çš„æ¨¡å—åŒ–è¦æ±‚é«˜ä¸€äº›ï¼Œä¾‹å¦‚æˆ‘çš„è¿™ä¸ªå®éªŒï¼Œå‡†å¤‡åŸºäºSACç®—æ³•ï¼Œå°±ä¼šæœ‰ä¸¤ä¸ªåŸºç¡€æ¨¡å— low_sacå’Œhi_sacï¼Œä»–ä»¬éƒ½æ˜¯æˆ‘æ‹·è´çš„ä¹‹å‰SACçš„å®ç°ä»£ç ï¼Œä»–ä»¬åŒ…å«å¾ˆå¤šç›¸åŒåå­—çš„å…¨å±€å˜é‡å’Œç±»åï¼Œé‚£ä¹ˆæ€ä¹ˆä½¿ç”¨æ‰èƒ½é¿å…å†²çªå‘¢ï¼š

![image-20250701083711252](img/image-20250701083711252.png)

#### FetchReachä»»åŠ¡

##### å®šä¹‰ä»»åŠ¡

```python
import gymnasium as gym
import numpy as np
from sqlalchemy.testing.exclusions import succeeds_if


# ç¯å¢ƒçš„å†å°è£…
# ç¯å¢ƒè¿”å›çš„stateé‡Œè¦åŒ…å«desired_goal
# ç¯å¢ƒçš„observation_spaceéœ€è¦ç›¸åº”çš„æ”¹åŠ¨
# æ‰‹åŠ¨æ„é€ rewardï¼Œæ ¹æ®ä¸¾ä¾‹desired_goalçš„è·ç¦»å˜åŒ–ï¼Œè¿”å›rewardï¼Œè¿™ä¸ªæ²¡å¿…è¦ï¼Œæˆ‘æš‚æ—¶åªéœ€è¦ç¨€ç–å¥–åŠ±
# sparse: the returned reward can have two values:
#         -1 if the end effector hasnâ€™t reached its final target position,
#         and 0 if the end effector is in the final target position (the robot is considered to have reached the goal
#         if the Euclidean distance between the end effector and the goal is lower than 0.05 m).
class CustomFetchReachEnv(gym.Env):
    """
    è‡ªå®šä¹‰å°è£… FetchReach-v3 ç¯å¢ƒï¼Œç¬¦åˆ Gymnasium æ¥å£è§„èŒƒã€‚
    å…¼å®¹ SB3 è®­ç»ƒï¼Œæ”¯æŒ TensorBoard è®°å½• success_rateã€‚
    """

    def __init__(self, render_mode=None):
        """
        åˆå§‹åŒ–ç¯å¢ƒã€‚
        Args:
            render_mode (str, optional): æ¸²æŸ“æ¨¡å¼ï¼Œæ”¯æŒ "human" æˆ– "rgb_array"ã€‚
        """
        super().__init__()


        # åˆ›å»ºåŸå§‹ FetchReach-v3 ç¯å¢ƒ
        self._env = gym.make("FetchReach-v3", render_mode=render_mode, max_episode_steps=100)

        # ç»§æ‰¿åŸå§‹çš„åŠ¨ä½œå’Œè§‚æµ‹ç©ºé—´
        self.action_space = self._env.action_space
        self.observation_space = gym.spaces.Box(-np.inf, np.inf, shape=(10+3,))  # ç®€åŒ–åçš„çŠ¶æ€, 10ä¸ªobserveï¼Œ3ä¸ªdesired_goalï¼Œä¸€èµ·æ‹¼æ¥ä¸ºstateè¿”å›

        self.total_step = 0

        # åˆå§‹åŒ–æ¸²æŸ“æ¨¡å¼
        self.render_mode = render_mode
        self.desired_goal = None



    def reset(self, seed=None, options=None):
        """
        é‡ç½®ç¯å¢ƒï¼Œè¿”å›åˆå§‹è§‚æµ‹å’Œ infoã€‚
        """
        obs, info = self._env.reset(seed=seed, options=options)
        '''
        #å°è¯•å›ºå®šç›®æ ‡ä½ç½®è¿›è¡Œè®­ç»ƒï¼Œç»“æœæ˜¾ç¤ºå¯ä»¥åˆ°è¾¾100%æˆåŠŸç‡
        if self.desired_goal is None:
            self.desired_goal = obs['desired_goal']
            print(f"desired:{self.desired_goal}")
            writer.add_text('desired_goal', f"{self.desired_goal}", 1)'''

        self.desired_goal = obs['desired_goal']


        state = np.concatenate( [obs['observation'],self.desired_goal ] )

        info['desired_goal'] = self.desired_goal



        return state, info

    def step(self, action):
        """
        æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å› (obs, reward, done, truncated, info)ã€‚
        æ³¨æ„ï¼šGymnasium çš„ step() è¿”å› 5 ä¸ªå€¼ï¼ˆåŒ…æ‹¬ truncatedï¼‰ã€‚
        """
        obs, external_reward, terminated, truncated, info = self._env.step(action)
        self.total_step += 1
        state = np.concatenate( [obs['observation'],self.desired_goal ] )
        info['desired_goal'] = self.desired_goal

        # ç¡®ä¿ info åŒ…å« is_successï¼ˆSB3 çš„ success_rate ä¾èµ–æ­¤å­—æ®µï¼‰
        if external_reward >= 0.0 and terminated:
            success = True
        else:
            success = False

        info["is_success"] = success

        return state, external_reward, terminated, truncated, info

    def render(self):
        """
        æ¸²æŸ“ç¯å¢ƒï¼ˆå¯é€‰ï¼‰ã€‚
        """
        return self._env.render()

    def close(self):
        """
        å…³é—­ç¯å¢ƒï¼Œé‡Šæ”¾èµ„æºã€‚
        """
        self._env.close()

    @property
    def unwrapped(self):
        """
        è¿”å›åŸå§‹ç¯å¢ƒï¼ˆç”¨äºè®¿é—®åŸå§‹æ–¹æ³•ï¼‰ã€‚
        """
        return self._env
```

##### å®šä¹‰ä½å±‚SAC

```python
import os
import random
import numpy as np
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
from collections import deque, namedtuple
from datetime import datetime

# è®¾å¤‡é…ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ç»éªŒå›æ”¾ç¼“å†²åŒº
Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))


class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, *args):
        """ä¿å­˜ä¸€ä¸ªtransitionåˆ°buffer"""
        self.buffer.append(Transition(*args))

    def sample(self, batch_size):
        """éšæœºé‡‡æ ·ä¸€ä¸ªbatchçš„transition"""
        transitions = random.sample(self.buffer, batch_size)
        # å°†batchçš„transitionsè½¬æ¢ä¸ºTransitionçš„batch
        batch = Transition(*zip(*transitions))

        # è½¬æ¢ä¸ºtensorå¹¶æŒ‡å®šè®¾å¤‡
        # state: (batch_size, state_dim) -> (batch_size, state_dim)
        state = torch.FloatTensor(np.array(batch.state)).to(device)
        # action: (batch_size, action_dim) -> (batch_size, action_dim)
        action = torch.FloatTensor(np.array(batch.action)).to(device)
        # reward: (batch_size,) -> (batch_size, 1)
        reward = torch.FloatTensor(np.array(batch.reward)).unsqueeze(1).to(device)
        # next_state: (batch_size, state_dim) -> (batch_size, state_dim)
        next_state = torch.FloatTensor(np.array(batch.next_state)).to(device)
        # done: (batch_size,) -> (batch_size, 1)
        done = torch.FloatTensor(np.array(batch.done)).unsqueeze(1).to(device)

        return state, action, reward, next_state, done

    def __len__(self):
        return len(self.buffer)


# ç­–ç•¥ç½‘ç»œ (Actor)
class GaussianPolicy(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=256, max_action=1.0):
        super(GaussianPolicy, self).__init__()
        self.max_action = max_action
        self.norm = nn.LayerNorm(state_dim)
        # ç‰¹å¾æå–å±‚
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)

        # è¾“å‡ºå‡å€¼å’Œlogæ ‡å‡†å·®
        self.mean = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        """å‰å‘ä¼ æ’­ï¼Œè¿”å›åŠ¨ä½œçš„å‡å€¼å’Œlogæ ‡å‡†å·®"""
        # state: (batch_size, state_dim) -> (batch_size, hidden_dim)
        state = self.norm(state)
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))

        # mean: (batch_size, action_dim)
        mean = self.mean(x)
        # log_std: (batch_size, action_dim)
        log_std = self.log_std(x)
        # é™åˆ¶log_stdçš„èŒƒå›´
        log_std = torch.clamp(log_std, min=-20, max=2)

        return mean, log_std

    def sample(self, state):
        """ä»ç­–ç•¥ä¸­é‡‡æ ·åŠ¨ä½œï¼Œå¹¶è®¡ç®—å¯¹æ•°æ¦‚ç‡"""
        mean, log_std = self.forward(state)
        std = log_std.exp()

        # é‡‡æ ·åŠ¨ä½œ
        normal_noise = torch.randn_like(mean)
        raw_action = mean + normal_noise * std  # æœªè¢« clamp çš„åŸå§‹ action

        # è®¡ç®—åŸå§‹å¯¹æ•°æ¦‚ç‡
        log_prob = -0.5 * (normal_noise.pow(2) + 2 * log_std + np.log(2 * np.pi))
        log_prob = log_prob.sum(dim=-1, keepdim=True)

        # ğŸ”§ âš ï¸ ä¿®æ­£ tanh çš„ log_prob BEFORE clamp
        correction = 2 * (np.log(2) - raw_action - F.softplus(-2 * raw_action))
        log_prob -= correction.sum(dim=-1, keepdim=True)

        # æœ€åæ‰æ‰§è¡Œ clampï¼ˆç”¨äºç¨³å®š backwardï¼Œä¸å½±å“ log_prob è®¡ç®—ï¼‰
        raw_action = torch.clamp(raw_action, -20, 20)

        # è¾“å‡ºæœ€ç»ˆ action
        action = torch.tanh(raw_action) * self.max_action

        return action, log_prob


# Qç½‘ç»œ (Critic)
class QNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(QNetwork, self).__init__()
        self.norm = nn.LayerNorm(state_dim+action_dim)
        # Q1ç½‘ç»œ
        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)

        # Q2ç½‘ç»œ
        self.fc4 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc5 = nn.Linear(hidden_dim, hidden_dim)
        self.fc6 = nn.Linear(hidden_dim, 1)

    def forward(self, state, action):
        """å‰å‘ä¼ æ’­ï¼Œè¿”å›ä¸¤ä¸ªQå€¼"""
        # state: (batch_size, state_dim)
        # action: (batch_size, action_dim)
        sa = torch.cat([state, action], dim=-1)
        sa = self.norm(sa)

        # Q1ç½‘ç»œ
        q1 = F.relu(self.fc1(sa))
        q1 = F.relu(self.fc2(q1))
        q1 = self.fc3(q1)

        # Q2ç½‘ç»œ
        q2 = F.relu(self.fc4(sa))
        q2 = F.relu(self.fc5(q2))
        q2 = self.fc6(q2)

        return q1, q2


# SACç®—æ³•ä¸»ä½“
class HIRO_LOW_SAC:
    def __init__(self, state_dim, action_dim, max_action, writer):
        # è¶…å‚æ•°
        self.gamma = 0.97
        self.tau = 0.005
        self.alpha = 0.2
        self.lr = 3e-4
        self.batch_size = 256
        self.buffer_size = 100000
        self.target_entropy = -action_dim
        self.automatic_entropy_tuning = True
        self.step_cnt = 0
        self.writer = writer

        # ç½‘ç»œåˆå§‹åŒ–
        self.actor = GaussianPolicy(state_dim, action_dim, max_action=max_action).to(device)
        self.critic = QNetwork(state_dim, action_dim).to(device)
        self.critic_target = QNetwork(state_dim, action_dim).to(device)
        self.critic_target.load_state_dict(self.critic.state_dict())

        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.lr)

        # è‡ªåŠ¨è°ƒèŠ‚æ¸©åº¦ç³»æ•°alpha
        if self.automatic_entropy_tuning:
            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)
            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=self.lr)

        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = ReplayBuffer(self.buffer_size)

    def select_action(self, state, evaluate=False):
        """é€‰æ‹©åŠ¨ä½œ"""
        state = torch.FloatTensor(state).unsqueeze(0).to(device)
        if evaluate:
            # è¯„ä¼°æ—¶ä¸æ·»åŠ å™ªå£°
            with torch.no_grad():
                mean, _ = self.actor(state)
                # è¯„ä¼°æ—¶å¸Œæœ›è¡¨ç°ç¨³å®šï¼Œå› æ­¤ç›´æ¥ä½¿ç”¨å‡å€¼ï¼ˆæ¦‚ç‡å¯†åº¦æœ€å¤§çš„ç‚¹ï¼‰.  tanhåæŠŠå€¼æ˜ å°„åˆ°[-1,1]ï¼Œ ä¹˜ä»¥max_actionå°± æ˜ å°„åˆ°ç¯å¢ƒåŠ¨ä½œç©ºé—´
                action = torch.tanh(mean) * self.actor.max_action
        else:
            # è®­ç»ƒæ—¶é‡‡æ ·åŠ¨ä½œ
            action, _ = self.actor.sample(state)

        return action.detach().cpu().numpy()[0]

    def update_parameters(self):
        """æ›´æ–°ç½‘ç»œå‚æ•°"""
        if len(self.replay_buffer) < self.batch_size*10:
            return None,None,None
        self.step_cnt += 1

        # ä»ç¼“å†²åŒºé‡‡æ ·ä¸€ä¸ªbatch
        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)

        with torch.no_grad():
            # é‡‡æ ·ä¸‹ä¸€ä¸ªåŠ¨ä½œå¹¶è®¡ç®—å…¶å¯¹æ•°æ¦‚ç‡
            next_action, next_log_prob = self.actor.sample(next_state)

            # è®¡ç®—ç›®æ ‡Qå€¼
            q1_next, q2_next = self.critic_target(next_state, next_action)
            min_q_next = torch.min(q1_next, q2_next) - self.alpha * next_log_prob
            min_q_next = min_q_next.view(-1,1)
            target_q = reward + (1 - done) * self.gamma * min_q_next

        # æ›´æ–°Criticç½‘ç»œ
        current_q1, current_q2 = self.critic(state, action)
        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # æ›´æ–°Actorç½‘ç»œ
        new_action, log_prob = self.actor.sample(state)
        q1, q2 = self.critic(state, new_action)
        min_q = torch.min(q1, q2)
        # æœ€å¤§åŒ–ç†µå’Œæœ€å¤§åŒ–min_qï¼Œå› ä¸ºæ˜¯æ¢¯åº¦ä¸‹é™ï¼Œè¦å®ç°æ¢¯åº¦ä¸Šå‡ï¼Œæ‰€ä»¥min_qå‰é¢æœ‰è´Ÿå·ï¼Œ ç†µæ˜¯ -log_probï¼Œè´Ÿè´Ÿå¾—æ­£
        actor_loss = (self.alpha * log_prob - min_q).mean()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=5.0)
        self.actor_optimizer.step()

        # è‡ªåŠ¨è°ƒèŠ‚alpha
        if self.automatic_entropy_tuning:
            alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()

            self.alpha_optimizer.zero_grad()
            alpha_loss.backward()
            self.alpha_optimizer.step()
            self.alpha = self.log_alpha.exp().item()

        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        with torch.no_grad():
            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)


        self.writer.add_scalar('lo/critic_loss', critic_loss.item(), self.step_cnt)
        self.writer.add_scalar('lo/actor_loss',  actor_loss.item(), self.step_cnt)
        self.writer.add_scalar('lo/alpha', self.alpha, self.step_cnt)

        return critic_loss.item(), actor_loss.item(), self.alpha


```

##### å®šä¹‰é«˜å±‚SAC

```python
import os
import random
import numpy as np
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
from collections import deque, namedtuple
from datetime import datetime

# è®¾å¤‡é…ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ç»éªŒå›æ”¾ç¼“å†²åŒº
Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done',
                                       'low_states', 'low_actions'))


class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, *args):
        """ä¿å­˜ä¸€ä¸ªtransitionåˆ°buffer"""
        self.buffer.append(Transition(*args))

    def sample(self, batch_size):
        """éšæœºé‡‡æ ·ä¸€ä¸ªbatchçš„transition"""
        transitions = random.sample(self.buffer, batch_size)
        # å°†batchçš„transitionsè½¬æ¢ä¸ºTransitionçš„batch
        batch = Transition(*zip(*transitions))

        # è½¬æ¢ä¸ºtensorå¹¶æŒ‡å®šè®¾å¤‡
        # state: (batch_size, state_dim) -> (batch_size, state_dim)
        state = torch.FloatTensor(np.array(batch.state)).to(device)
        # action: (batch_size, action_dim) -> (batch_size, action_dim)
        action = torch.FloatTensor(np.array(batch.action)).to(device)
        # reward: (batch_size,) -> (batch_size, 1)
        reward = torch.FloatTensor(np.array(batch.reward)).unsqueeze(1).to(device)
        # next_state: (batch_size, state_dim) -> (batch_size, state_dim)
        next_state = torch.FloatTensor(np.array(batch.next_state)).to(device)
        # done: (batch_size,) -> (batch_size, 1)
        done = torch.FloatTensor(np.array(batch.done)).unsqueeze(1).to(device)

        # low_states: (batch_size, num, state_dim)
        low_states = list(batch.low_states)
        # low_actions: (batch_size, num, action_dim)
        low_actions = list(batch.low_actions)

        return state, action, reward, next_state, done, low_states, low_actions

    def __len__(self):
        return len(self.buffer)


# ç­–ç•¥ç½‘ç»œ (Actor)
class GaussianPolicy(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=256, max_action=2.0):
        super(GaussianPolicy, self).__init__()
        self.max_action = max_action

        # å…±äº«çš„ç‰¹å¾æå–å±‚
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)

        # è¾“å‡ºå‡å€¼å’Œlogæ ‡å‡†å·®
        self.mean = nn.Linear(hidden_dim, action_dim)
        self.log_std = nn.Linear(hidden_dim, action_dim)

    def forward(self, state):
        """å‰å‘ä¼ æ’­ï¼Œè¿”å›åŠ¨ä½œçš„å‡å€¼å’Œlogæ ‡å‡†å·®"""
        # state: (batch_size, state_dim) -> (batch_size, hidden_dim)
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))

        # mean: (batch_size, action_dim)
        mean = self.mean(x)
        # log_std: (batch_size, action_dim)
        log_std = self.log_std(x)
        # é™åˆ¶log_stdçš„èŒƒå›´
        log_std = torch.clamp(log_std, min=-20, max=2)

        return mean, log_std

    def sample(self, state):
        """ä»ç­–ç•¥ä¸­é‡‡æ ·åŠ¨ä½œï¼Œå¹¶è®¡ç®—å¯¹æ•°æ¦‚ç‡"""
        # è·å–å‡å€¼å’Œlogæ ‡å‡†å·®
        # mean: (batch_size, action_dim)
        # log_std: (batch_size, action_dim)
        mean, log_std = self.forward(state)
        std = log_std.exp()

        # é‡å‚æ•°åŒ–æŠ€å·§é‡‡æ ·åŠ¨ä½œ
        # normal_noise: (batch_size, action_dim)
        normal_noise = torch.randn_like(mean)
        # action: (batch_size, action_dim)
        raw_action = mean + normal_noise * std

        # è®¡ç®—tanhå˜æ¢å‰çš„å¯¹æ•°æ¦‚ç‡
        log_prob = -0.5 * (normal_noise.pow(2) + 2 * log_std + np.log(2 * np.pi))
        log_prob = log_prob.sum(dim=-1, keepdim=True)

        # åº”ç”¨tanhå˜æ¢
        action = torch.tanh(raw_action) * self.max_action

        # æ·»åŠ tanhçš„Jacobianä¿®æ­£
        log_prob -= (2 * (np.log(2) - raw_action - F.softplus(-2 * raw_action))).sum(dim=-1, keepdim=True)

        return action, log_prob


# Qç½‘ç»œ (Critic)
class QNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(QNetwork, self).__init__()

        # Q1ç½‘ç»œ
        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)

        # Q2ç½‘ç»œ
        self.fc4 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc5 = nn.Linear(hidden_dim, hidden_dim)
        self.fc6 = nn.Linear(hidden_dim, 1)

    def forward(self, state, action):
        """å‰å‘ä¼ æ’­ï¼Œè¿”å›ä¸¤ä¸ªQå€¼"""
        # state: (batch_size, state_dim)
        # action: (batch_size, action_dim)
        sa = torch.cat([state, action], dim=-1)

        # Q1ç½‘ç»œ
        q1 = F.relu(self.fc1(sa))
        q1 = F.relu(self.fc2(q1))
        q1 = self.fc3(q1)

        # Q2ç½‘ç»œ
        q2 = F.relu(self.fc4(sa))
        q2 = F.relu(self.fc5(q2))
        q2 = self.fc6(q2)

        return q1, q2


# SACç®—æ³•ä¸»ä½“
class HIRO_HI_SAC:
    def __init__(self, state_dim, action_dim, max_action, writer):
        # è¶…å‚æ•°
        self.gamma = 0.99
        self.tau = 0.005
        self.alpha = 0.2
        self.lr = 3e-4
        self.batch_size = 128
        self.buffer_size = 100000
        self.target_entropy = -action_dim
        self.automatic_entropy_tuning = True
        self.step_cnt = 0
        self.writer = writer

        # ç½‘ç»œåˆå§‹åŒ–
        self.actor = GaussianPolicy(state_dim, action_dim, max_action=max_action).to(device)
        self.critic = QNetwork(state_dim, action_dim).to(device)
        self.critic_target = QNetwork(state_dim, action_dim).to(device)
        self.critic_target.load_state_dict(self.critic.state_dict())

        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.lr)

        # è‡ªåŠ¨è°ƒèŠ‚æ¸©åº¦ç³»æ•°alpha
        if self.automatic_entropy_tuning:
            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)
            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=self.lr)

        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.replay_buffer = ReplayBuffer(self.buffer_size)

    def select_action(self, state, evaluate=False):
        """é€‰æ‹©åŠ¨ä½œ"""
        state = torch.FloatTensor(state).unsqueeze(0).to(device)
        if evaluate:
            # è¯„ä¼°æ—¶ä¸æ·»åŠ å™ªå£°
            with torch.no_grad():
                mean, _ = self.actor(state)
                # è¯„ä¼°æ—¶å¸Œæœ›è¡¨ç°ç¨³å®šï¼Œå› æ­¤ç›´æ¥ä½¿ç”¨å‡å€¼ï¼ˆæ¦‚ç‡å¯†åº¦æœ€å¤§çš„ç‚¹ï¼‰.  tanhåæŠŠå€¼æ˜ å°„åˆ°[-1,1]ï¼Œ ä¹˜ä»¥max_actionå°± æ˜ å°„åˆ°ç¯å¢ƒåŠ¨ä½œç©ºé—´
                action = torch.tanh(mean) * self.actor.max_action
        else:
            # è®­ç»ƒæ—¶é‡‡æ ·åŠ¨ä½œ
            action, _ = self.actor.sample(state)

        return action.detach().cpu().numpy()

    # å¯¹é«˜å±‚ç­–ç•¥çš„å•æ¡transitionè¿›è¡ŒçŸ«æ­£é‡æ ‡æ³¨
    def relabel_trainsitions(self, lo_policy,
                             action: torch.Tensor,  # (action_dim,) é«˜å±‚åŠ¨ä½œï¼Œéœ€è¦é‡æ–°æ ‡æ³¨
                             low_states: torch.Tensor,  # (seq_len+1, state_dim)ï¼Œä½å±‚çš„çŠ¶æ€åºåˆ—
                             low_actions: torch.Tensor,  # (seq_len, action_dim)ï¼Œä½å±‚çš„åŠ¨ä½œåºåˆ—
                             state_diff,  # è®¡ç®—ä¸¤ä¸ªçŠ¶æ€çš„å·®å€¼å‡½æ•°ï¼Œå…è®¸å¼€å‘è€…è‡ªå®šä¹‰
                             state_modify, # ä¿®æ”¹ä½å±‚ç­–ç•¥çš„è¾“å…¥ï¼Œè®©å®ƒç»„åˆçš„æ˜¯å¯èƒ½çš„candidate
                             candidate_num=10):
        """
        å¯¹å•ä¸ªé«˜å±‚ transitionï¼ŒåŸºäºå½“å‰ lo_policy é‡æ–°æ ‡æ³¨é«˜å±‚åŠ¨ä½œï¼ˆgoalï¼‰
        low_statesçš„é•¿åº¦æ¯”low_actionsçš„é•¿åº¦å¤§ 1

        è¿”å›:
            best_goal: (action_dim,) tensor, è¡¨ç¤ºæ–°çš„é«˜å±‚åŠ¨ä½œ
        """
        seq_len = low_actions.shape[0]
        state_dim = low_states.shape[1]
        action_dim = low_actions.shape[1]
        device = action.device

        # ç”Ÿæˆå€™é€‰é«˜å±‚åŠ¨ä½œ g~ï¼šå…± candidate_num ä¸ª
        s_0 = low_states[0]  # (state_dim,)
        s_c = low_states[-1]  # (state_dim,)
        diff = state_diff(s_c,s_0)  # ç”¨ä½œé‡‡æ ·ä¸­å¿ƒ
        diff = torch.tensor(diff, device=device, dtype=torch.float32)
        candidates = [action, diff]  # åŸå§‹ actionã€diff éƒ½åŠ å…¥
        for _ in range(candidate_num - 2):
            noise = torch.randn_like(diff, device=device, dtype=torch.float32) * 0.5  # å¯æ ¹æ®åŠ¨ä½œèŒƒå›´è°ƒæ•´
            candidates.append(diff + noise)

        scores = []
        for g0 in candidates: #å¯¹æ¯ä¸€å„å€™é€‰é«˜å±‚åŠ¨ä½œï¼Œè®¡ç®—ä½å±‚å¯¹åº”çš„åŠ¨ä½œåºåˆ—
            score = 0
            for t in range(seq_len):
                lo_policy_input = state_modify( np.array( [low_states[t] ] ), np.array( [g0.cpu().numpy()] ))
                lo_policy_input = lo_policy_input[0]
                new_low_action = lo_policy(lo_policy_input)
                loss = -F.mse_loss(torch.FloatTensor(low_actions[t]), torch.FloatTensor(new_low_action)).item()
                score += loss
            scores.append(score/(seq_len+1e-8))


        # æ‰¾å‡ºä½¿ loss æœ€å°çš„ g
        best_idx = torch.argmax(torch.tensor(scores,device=device))
        best_goal = candidates[best_idx.cpu().item()].detach()
        return best_goal

    def update_parameters(self, lo_policy, state_diff, state_modify):
        """æ›´æ–°ç½‘ç»œå‚æ•°"""
        if len(self.replay_buffer) < self.batch_size:
            return None,None,None
        self.step_cnt += 1

        # ä»ç¼“å†²åŒºé‡‡æ ·ä¸€ä¸ªbatch
        state, action, reward, next_state, done, low_states, low_actions = self.replay_buffer.sample(self.batch_size)
        # low_states, low_actions è¿™ä¸¤ä¸ªæ˜¯listï¼Œå…ƒç´ æ˜¯ä½å±‚çš„åºåˆ—ï¼Œé•¿åº¦ä¸ä¸€ï¼Œå¤§å¤šæ•°æ—¶å€™ç­‰äºConfig.new_g_intervalï¼Œä½†æœ‰æ—¶å€™ç”±äºå›åˆç»“æŸï¼Œé•¿åº¦ä¸è¶³Config.new_g_interval
        B = state.shape[0]
        relabeled_action = torch.zeros_like(action)
        for sample_idx in range(B):
            best_goal = self.relabel_trainsitions(lo_policy, action[sample_idx], low_states[sample_idx], low_actions[sample_idx], state_diff, state_modify)
            relabeled_action[sample_idx] = best_goal

        action = relabeled_action

        with torch.no_grad():
            # é‡‡æ ·ä¸‹ä¸€ä¸ªåŠ¨ä½œå¹¶è®¡ç®—å…¶å¯¹æ•°æ¦‚ç‡
            next_action, next_log_prob = self.actor.sample(next_state)

            # è®¡ç®—ç›®æ ‡Qå€¼
            q1_next, q2_next = self.critic_target(next_state, next_action)
            min_q_next = torch.min(q1_next, q2_next) - self.alpha * next_log_prob
            target_q = reward + (1 - done) * self.gamma * min_q_next

        # æ›´æ–°Criticç½‘ç»œ
        current_q1, current_q2 = self.critic(state, action)
        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # æ›´æ–°Actorç½‘ç»œ
        new_action, log_prob = self.actor.sample(state)
        q1, q2 = self.critic(state, new_action)
        min_q = torch.min(q1, q2)
        # æœ€å¤§åŒ–ç†µå’Œæœ€å¤§åŒ–min_qï¼Œå› ä¸ºæ˜¯æ¢¯åº¦ä¸‹é™ï¼Œè¦å®ç°æ¢¯åº¦ä¸Šå‡ï¼Œæ‰€ä»¥min_qå‰é¢æœ‰ç¬¦å·ï¼Œ ç†µæ˜¯ -log_probï¼Œè´Ÿè´Ÿå¾—æ­£
        actor_loss = (self.alpha * log_prob - min_q).mean()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # è‡ªåŠ¨è°ƒèŠ‚alpha
        if self.automatic_entropy_tuning:
            alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()

            self.alpha_optimizer.zero_grad()
            alpha_loss.backward()
            self.alpha_optimizer.step()

            self.alpha = self.log_alpha.exp()

        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)


        self.writer.add_scalar('hi/critic_loss', critic_loss.item(), self.step_cnt)
        self.writer.add_scalar('hi/actor_loss',  actor_loss.item(), self.step_cnt)
        self.writer.add_scalar('hi/alpha', self.alpha.item(), self.step_cnt)

        return critic_loss.item(), actor_loss.item(), self.alpha.item()
```

##### è®­ç»ƒ

ä¸èƒ½æ”¶æ•›ï¼Œéœ€è¦åˆ†æ®µè®­ç»ƒï¼Œè§ä¸‹é¢ç¨³æ‰“ç¨³æ‰çš„è¯¦ç»†è¿‡ç¨‹ã€‚

```python
import datetime

import numpy
import numpy as np

import my_hi_sac
import my_low_sac
import my_fetchreach_env
import os
import torch
from torch.utils.tensorboard import SummaryWriter

class Config:
    max_episodes = 1000
    pretrain_lo_episodes = 1000
    max_episode_steps = 100
    new_g_interval = 20

def encode_g_in_state(state:numpy.ndarray, g:numpy.ndarray):
    assert state.shape[0] == 1 and g.shape[0] == 1, ""
    newstate = numpy.concat( [ state[:, 0:10], g] , axis=-1)
    return newstate

def state_diff(b:torch.Tensor, a:torch.Tensor): # envè¿”å›çš„çŠ¶æ€ï¼Œå‰ä¸‰ä¸ªå…ƒç´ æ˜¯æ‰‹è‡‚æœ«æ®µçš„x,y,zåæ ‡ï¼Œæˆ‘ä»¬çš„ä½å±‚ç›®æ ‡ä¹Ÿæ˜¯ç§»åŠ¨è¿™ä¸ªå·®å€¼
    return b[:3] - a[:3]

def intrinsic_reward(state:numpy.ndarray, g:numpy.ndarray, next_state: numpy.ndarray):
    diff = state_diff(torch.FloatTensor(next_state), torch.FloatTensor(state) )
    dist = torch.nn.functional.mse_loss(diff, torch.FloatTensor(g))
    if dist <= 0.01:
        return 0, True
    else:
        return -dist.cpu().item(), False
    

def generate_zero_mean_g():
    # éšæœºé‡‡æ ·å‰ä¸¤ä¸ªå…ƒç´ ï¼ŒèŒƒå›´æ˜¯ [-0.2, -0.1] âˆª [0.1, 0.2]
    def sample_component():
        sign = np.random.choice([-1, 1])
        return sign * np.random.uniform(0.1, 0.2)

    x1 = sample_component()
    x2 = sample_component()
    x3 = sample_component()

    return np.array([[x1, x2, x3]])

def pretrain_low_policy(env, lo:my_low_sac.HIRO_LOW_SAC):
    lo_episode_cnt = 0
    for episode in range(1, Config.pretrain_lo_episodes):
        state, _ = env.reset()

        episode_reward = 0
        lo_episode_rewards = []
        lo_rw = 0
        lo_done = False
        step_cnt = 0 # ç”¨æ¥å†³å®šlo episodeçš„èµ·æ­¢
        g = None # é«˜å±‚ç»™åˆ°ä½å±‚çš„å­ç›®æ ‡
        s_hi = None # lo episodeçš„èµ·å§‹çŠ¶æ€
        for i in range(Config.max_episode_steps):  # ä¸€ä¸ªå›åˆæœ€å¤šä¸ç¯å¢ƒäº¤äº’xxæ¬¡

            if step_cnt % Config.new_g_interval == 0:
                # å›ºå®šé•¿åº¦çš„lo episodeå¼€å§‹äº†
                lo_episode_cnt += 1
                g = generate_zero_mean_g()
                s_hi = state
                lo_done = False
                lo_episode_rewards = []
    
            assert g is not None, ""
            state = encode_g_in_state(numpy.array([state]), g) # æŠŠgä½œä¸ºè¾“å…¥çš„ä¸€éƒ¨åˆ†
            state = state[0]
            # é€‰æ‹©åŠ¨ä½œ
            action = lo.select_action(state)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, env_reward, term, trunc, _ = env.step(action)
            done = term or trunc
            step_cnt += 1
            episode_reward += env_reward

            if not lo_done: #å½“å‰lo episodeè¿˜æ²¡æœ‰ç»“æŸï¼Œé‚£ä¹ˆå°±è¦è®¡ç®—å†…éƒ¨å¥–åŠ±ã€ç¡®å®šæ˜¯å¦ç»“æŸã€å­˜å‚¨æ—¶é—´æ­¥
                lo_rw, lo_done = intrinsic_reward(s_hi, g[0], next_state)
                lo_done = lo_done or done or (step_cnt % Config.new_g_interval == 0)  # ä½å±‚å›åˆæˆªæ–­äº†,lo_doneä¹Ÿå¿…é¡»è®¾ç½®ä¸ºTrue
                # å­˜å‚¨transition
                lo.replay_buffer.push(state, action, lo_rw, next_state, lo_done)
                lo_episode_rewards.append( lo_rw)

            # æ›´æ–°çŠ¶æ€
            state = next_state

            # æ›´æ–°ç½‘ç»œå‚æ•°
            lo.update_parameters()

            if done or step_cnt % Config.new_g_interval == 0:
                #å›ºå®šé•¿åº¦çš„lo episodeç»“æŸäº†ï¼Œ ä¸»è¦æ˜¯ä¸ŠæŠ¥æ˜¯å¦æˆåŠŸã€å†…éƒ¨å¥–åŠ±çš„å‡å€¼
                if abs(lo_rw) <= 0.01:
                    lo.writer.add_scalar('lo/lo_episode_suc', 1, lo_episode_cnt)
                else:
                    lo.writer.add_scalar('lo/lo_episode_suc', 0, lo_episode_cnt)
                lo.writer.add_scalar('lo/avg_intrinsic_reward', np.mean(lo_episode_rewards), lo_episode_cnt)
            
            if done:
                break

        # è®°å½•åˆ°TensorBoard
        lo.writer.add_scalar('lo/episode_reward', episode_reward, episode)


def train(env, hi:my_hi_sac.HIRO_HI_SAC, lo:my_low_sac.HIRO_LOW_SAC):

    def lo_policy(state):
        return lo.select_action(state, True)

    best_reward = -float('inf')
    lo_episode_cnt = 0
    for episode in range(1, Config.max_episodes + 1):
        state, _ = env.reset()
        episode_reward = 0
        lo_rw = 0
        step_cnt = 0 #ä¸€å®šè¦åˆå§‹åŒ–ä¸º0ï¼Œå› ä¸ºä¸‹é¢åˆ©ç”¨äº†è¿™ä¸ªå€¼æ¨¡cç­‰äº0äº§ç”Ÿg
        g = None
        low_states = []
        low_actions = []
        env_rewards = []

        s_hi = None
        for i in range(Config.max_episode_steps):  #ä¸€ä¸ªå›åˆæœ€å¤šä¸ç¯å¢ƒäº¤äº’xxæ¬¡


            if step_cnt % Config.new_g_interval == 0:
                lo_episode_cnt += 1
                if episode < Config.pretrain_lo_episodes:
                    g = generate_zero_mean_g()
                else:
                    g = hi.select_action(state)
                assert g.shape == (1,3), ""
                s_hi = state
                low_states = []
                low_actions = []
                env_rewards = []

            assert g is not None,  ""
            state = encode_g_in_state(numpy.array([state]), g)
            state = state[0]
            # é€‰æ‹©åŠ¨ä½œ
            action = lo.select_action(state)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, term, trunc,_ = env.step(action)
            done = term or trunc
            step_cnt += 1

            lo_rw, lo_done = intrinsic_reward(s_hi, g[0], next_state)
            lo_done = lo_done  or done or (step_cnt%Config.new_g_interval==0) #ä½å±‚å›åˆæˆªæ–­äº†,lo_doneä¹Ÿå¿…é¡»è®¾ç½®ä¸ºTrue

            low_states.append(state)
            low_actions.append(action)
            env_rewards.append(reward)

            # å­˜å‚¨transition
            lo.replay_buffer.push(state, action, lo_rw, next_state, lo_done)

            # æ›´æ–°çŠ¶æ€
            state = next_state
            episode_reward += reward

            # æ›´æ–°ç½‘ç»œå‚æ•°
            critic_loss, actor_loss, alpha = critic_loss, actor_loss, alpha = lo.update_parameters()

            if lo_done:
                if abs(lo_rw) <= 0.01:
                    lo.writer.add_scalar('lo/lo_episode_suc', 1, lo_episode_cnt)
                else:
                    lo.writer.add_scalar('lo/lo_episode_suc', 0, lo_episode_cnt)
                s_hi_next = state
                r_sum = sum(env_rewards)
                low_states.append(state)  # åŠ å…¥æœ€åçš„ s_t+c
                low_states = numpy.array(low_states)
                low_actions = numpy.array(low_actions)
                hi.replay_buffer.push( s_hi, g[0], r_sum, s_hi_next, done, low_states, low_actions )
                if episode > Config.pretrain_lo_episodes:
                    critic_loss, actor_loss, alpha = hi.update_parameters(lo_policy, state_diff, encode_g_in_state)
                # begin a new lo_episode
                lo_episode_cnt += 1
                if episode < Config.pretrain_lo_episodes:
                    g = generate_zero_mean_g()
                else:
                    g = hi.select_action(state)
                assert g.shape == (1, 3), ""
                s_hi = state
                low_states = []
                low_actions = []
                env_rewards = []

            if done:
                break

        # è®°å½•åˆ°TensorBoard
        lo.writer.add_scalar('lo/episode_reward', episode_reward, episode)
        lo.writer.add_scalar('lo/intrinsic_reward', lo_rw, episode)


# è¯„ä¼°å‡½æ•°
def evaluate(env, hi:my_hi_sac.HIRO_HI_SAC, lo:my_low_sac.HIRO_LOW_SAC, num_episodes=10):
    reward_list = []
    for episode in range(1, num_episodes + 1):
        state, _ = env.reset()
        episode_reward = 0
        done = False
        step_cnt = 0
        g = None
        for i in range(Config.max_episode_steps):
            if step_cnt % Config.new_g_interval == 0:
                g = hi.select_action(state, True)
                assert g.shape == (1, 3), ""

            assert g is not None, ""
            state = encode_g_in_state(numpy.array([state]), g)
            # é€‰æ‹©åŠ¨ä½œ
            action = lo.select_action(state[0], True)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, term, trunc, _ = env.step(action)
            done = term or trunc
            step_cnt += 1

            # æ›´æ–°çŠ¶æ€
            state = next_state
            episode_reward += reward

            if done:
                break
        reward_list.append(episode_reward)
        print(f"Evaluation Episode: {episode}, Reward: {episode_reward:.2f}")
    return numpy.mean(reward_list)

# ä¸»å‡½æ•°
def main():
    # åˆ›å»ºç¯å¢ƒ
    env = my_fetchreach_env.CustomFetchReachEnv()
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    max_action = float(env.action_space.high[0])
    print(f"state_dim:{state_dim}, action_dim:{action_dim}, max_action:{max_action}")

    writer = SummaryWriter(log_dir=f'logs/HIRO_FetchReach_{datetime.datetime.now().strftime("%m%d_%H%M%S")}')
    # åˆ›å»ºSACä»£ç†
    hi = my_hi_sac.HIRO_HI_SAC(state_dim, 3, 1, writer) # é«˜å±‚ç­–ç•¥è¾“å‡ºçš„æ˜¯g,ç›¸å¯¹äºå½“å‰çš„ä½ç½®çš„xyzåç§»é‡ï¼Œå‡è®¾åç§»é‡æœ€å¤š1ç±³
    lo = my_low_sac.HIRO_LOW_SAC(state_dim, action_dim, max_action, writer)

    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs("checkpoints", exist_ok=True)

    #train(env, hi, lo)
    pretrain_low_policy(env, lo)

    env.close()
    env = my_fetchreach_env.CustomFetchReachEnv('human')
    evaluate(env, hi, lo)


if __name__ == '__main__':
    main()
```

![image-20250701153633664](img/image-20250701153633664.png)

##### ç¨³æ‰“ç¨³æ‰

ä¸Šé¢çš„ä¸€æ°”å‘µæˆçš„ä»£ç å¹¶ä¸èƒ½æ”¶æ•›ï¼Œé‚£å°±ä¸€æ­¥ä¸€æ­¥æ¥ï¼Œæ…¢æ…¢ä¸Šå¤æ‚åº¦

###### step1ï¼šç”¨SB3çš„SAC æå®šå›ºå®šçš„çŸ­è·ç¦»å°ç›®æ ‡

ä¸¤ä¸ªå‘ç°ï¼š

1. æˆ‘åœ¨è¿™é‡Œè¸©äº†ä¸ªå‘ï¼šéšä¾¿å†™äº†ä¸ªå°ç›®æ ‡ï¼šåœ¨resetåˆå§‹ä½ç½®çš„åŸºç¡€ä¸Šï¼Œå†ä½ç§»[0.1, 0.1, 0.1]ï¼Œå®é™…ä¸Šæ˜¯ä¸å¯è¾¾çš„ï¼Œå› ä¸ºresetåæœºæ¢°æ‰‹è‡‚ä¼¸ç›´æ°´å¹³çŠ¶ï¼Œxyzä¸‰ä¸ªæ–¹å‘éƒ½ä¸ºæ­£å°±è¡¨ç¤ºç›®æ ‡ä½ç½®åœ¨å¯è¾¾èŒƒå›´ï¼ˆåŠçƒï¼‰å¤–é¢äº†ã€‚æˆ‘åœ¨è¿™é‡Œæµªè´¹äº†ä¸€æ•´å¤©ã€‚
2. è¾“å…¥åˆ°æ·±åº¦ç½‘ç»œé‡Œçš„çŠ¶æ€ï¼Œåé¢ä¸‰ä¸ªç»´åº¦æ˜¯ç›®æ ‡ä½ç½®çš„ç»å¯¹ä½ç½®ï¼Œè¿˜æ˜¯ç›¸å¯¹åˆå§‹ä½ç½®çš„ä½ç§»ï¼ŒéªŒè¯äº†éƒ½èƒ½æ”¶æ•›ã€‚ä½†æˆ‘è§‰å¾—ä½ç½®å¥½ä¸€äº›ï¼Œå› ä¸ºç›¸å¯¹ä½ç½®æ˜¯ç›¸å¯¹åˆå§‹ä½ç½®çš„ä½ç§»ï¼Œç½‘ç»œè¿˜è¦è®°ä½åˆå§‹ä½ç½®...

envç»™å‡ºçš„å‡ ä¸ªå¯è¡Œçš„å°ç›®æ ‡ gï¼Œæœ‰çš„é•¿åº¦åå¤§çš„ä¹Ÿä¸å®¹æ˜“æ”¶æ•›ï¼Œè¦æŒ‘ç»å¯¹å€¼å°çš„åšä¸ºå°ç›®æ ‡

```
# resetåç«‹å³æ‰§è¡Œ g = obs['desired_goal'] - obs['achieved_goal']ï¼Œ
# å¯ä»¥å¾—åˆ°æœ‰ä¸‹é¢è¿™äº›å€¼ï¼š
g=[-0.05197624  0.41593705 -0.38898413]
g=[-0.08951826  0.62014015 -0.35802831]
g=[-0.16458111  0.42454838 -0.15700847]
g=[-0.21737     0.40604419 -0.16089385]
g=[ 0.03985209  0.4883993  -0.10840308]
g=[-0.00156387  0.57738327 -0.24667852]
g=[-0.08161544  0.4379823  -0.29055602]
g=[-0.05553611  0.60416518 -0.16857048]
g=[-0.14497306  0.63118409 -0.30566294]
g=[ 0.01801001  0.4210462  -0.22033174]
g=[ 0.04373577  0.39730558 -0.15274272]
g=[-0.20449369  0.4847844  -0.22082668]
g=[-0.16781741  0.61902692 -0.34448007]
g=[ 0.01637453  0.59159179 -0.15796917]
g=[ 0.00623025  0.44328504 -0.1904544 ]
g=[-0.03524174  0.57974617 -0.29795416]
```

å°ç›®æ ‡ä½ç§»[-0.08951826,  0.12014015, +0.15802831]ï¼Œæ”¶æ•›å¾—å¾ˆå¥½

![image-20250702160419049](img/image-20250702160419049.png)

```python
import random
import time

import gymnasium as gym
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback
from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize, VecMonitor
from stable_baselines3.common.monitor import Monitor
import numpy as np
import gymnasium_robotics
import math



# ç¯å¢ƒçš„å†å°è£…
# ç¯å¢ƒè¿”å›çš„stateé‡Œè¦åŒ…å«desired_goal
# ç¯å¢ƒçš„observation_spaceéœ€è¦ç›¸åº”çš„æ”¹åŠ¨
# æ‰‹åŠ¨æ„é€ rewardï¼Œæ ¹æ®ä¸¾ä¾‹desired_goalçš„è·ç¦»å˜åŒ–ï¼Œè¿”å›reward
class CustomFetchReachEnv(gym.Env):
    """
    è‡ªå®šä¹‰å°è£… FetchReach-v3 ç¯å¢ƒï¼Œç¬¦åˆ Gymnasium æ¥å£è§„èŒƒã€‚
    å…¼å®¹ SB3 è®­ç»ƒï¼Œæ”¯æŒ TensorBoard è®°å½• success_rateã€‚
    """

    def __init__(self, render_mode=None):
        """
        åˆå§‹åŒ–ç¯å¢ƒã€‚
        Args:
            render_mode (str, optional): æ¸²æŸ“æ¨¡å¼ï¼Œæ”¯æŒ "human" æˆ– "rgb_array"ã€‚
        """
        super().__init__()


        # åˆ›å»ºåŸå§‹ FetchReach-v3 ç¯å¢ƒ
        self._env = gym.make("FetchReach-v3", render_mode=render_mode, max_episode_steps=100)

        # ç»§æ‰¿åŸå§‹çš„åŠ¨ä½œå’Œè§‚æµ‹ç©ºé—´
        self.action_space = self._env.action_space
        self.observation_space = gym.spaces.Box(-np.inf, np.inf, shape=(10+3,))  # ç®€åŒ–åçš„çŠ¶æ€, 10ä¸ªobserveï¼Œ3ä¸ªdesired_goalï¼Œä¸€èµ·æ‹¼æ¥ä¸ºstateè¿”å›



        self.total_step = 0

        # åˆå§‹åŒ–æ¸²æŸ“æ¨¡å¼
        self.render_mode = render_mode
        self.desired_goal = None
        self.g = None


    def reset(self, seed=None, options=None):
        """
        é‡ç½®ç¯å¢ƒï¼Œè¿”å›åˆå§‹è§‚æµ‹å’Œ infoã€‚
        """
        obs, info = self._env.reset(seed=seed, options=options)
		
        # å…³é”®ä»£ç ï¼š
        #å°è¯•å›ºå®šç›®æ ‡ä½ç½®è¿›è¡Œè®­ç»ƒï¼Œç»“æœæ˜¾ç¤ºå¯ä»¥åˆ°è¾¾100%æˆåŠŸç‡
        # è¿™ä¸ªä½ç§»på¯ä¸èƒ½ä¹±å†™ï¼Œè¡€çš„æ•™è®­ã€‚
        self.g = np.array( [-0.08951826,  0.12014015, +0.15802831] )
        self.desired_goal = obs['achieved_goal'] + self.g


        state = np.concatenate( [obs['observation'], self.desired_goal ] ) #è¿™é‡ŒæŠŠself.gç¼–ç è¿›å»ä¹Ÿæ˜¯å¯ä»¥çš„

        info['desired_goal'] = self.desired_goal



        return state, info

    def step(self, action):
        """
        æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å› (obs, reward, done, truncated, info)ã€‚
        æ³¨æ„ï¼šGymnasium çš„ step() è¿”å› 5 ä¸ªå€¼ï¼ˆåŒ…æ‹¬ truncatedï¼‰ã€‚
        """
        obs, external_reward, terminated, truncated, info = self._env.step(action)
        self.total_step += 1
        state = np.concatenate( [obs['observation'], self.desired_goal ] )#è¿™é‡ŒæŠŠself.gç¼–ç è¿›å»ä¹Ÿæ˜¯å¯ä»¥çš„
        info['desired_goal'] = self.desired_goal

        # è·å– gripper ä½ç½®å’Œç›®æ ‡ä½ç½®ï¼ˆFetchReach çš„ obs åŒ…å«è¿™äº›ä¿¡æ¯ï¼‰
        gripper_pos = obs["observation"][:3]  # å‰ 3 ç»´æ˜¯ gripper çš„ (x, y, z)
        target_pos = self.desired_goal # ç›®æ ‡ä½ç½®

        # è®¡ç®— gripper åˆ°ç›®æ ‡çš„æ¬§æ°è·ç¦»
        distance = np.linalg.norm(gripper_pos - target_pos)

        success = np.linalg.norm(obs['achieved_goal'] - self.desired_goal) < 0.05
        if success:
            external_reward = 1
            terminated = True
        else:
            external_reward = -distance

        # ç¡®ä¿ info åŒ…å« is_successï¼ˆSB3 çš„ success_rate ä¾èµ–æ­¤å­—æ®µï¼‰
        info["is_success"] = success

        return state, external_reward, terminated, truncated, info

    def render(self):
        """
        æ¸²æŸ“ç¯å¢ƒï¼ˆå¯é€‰ï¼‰ã€‚
        """
        return self._env.render()

    def close(self):
        """
        å…³é—­ç¯å¢ƒï¼Œé‡Šæ”¾èµ„æºã€‚
        """
        self._env.close()

    @property
    def unwrapped(self):
        """
        è¿”å›åŸå§‹ç¯å¢ƒï¼ˆç”¨äºè®¿é—®åŸå§‹æ–¹æ³•ï¼‰ã€‚
        """
        return self._env



# 1. å¤šè¿›ç¨‹ç¯å¢ƒåˆ›å»º
def make_env(seed):
    def _init():
        env = CustomFetchReachEnv()
        env = Monitor(env)  # å•ç¯å¢ƒç›‘æ§
        env.reset(seed=seed)
        return env
    return _init

if __name__ == '__main__':
    n_envs = 16
    env = SubprocVecEnv([make_env(i) for i in range(n_envs)])
    env = VecMonitor(env)  # â• è®°å½•æ¯å›åˆ reward/length
    env = VecNormalize(env, norm_obs=False, norm_reward=False)  # å’Œ Hugging Face æ¨¡å‹ä¸€è‡´


    def linear_schedule(initial_value):
        def func(progress_remaining):
            return initial_value * progress_remaining  # 1 â†’ 0
        return func


    policy_kwargs = dict(
        net_arch=dict(
            pi=[256, 256, 256],  # actor ç½‘ç»œç»“æ„
            qf=[256, 256, 256]  # critic (Q-network) ç»“æ„
        )
    )


    # 2. SAC è®­ç»ƒè¶…å‚æ•°ï¼ˆå‚è€ƒ RL Zooï¼‰
    model = SAC(
        "MlpPolicy",
        env,
        batch_size=256,
        buffer_size=1_000_000,
        learning_starts=10_000,
        learning_rate=3e-4,
        tau=0.005,
        gamma=0.97,
        train_freq=1,
        gradient_steps=1,
        ent_coef=0.01,
        verbose=0,
        tensorboard_log='logs/',
        policy_kwargs=policy_kwargs,
    )

    #è®­ç»ƒ
    total_timesteps = int(1e6)
    model.learn(
        total_timesteps=total_timesteps,
    )
```

###### step2ï¼šç”¨SB3çš„SACæå®šå¤šä¸ªçŸ­è·ç¦»å°ç›®æ ‡

è¿™æ¬¡ g ä¸æ˜¯å›ºå®šçš„ï¼Œæ˜¯å˜åŒ–çš„ä¸€æ‰¹ï¼Œèƒ½å¤Ÿæ”¶æ•›ã€‚

![image-20250702171352797](img/image-20250702171352797.png)

```python
import random
import time

import gymnasium as gym
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback
from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize, VecMonitor
from stable_baselines3.common.monitor import Monitor
import numpy as np
import gymnasium_robotics
import math




# ç¯å¢ƒçš„å†å°è£…
# ç¯å¢ƒè¿”å›çš„stateé‡Œè¦åŒ…å«desired_goal
# ç¯å¢ƒçš„observation_spaceéœ€è¦ç›¸åº”çš„æ”¹åŠ¨
# æ‰‹åŠ¨æ„é€ rewardï¼Œæ ¹æ®ä¸¾ä¾‹desired_goalçš„è·ç¦»å˜åŒ–ï¼Œè¿”å›reward
class CustomFetchReachEnv(gym.Env):
    """
    è‡ªå®šä¹‰å°è£… FetchReach-v3 ç¯å¢ƒï¼Œç¬¦åˆ Gymnasium æ¥å£è§„èŒƒã€‚
    å…¼å®¹ SB3 è®­ç»ƒï¼Œæ”¯æŒ TensorBoard è®°å½• success_rateã€‚
    """

    def __init__(self, render_mode=None):
        """
        åˆå§‹åŒ–ç¯å¢ƒã€‚
        Args:
            render_mode (str, optional): æ¸²æŸ“æ¨¡å¼ï¼Œæ”¯æŒ "human" æˆ– "rgb_array"ã€‚
        """
        super().__init__()


        # åˆ›å»ºåŸå§‹ FetchReach-v3 ç¯å¢ƒ
        self._env = gym.make("FetchReach-v3", render_mode=render_mode, max_episode_steps=100)

        # ç»§æ‰¿åŸå§‹çš„åŠ¨ä½œå’Œè§‚æµ‹ç©ºé—´
        self.action_space = self._env.action_space
        self.observation_space = gym.spaces.Box(-np.inf, np.inf, shape=(10+3,))  # ç®€åŒ–åçš„çŠ¶æ€, 10ä¸ªobserveï¼Œ3ä¸ªdesired_goalï¼Œä¸€èµ·æ‹¼æ¥ä¸ºstateè¿”å›



        self.total_step = 0

        # åˆå§‹åŒ–æ¸²æŸ“æ¨¡å¼
        self.render_mode = render_mode
        self.desired_goal = None
        self.g = None
        self.g_list = []
        self.generate_g()

    # å…³é”®ä»£ç ï¼š
    # äº§ç”Ÿéšæœºçš„å¯è¾¾çš„10ä¸ªä½ç§»å¾ˆå°çš„ä½å±‚ç›®æ ‡ï¼Œç”¨æ¥è®­ç»ƒ
    def generate_g(self):
        self.g_list = []
        while len(self.g_list) < 10:
            obs, _ = self._env.reset()
            g = (obs["desired_goal"] - obs["achieved_goal"])
            while np.linalg.norm(g) > 0.15:
                g = g * random.uniform(0.7, 0.9)
            self.g_list.append(g)
        print(f"get {len(self.g_list)} little goals ")


    def reset(self, seed=None, options=None):
        """
        é‡ç½®ç¯å¢ƒï¼Œè¿”å›åˆå§‹è§‚æµ‹å’Œ infoã€‚
        """
        obs, info = self._env.reset(seed=seed, options=options)


        self.g = self.g_list[ random.randint(0, len(self.g_list)-1) ]
        self.desired_goal = obs['achieved_goal'] + self.g


        state = np.concatenate( [obs['observation'], self.desired_goal ] ) #è¿™é‡ŒæŠŠself.gç¼–ç è¿›å»ä¹Ÿæ˜¯å¯ä»¥çš„

        info['desired_goal'] = self.desired_goal



        return state, info

    def step(self, action):
        """
        æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å› (obs, reward, done, truncated, info)ã€‚
        æ³¨æ„ï¼šGymnasium çš„ step() è¿”å› 5 ä¸ªå€¼ï¼ˆåŒ…æ‹¬ truncatedï¼‰ã€‚
        """
        obs, external_reward, terminated, truncated, info = self._env.step(action)
        self.total_step += 1
        state = np.concatenate( [obs['observation'], self.desired_goal ] )#è¿™é‡ŒæŠŠself.gç¼–ç è¿›å»ä¹Ÿæ˜¯å¯ä»¥çš„
        info['desired_goal'] = self.desired_goal

        # è·å– gripper ä½ç½®å’Œç›®æ ‡ä½ç½®ï¼ˆFetchReach çš„ obs åŒ…å«è¿™äº›ä¿¡æ¯ï¼‰
        gripper_pos = obs["observation"][:3]  # å‰ 3 ç»´æ˜¯ gripper çš„ (x, y, z)

        # è®¡ç®— gripper åˆ°ç›®æ ‡çš„æ¬§æ°è·ç¦»
        distance = np.linalg.norm(gripper_pos - self.desired_goal)

        success = distance < 0.05
        if success:
            external_reward = 1
            terminated = True
        else:
            external_reward = -distance

        # ç¡®ä¿ info åŒ…å« is_successï¼ˆSB3 çš„ success_rate ä¾èµ–æ­¤å­—æ®µï¼‰
        info["is_success"] = success

        return state, external_reward, terminated, truncated, info

    def render(self):
        """
        æ¸²æŸ“ç¯å¢ƒï¼ˆå¯é€‰ï¼‰ã€‚
        """
        return self._env.render()

    def close(self):
        """
        å…³é—­ç¯å¢ƒï¼Œé‡Šæ”¾èµ„æºã€‚
        """
        self._env.close()

    @property
    def unwrapped(self):
        """
        è¿”å›åŸå§‹ç¯å¢ƒï¼ˆç”¨äºè®¿é—®åŸå§‹æ–¹æ³•ï¼‰ã€‚
        """
        return self._env



# 1. å¤šè¿›ç¨‹ç¯å¢ƒåˆ›å»º
def make_env(seed):
    def _init():
        env = CustomFetchReachEnv()
        env = Monitor(env)  # å•ç¯å¢ƒç›‘æ§
        env.reset(seed=seed)
        return env
    return _init

if __name__ == '__main__':
    n_envs = 16
    env = SubprocVecEnv([make_env(i) for i in range(n_envs)])
    env = VecMonitor(env)  # â• è®°å½•æ¯å›åˆ reward/length
    env = VecNormalize(env, norm_obs=False, norm_reward=False)  # å’Œ Hugging Face æ¨¡å‹ä¸€è‡´


    def linear_schedule(initial_value):
        def func(progress_remaining):
            return initial_value * progress_remaining  # 1 â†’ 0
        return func


    policy_kwargs = dict(
        net_arch=dict(
            pi=[256, 256, 256],  # actor ç½‘ç»œç»“æ„
            qf=[256, 256, 256]  # critic (Q-network) ç»“æ„
        )
    )


    # 2. SAC è®­ç»ƒè¶…å‚æ•°ï¼ˆå‚è€ƒ RL Zooï¼‰
    model = SAC(
        "MlpPolicy",
        env,
        batch_size=256,
        buffer_size=1_000_000,
        learning_starts=10_000,
        learning_rate=3e-4,
        tau=0.005,
        gamma=0.97,
        train_freq=1,
        gradient_steps=1,
        ent_coef=0.01,
        verbose=0,
        tensorboard_log='logs/',
        policy_kwargs=policy_kwargs,
    )

    #è®­ç»ƒ
    total_timesteps = int(1e6)
    model.learn(
        total_timesteps=total_timesteps,
    )
```

###### step3ï¼šç”¨SB3çš„SACæå®šå¸¦å›ºå®šç›®æ ‡çš„ä½å±‚å°å›åˆ

![image-20250702161211316](img/image-20250702161211316.png)

```python
import random
import time

import gymnasium as gym
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback
from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize, VecMonitor
from stable_baselines3.common.monitor import Monitor
import numpy as np
import gymnasium_robotics
import math



class CustomFetchReachEnv_v2(gym.Env):
    """
    è‡ªå®šä¹‰å°è£… FetchReach-v3 ç¯å¢ƒï¼Œç¬¦åˆ Gymnasium æ¥å£è§„èŒƒã€‚
    å…¼å®¹ SB3 è®­ç»ƒï¼Œæ”¯æŒ TensorBoard è®°å½• success_rateã€‚
    """

    def __init__(self, render_mode=None):
        """
        åˆå§‹åŒ–ç¯å¢ƒã€‚
        Args:
            render_mode (str, optional): æ¸²æŸ“æ¨¡å¼ï¼Œæ”¯æŒ "human" æˆ– "rgb_array"ã€‚
        """
        super().__init__()


        # åˆ›å»ºåŸå§‹ FetchReach-v3 ç¯å¢ƒ
        self._env = gym.make("FetchReach-v3", render_mode=render_mode, max_episode_steps=100)

        # ç»§æ‰¿åŸå§‹çš„åŠ¨ä½œå’Œè§‚æµ‹ç©ºé—´
        self.action_space = self._env.action_space
        #ç›®å‰å®šä¹‰çš„ observation_space ä¸º shape=(13,)ï¼Œå®é™…ç”± 10 ç»´åŸå§‹ observation + 3 ç»´ desired_goal æ‹¼æ¥è€Œæˆ
        self.observation_space = gym.spaces.Box(-np.inf, np.inf, shape=(10+3,))

        self.total_step = 0

        # åˆå§‹åŒ–æ¸²æŸ“æ¨¡å¼
        self.render_mode = render_mode
        self.desired_goal = None

        # è®°å½•ä¸€ä¸ªå°å›åˆçš„ç›¸å…³ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç›®æ ‡ï¼Œå¼€å§‹çš„çŠ¶æ€ï¼Œæ­¥æ•°
        self.g = None
        self.lo_episode_start_s = None
        self.lo_episode_step_cnt = 0


    def reset(self, seed=None, options=None):
        """
        é‡ç½®ç¯å¢ƒï¼Œè¿”å›åˆå§‹è§‚æµ‹å’Œ infoã€‚
        """

        # å…³é”®ä»£ç ï¼š
        # å›ºå®šå°ç›®æ ‡ä¸ºè¿™ä¹ˆå¤šï¼Œä¹Ÿå°±æ˜¯å¸Œæœ›æ‰‹è‡‚æœ«ç«¯åœ¨x,y,zæ–¹å‘ä½ç§»0.1m
        self.g = np.array([-0.08951826,  0.12014015, +0.15802831])
        obs, info = self._env.reset(seed=seed, options=options)

        init_pos = obs['achieved_goal']
        self.desired_goal = init_pos + self.g


        state = np.concatenate( [obs['observation'],self.desired_goal ] )
        self.lo_episode_start_s = state #è®°å½•å°å›åˆçš„å¼€å§‹çŠ¶æ€
        self.lo_episode_step_cnt = 0 # å°å›åˆæ­¥æ•°æ¸…0
        info['desired_goal'] = self.desired_goal

        return state, info

    def step(self, action):

        """
        æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å› (obs, reward, done, truncated, info)ã€‚
        æ³¨æ„ï¼šGymnasium çš„ step() è¿”å› 5 ä¸ªå€¼ï¼ˆåŒ…æ‹¬ truncatedï¼‰ã€‚
        """
        obs, external_reward, terminated, truncated, info = self._env.step(action)
        self.total_step += 1
        self.lo_episode_step_cnt += 1
        state = np.concatenate( [obs['observation'],self.desired_goal ] )
        info['desired_goal'] = self.desired_goal

        # è®¡ç®—å½“å‰çŠ¶æ€ä¸å°å›åˆå¼€å§‹çŠ¶æ€çš„å·®å€¼
        dist = np.linalg.norm(self.desired_goal - obs['achieved_goal'])

        if dist < 0.05: #å¾ˆæ¥è¿‘å°ç›®æ ‡äº†ï¼Œè®¤ä¸ºæˆåŠŸå®Œæˆç›®æ ‡
            terminated = True
            success = True
            external_reward = 1
        else:
            success = False
            external_reward = -dist


        # å°å›åˆå…è®¸çš„æœ€å¤§æ­¥æ•°åˆ°äº†ï¼ˆ20æ­¥ï¼‰
        if self.lo_episode_step_cnt >=20 and not terminated:
            truncated = True

        info["is_success"] = success

        return state, external_reward, terminated, truncated, info

    def render(self):
        """
        æ¸²æŸ“ç¯å¢ƒï¼ˆå¯é€‰ï¼‰ã€‚
        """
        return self._env.render()

    def close(self):
        """
        å…³é—­ç¯å¢ƒï¼Œé‡Šæ”¾èµ„æºã€‚
        """
        self._env.close()

    @property
    def unwrapped(self):
        """
        è¿”å›åŸå§‹ç¯å¢ƒï¼ˆç”¨äºè®¿é—®åŸå§‹æ–¹æ³•ï¼‰ã€‚
        """
        return self._env


# 1. å¤šè¿›ç¨‹ç¯å¢ƒåˆ›å»º
def make_env(seed):
    def _init():
        env = CustomFetchReachEnv_v2()
        env = Monitor(env)  # å•ç¯å¢ƒç›‘æ§
        env.reset(seed=seed)
        return env
    return _init

if __name__ == '__main__':
    n_envs = 16
    env = SubprocVecEnv([make_env(i) for i in range(n_envs)])
    env = VecMonitor(env)  # â• è®°å½•æ¯å›åˆ reward/length
    env = VecNormalize(env, norm_obs=False, norm_reward=False)  # å’Œ Hugging Face æ¨¡å‹ä¸€è‡´


    def linear_schedule(initial_value):
        def func(progress_remaining):
            return initial_value * progress_remaining  # 1 â†’ 0
        return func


    policy_kwargs = dict(
        net_arch=dict(
            pi=[256, 256, 256],  # actor ç½‘ç»œç»“æ„
            qf=[256, 256, 256]  # critic (Q-network) ç»“æ„
        )
    )


    # 2. SAC è®­ç»ƒè¶…å‚æ•°ï¼ˆå‚è€ƒ RL Zooï¼‰
    model = SAC(
        "MlpPolicy",
        env,
        batch_size=256,
        buffer_size=1_000_000,
        learning_starts=10_000,
        learning_rate=3e-4,
        tau=0.005,
        gamma=0.97,
        train_freq=1,
        gradient_steps=1,
        ent_coef=0.01,
        verbose=0,
        tensorboard_log='logs/',
        policy_kwargs=policy_kwargs,
    )

    #è®­ç»ƒ
    total_timesteps = int(1e6)
    model.learn(
        total_timesteps=total_timesteps,
    )
```

###### step4ï¼šç”¨SB3çš„SACæå®šä¸åŒå°ç›®æ ‡çš„ä½å±‚å°å›åˆ

å¯ä»¥æ”¶æ•›ï¼š

![image-20250702174953042](img/image-20250702174953042.png)

```python
import random
import time

import gymnasium as gym
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback
from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize, VecMonitor
from stable_baselines3.common.monitor import Monitor
import numpy as np
import gymnasium_robotics
import math



class CustomFetchReachEnv_v2(gym.Env):
    """
    è‡ªå®šä¹‰å°è£… FetchReach-v3 ç¯å¢ƒï¼Œç¬¦åˆ Gymnasium æ¥å£è§„èŒƒã€‚
    å…¼å®¹ SB3 è®­ç»ƒï¼Œæ”¯æŒ TensorBoard è®°å½• success_rateã€‚
    """

    def __init__(self, render_mode=None):
        """
        åˆå§‹åŒ–ç¯å¢ƒã€‚
        Args:
            render_mode (str, optional): æ¸²æŸ“æ¨¡å¼ï¼Œæ”¯æŒ "human" æˆ– "rgb_array"ã€‚
        """
        super().__init__()


        # åˆ›å»ºåŸå§‹ FetchReach-v3 ç¯å¢ƒ
        self._env = gym.make("FetchReach-v3", render_mode=render_mode, max_episode_steps=100)

        # ç»§æ‰¿åŸå§‹çš„åŠ¨ä½œå’Œè§‚æµ‹ç©ºé—´
        self.action_space = self._env.action_space
        #ç›®å‰å®šä¹‰çš„ observation_space ä¸º shape=(13,)ï¼Œå®é™…ç”± 10 ç»´åŸå§‹ observation + 3 ç»´ desired_goal æ‹¼æ¥è€Œæˆ
        self.observation_space = gym.spaces.Box(-np.inf, np.inf, shape=(10+3,))

        self.total_step = 0

        # åˆå§‹åŒ–æ¸²æŸ“æ¨¡å¼
        self.render_mode = render_mode
        self.desired_goal = None

        # è®°å½•ä¸€ä¸ªå°å›åˆçš„ç›¸å…³ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç›®æ ‡ï¼Œå¼€å§‹çš„çŠ¶æ€ï¼Œæ­¥æ•°
        self.g = None
        self.lo_episode_start_s = None
        self.lo_episode_step_cnt = 0
        self.g_list = []
        self.generate_g()

    # å…³é”®ä»£ç 
    # äº§ç”Ÿéšæœºçš„å¯è¾¾çš„10ä¸ªä½ç§»å¾ˆå°çš„ä½å±‚ç›®æ ‡ï¼Œç”¨æ¥è®­ç»ƒ
    def generate_g(self):
        self.g_list = []
        while len(self.g_list) < 10:
            obs, _ = self._env.reset()
            g = (obs["desired_goal"] - obs["achieved_goal"])
            while np.linalg.norm(g) > 0.15:
                g = g * random.uniform(0.7, 0.9)
            self.g_list.append(g)
        print(f"get {len(self.g_list)} little goals ")


    def reset(self, seed=None, options=None):
        """
        é‡ç½®ç¯å¢ƒï¼Œè¿”å›åˆå§‹è§‚æµ‹å’Œ infoã€‚
        """

        # å…³é”®ä»£ç ï¼š
        # å›ºå®šå°ç›®æ ‡ä¸ºè¿™ä¹ˆå¤šï¼Œä¹Ÿå°±æ˜¯å¸Œæœ›æ‰‹è‡‚æœ«ç«¯åœ¨x,y,zæ–¹å‘ä½ç§»0.1m
        self.g = self.g_list[ random.randint(0, len(self.g_list)-1) ]
        obs, info = self._env.reset(seed=seed, options=options)

        init_pos = obs['achieved_goal']
        self.desired_goal = init_pos + self.g


        state = np.concatenate( [obs['observation'],self.desired_goal ] )
        self.lo_episode_start_s = state #è®°å½•å°å›åˆçš„å¼€å§‹çŠ¶æ€
        self.lo_episode_step_cnt = 0 # å°å›åˆæ­¥æ•°æ¸…0
        info['desired_goal'] = self.desired_goal

        return state, info

    def step(self, action):

        """
        æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å› (obs, reward, done, truncated, info)ã€‚
        æ³¨æ„ï¼šGymnasium çš„ step() è¿”å› 5 ä¸ªå€¼ï¼ˆåŒ…æ‹¬ truncatedï¼‰ã€‚
        """
        obs, external_reward, terminated, truncated, info = self._env.step(action)
        self.total_step += 1
        self.lo_episode_step_cnt += 1
        state = np.concatenate( [obs['observation'],self.desired_goal ] )
        info['desired_goal'] = self.desired_goal

        # è®¡ç®—å½“å‰çŠ¶æ€ä¸å°å›åˆå¼€å§‹çŠ¶æ€çš„å·®å€¼

        dist = np.linalg.norm(self.desired_goal - obs['achieved_goal'])

        if dist < 0.05: #å¾ˆæ¥è¿‘å°ç›®æ ‡äº†ï¼Œè®¤ä¸ºæˆåŠŸå®Œæˆç›®æ ‡
            terminated = True
            success = True
            external_reward = 1
        else:
            success = False
            external_reward = -dist


        # å°å›åˆå…è®¸çš„æœ€å¤§æ­¥æ•°åˆ°äº†ï¼ˆ20æ­¥ï¼‰
        if self.lo_episode_step_cnt >=20 and not terminated:
            truncated = True

        info["is_success"] = success

        return state, external_reward, terminated, truncated, info

    def render(self):
        """
        æ¸²æŸ“ç¯å¢ƒï¼ˆå¯é€‰ï¼‰ã€‚
        """
        return self._env.render()

    def close(self):
        """
        å…³é—­ç¯å¢ƒï¼Œé‡Šæ”¾èµ„æºã€‚
        """
        self._env.close()

    @property
    def unwrapped(self):
        """
        è¿”å›åŸå§‹ç¯å¢ƒï¼ˆç”¨äºè®¿é—®åŸå§‹æ–¹æ³•ï¼‰ã€‚
        """
        return self._env


# 1. å¤šè¿›ç¨‹ç¯å¢ƒåˆ›å»º
def make_env(seed):
    def _init():
        env = CustomFetchReachEnv_v2()
        env = Monitor(env)  # å•ç¯å¢ƒç›‘æ§
        env.reset(seed=seed)
        return env
    return _init

if __name__ == '__main__':
    n_envs = 16
    env = SubprocVecEnv([make_env(i) for i in range(n_envs)])
    env = VecMonitor(env)  # â• è®°å½•æ¯å›åˆ reward/length
    env = VecNormalize(env, norm_obs=False, norm_reward=False)  # å’Œ Hugging Face æ¨¡å‹ä¸€è‡´


    def linear_schedule(initial_value):
        def func(progress_remaining):
            return initial_value * progress_remaining  # 1 â†’ 0
        return func


    policy_kwargs = dict(
        net_arch=dict(
            pi=[256, 256, 256],  # actor ç½‘ç»œç»“æ„
            qf=[256, 256, 256]  # critic (Q-network) ç»“æ„
        )
    )


    # 2. SAC è®­ç»ƒè¶…å‚æ•°ï¼ˆå‚è€ƒ RL Zooï¼‰
    model = SAC(
        "MlpPolicy",
        env,
        batch_size=256,
        buffer_size=1_000_000,
        learning_starts=10_000,
        learning_rate=3e-4,
        tau=0.005,
        gamma=0.97,
        train_freq=1,
        gradient_steps=1,
        ent_coef=0.01,
        verbose=0,
        tensorboard_log='logs/',
        policy_kwargs=policy_kwargs,
    )

    #è®­ç»ƒ
    total_timesteps = int(1e6)
    model.learn( total_timesteps=total_timesteps)
```

###### step5ï¼šå¼€å§‹ä¸Šæ‰‹æ“SACï¼Œæå®šå›ºå®šç›®æ ‡çš„å¤§å›åˆ

åé¢å„æ­¥éª¤éƒ½æ˜¯æ‰‹æ“SACä»£ç ã€‚

ä¸€ä¸ªå®Œæ•´çš„FetchReachå›åˆï¼Œèµ·å§‹çŠ¶æ€æ˜¯ç¡®å®šçš„ï¼Œç›®çš„ä½ç½®ä¹Ÿæ˜¯ç¡®å®šçš„ï¼Œä¸”ç›®çš„ä½ç½®ç›¸æ¯”èµ·å§‹ä½ç½®çš„ä½ç§»æ˜¯ç¡®å®šçš„å°ä½ç§»gã€‚å¯ä»¥è®¤ä¸ºæ˜¯æœ€ç®€å•çš„è¿ç»­åŠ¨ä½œä»»åŠ¡ã€‚

![image-20250702195627796](img/image-20250702195627796.png)

```python
import datetime
from collections import deque

import numpy
import numpy as np

import my_hi_sac
import my_low_sac
import my_fetchreach_env
import os
import torch
from torch.utils.tensorboard import SummaryWriter

class Config:
    max_episodes = 1000
    pretrain_lo_episodes = 1000
    max_episode_steps = 100
    #new_g_interval = 20

def modify_desired_in_state(state:numpy.ndarray, desired:numpy.ndarray):
    assert state.shape[0] ==13  and desired.shape[0] == 3, ""
    new_state = numpy.concat( [ state[0:10], desired] , axis=-1)
    return new_state



def intrinsic_reward(desired:numpy.ndarray, next_state: numpy.ndarray):

    diff = desired - next_state[:3]
    assert diff.shape==(3,), ""
    dist = np.linalg.norm(diff)
    if dist <= 0.05:
        return 1, True, dist
    else:
        return -dist, False, dist


def generate_g():
    return np.array([-0.08951826,  0.12014015, +0.15802831]) # todo:ä¸´æ—¶é™åˆ¶


def pretrain_low_policy(env, lo:my_low_sac.HIRO_LOW_SAC):
    lo_episode_cnt = 0 #ä½å±‚å›åˆä¸ªæ•°ï¼Œæ–¹ä¾¿tbä¸ŠæŠ¥åšæ¨ªåæ ‡
    lo_result=deque(maxlen=100)
    for episode in range(1, Config.pretrain_lo_episodes):
        state, _ = env.reset()
        g = generate_g()
        lo_desired = g + state[:3]

        lo_episode_rewards = [] #ä½å±‚ä¸€ä¸ªå›åˆæ¯ä¸ªæ—¶é—´æ­¥çš„å†…éƒ¨å¥–åŠ±

        lo_done = False # æ ‡è¯†ä½å±‚å›åˆæ˜¯å¦ç»“æŸ
        step_cnt = 0 # ç”¨æ¥å†³å®šlo episodeçš„èµ·æ­¢
        for i in range(Config.max_episode_steps):  # ä¸€ä¸ªå¤§å›åˆæœ€å¤šä¸ç¯å¢ƒäº¤äº’xxæ¬¡

            assert lo_desired is not None, ""
            state = modify_desired_in_state(state, lo_desired) # ä¿®æ”¹
            # é€‰æ‹©åŠ¨ä½œ
            action = lo.select_action(state)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, env_reward, term, trunc, _ = env.step(action)
            # ç¡®ä¿å†™å…¥bufferçš„æ•°æ®ä¸¥æ ¼ç»Ÿä¸€,å› ä¸ºupdateçš„æ—¶å€™ä¼šç”¨åˆ°ã€‚
            next_state = modify_desired_in_state(next_state, lo_desired)
            done = term or trunc
            step_cnt += 1

            # å¯èƒ½å‡ºç°ä½å±‚å·²ç»å®Œæˆäº†ç›®æ ‡ï¼Œä½†ä½å±‚çš„å›åˆé•¿åº¦è¿˜æ²¡æœ‰åˆ°æ¢æ–°ç›®æ ‡çš„æ—¶å€™ã€‚
            # è¿™ç§æƒ…å†µä¸‹ï¼Œç»§ç»­ä¸ç¯å¢ƒäº¤äº’ï¼Œä½†æ˜¯ä¸å†è®¡ç®—å†…éƒ¨å¥–åŠ±ã€ä¸è®°å½•ä½å±‚çš„æ—¶é—´æ­¥ä¿¡æ¯
            # å¦‚æœå½“å‰lo episodeè¿˜æ²¡æœ‰ç»“æŸï¼Œé‚£ä¹ˆå°±è¦è®¡ç®—å†…éƒ¨å¥–åŠ±ã€ç¡®å®šæ˜¯å¦ç»“æŸã€å­˜å‚¨æ—¶é—´æ­¥
            if not lo_done:
                lo_rw, lo_done, dist = intrinsic_reward(lo_desired, next_state)
                lo_done = (lo_done or done  ) # ä½å±‚å›åˆæˆªæ–­äº†,lo_doneä¹Ÿå¿…é¡»è®¾ç½®ä¸ºTrue
                # å­˜å‚¨transition
                lo.replay_buffer.push(state, action, lo_rw, next_state, lo_done)
                #print(f"add transition:{state}, {action}, {lo_rw}, {next_state}, {lo_done}")
                # æ›´æ–°ç½‘ç»œå‚æ•°
                lo.update_parameters()
                lo_episode_rewards.append( lo_rw)
                if lo_done:
                    # å›ºå®šé•¿åº¦çš„lo episodeç»“æŸäº†ï¼Œ ä¸»è¦æ˜¯ä¸ŠæŠ¥æ˜¯å¦æˆåŠŸã€å†…éƒ¨å¥–åŠ±çš„å‡å€¼
                    if lo_rw >= 0:
                        lo_result.append(1)
                    else:
                        lo_result.append(0)
                    lo.writer.add_scalar('lo/avg_intrinsic_reward', np.mean(lo_episode_rewards), episode)
            # æ›´æ–°çŠ¶æ€
            state = next_state
            if done:
                break

        lo.writer.add_scalar('lo/suc_ratio', np.mean(lo_result), episode)

# ä¸»å‡½æ•°
def main():
    # åˆ›å»ºç¯å¢ƒ
    env = my_fetchreach_env.CustomFetchReachEnv()
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    max_action = float(env.action_space.high[0])
    print(f"state_dim:{state_dim}, action_dim:{action_dim}, max_action:{max_action}")

    writer = SummaryWriter(log_dir=f'logs/HIRO_FetchReach_{datetime.datetime.now().strftime("%m%d_%H%M%S")}')
    # åˆ›å»ºSACä»£ç†
    hi = my_hi_sac.HIRO_HI_SAC(state_dim, 3, 1, writer) # é«˜å±‚ç­–ç•¥è¾“å‡ºçš„æ˜¯g,ç›¸å¯¹äºå½“å‰çš„ä½ç½®çš„xyzåç§»é‡ï¼Œå‡è®¾åç§»é‡æœ€å¤š1ç±³
    lo = my_low_sac.HIRO_LOW_SAC(state_dim, action_dim, max_action, writer)

    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs("checkpoints", exist_ok=True)

    pretrain_low_policy(env, lo)

if __name__ == '__main__':
    main()
```

###### step6ï¼šæå®šå›ºå®šä½ç§»çš„å°å›åˆ

è¿™ä¸ªä»»åŠ¡æ¯”step5è¦éš¾ï¼Œstep5çš„ä»»åŠ¡çš„èµ·å§‹çŠ¶æ€å’Œç›®æ ‡éƒ½æ˜¯å›ºå®šçš„ï¼Œæ˜¯æœ€ç®€å•çš„ä»»åŠ¡ï¼Œä¸”å…è®¸100æ­¥ï¼›step6çš„æ¯ä¸ªå°å›åˆçš„èµ·å§‹çŠ¶æ€å’Œç›®æ ‡éƒ½ä¸æ˜¯ä¸€æ ·çš„ï¼ˆè™½ç„¶ç›®å‰ä½ç§»gè¿˜æ˜¯å›ºå®šçš„ï¼‰ï¼Œä¸”å°å›åˆé•¿åº¦åªæœ‰20æ­¥ã€‚æœ¬è´¨ä¸Šå°±æ˜¯ä¸åŒçš„ä½å±‚ç›®æ ‡çš„å®Œæˆèƒ½åŠ›äº†ã€‚

åŠ æ·±äº†ç¥ç»ç½‘ç»œï¼ŒæˆåŠŸç‡ä¹Ÿæ²¡æœ‰æå‡ï¼Œåªèƒ½åˆ°40%çš„æˆåŠŸç‡ã€‚

**æˆ‘ç™¾æ€ä¸å¾—å…¶è§£ï¼Œåæ¥æƒ³æ˜ç™½äº†ï¼ŒåŒæ ·çš„å°ä½ç§»gå¸¸é‡ï¼Œåœ¨æœºæ¢°è‡‚ç§»åŠ¨åˆ°æŸäº›çŠ¶æ€ä¸‹å¯èƒ½å°±æ˜¯ä¸å¯è¾¾çš„**ï¼Œä¸èƒ½ä½œä¸ºè¿™ä¸ªçŠ¶æ€èµ·å§‹çš„å°å›åˆçš„ç›®çš„ä½ç§»ã€‚

![image-20250703115630836](img/image-20250703115630836.png)

```python
import datetime
from collections import deque

import numpy
import numpy as np

import my_hi_sac
import my_low_sac
import my_fetchreach_env
import os
import torch
from torch.utils.tensorboard import SummaryWriter

class Config:
    max_episodes = 1000
    pretrain_lo_episodes = 1000
    max_episode_steps = 100
    new_g_interval = 20

def modify_desired_in_state(state:numpy.ndarray, desired:numpy.ndarray):
    assert state.shape[0] ==13  and desired.shape[0] == 3, ""
    new_state = numpy.concat( [ state[0:10], desired] , axis=-1)
    return new_state



def intrinsic_reward(desired:numpy.ndarray, next_state: numpy.ndarray):

    diff = desired - next_state[:3]
    assert diff.shape==(3,), ""
    dist = np.linalg.norm(diff)
    if dist <= 0.05:
        return 1, True, dist
    else:
        return -dist, False, dist


def generate_g():
    return np.array([-0.08951826,  0.12014015, +0.15802831]) # todo:ä¸´æ—¶é™åˆ¶


def pretrain_low_policy(env, lo:my_low_sac.HIRO_LOW_SAC):
    lo_episode_cnt = 0 #ä½å±‚å›åˆä¸ªæ•°ï¼Œæ–¹ä¾¿tbä¸ŠæŠ¥åšæ¨ªåæ ‡
    lo_result=deque(maxlen=100)
    for episode in range(1, Config.pretrain_lo_episodes):
        state, _ = env.reset()

        step_cnt = 0 # ç”¨æ¥å†³å®šlo episodeçš„èµ·æ­¢
        for i in range(Config.max_episode_steps):  # ä¸€ä¸ªå¤§å›åˆæœ€å¤šä¸ç¯å¢ƒäº¤äº’xxæ¬¡

            if step_cnt % Config.new_g_interval == 0:
                #å¼€å§‹ä¸€ä¸ªå°å›åˆï¼Œåº”ç”¨å›ºå®šä½ç§»åçš„ä½ç½®ä½œä¸ºç›®æ ‡ï¼Œå…³é”®ä»£ç 
                # åæ¥æƒ³æ˜ç™½äº†ï¼Œè¿™æ—¶å€™stateä¸æ˜¯æœºæ¢°è‡‚çš„å¤§å›åˆèµ·å§‹ä½ç½®ï¼Œæ˜¯ä¸­é€”æŸä¸ªä½ç½® state+gå¯èƒ½æ˜¯æœºæ¢°è‡‚ä¸å¯è¾¾çš„ï¼Œæ“
                g = generate_g()
                lo_desired = g + state[:3]
                lo_episode_rewards = []  # ä½å±‚ä¸€ä¸ªå›åˆæ¯ä¸ªæ—¶é—´æ­¥çš„å†…éƒ¨å¥–åŠ±
                lo_done = False  # æ ‡è¯†ä½å±‚å›åˆæ˜¯å¦ç»“æŸ

            assert lo_desired is not None, ""
            state = modify_desired_in_state(state, lo_desired) # ä¿®æ”¹
            # é€‰æ‹©åŠ¨ä½œ
            action = lo.select_action(state)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, env_reward, term, trunc, _ = env.step(action)
            # ç¡®ä¿å†™å…¥bufferçš„æ•°æ®ä¸¥æ ¼ç»Ÿä¸€,å› ä¸ºupdateçš„æ—¶å€™ä¼šç”¨åˆ°ã€‚
            next_state = modify_desired_in_state(next_state, lo_desired)
            done = term or trunc
            step_cnt += 1

            # å¯èƒ½å‡ºç°ä½å±‚å·²ç»å®Œæˆäº†ç›®æ ‡ï¼Œä½†ä½å±‚çš„å›åˆé•¿åº¦è¿˜æ²¡æœ‰åˆ°æ¢æ–°ç›®æ ‡çš„æ—¶å€™ã€‚
            # è¿™ç§æƒ…å†µä¸‹ï¼Œç»§ç»­ä¸ç¯å¢ƒäº¤äº’ï¼Œä½†æ˜¯ä¸å†è®¡ç®—å†…éƒ¨å¥–åŠ±ã€ä¸è®°å½•ä½å±‚çš„æ—¶é—´æ­¥ä¿¡æ¯
            # å¦‚æœå½“å‰lo episodeè¿˜æ²¡æœ‰ç»“æŸï¼Œé‚£ä¹ˆå°±è¦è®¡ç®—å†…éƒ¨å¥–åŠ±ã€ç¡®å®šæ˜¯å¦ç»“æŸã€å­˜å‚¨æ—¶é—´æ­¥
            assert lo_done is not None, ""
            if not lo_done:
                lo_rw, lo_done, dist = intrinsic_reward(lo_desired, next_state)
                lo_done = (lo_done or done or (step_cnt % Config.new_g_interval == 0) ) # ä½å±‚å›åˆæˆªæ–­äº†,lo_doneä¹Ÿå¿…é¡»è®¾ç½®ä¸ºTrue
                # å­˜å‚¨transition
                lo.replay_buffer.push(state, action, lo_rw, next_state, lo_done)
                #print(f"add transition:{state}, {action}, {lo_rw}, {next_state}, {lo_done}")
                # æ›´æ–°ç½‘ç»œå‚æ•°
                lo.update_parameters()
                assert lo_episode_rewards is not None, ""
                lo_episode_rewards.append( lo_rw)
                if lo_done:
                    # å›ºå®šé•¿åº¦çš„lo episodeç»“æŸäº†ï¼Œ ä¸»è¦æ˜¯ä¸ŠæŠ¥æ˜¯å¦æˆåŠŸã€å†…éƒ¨å¥–åŠ±çš„å‡å€¼
                    if lo_rw >= 0:
                        lo_result.append(1)
                    else:
                        lo_result.append(0)
                    lo.writer.add_scalar('lo/avg_intrinsic_reward', np.mean(lo_episode_rewards), episode)
            # æ›´æ–°çŠ¶æ€
            state = next_state
            if done:
                break

        lo.writer.add_scalar('lo/suc_ratio', np.mean(lo_result), episode)

# ä¸»å‡½æ•°
def main():
    # åˆ›å»ºç¯å¢ƒ
    env = my_fetchreach_env.CustomFetchReachEnv()
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    max_action = float(env.action_space.high[0])
    print(f"state_dim:{state_dim}, action_dim:{action_dim}, max_action:{max_action}")

    writer = SummaryWriter(log_dir=f'logs/HIRO_FetchReach_{datetime.datetime.now().strftime("%m%d_%H%M%S")}')
    # åˆ›å»ºSACä»£ç†
    hi = my_hi_sac.HIRO_HI_SAC(state_dim, 3, 1, writer) # é«˜å±‚ç­–ç•¥è¾“å‡ºçš„æ˜¯g,ç›¸å¯¹äºå½“å‰çš„ä½ç½®çš„xyzåç§»é‡ï¼Œå‡è®¾åç§»é‡æœ€å¤š1ç±³
    lo = my_low_sac.HIRO_LOW_SAC(state_dim, action_dim, max_action, writer)

    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs("checkpoints", exist_ok=True)

    pretrain_low_policy(env, lo)



if __name__ == '__main__':
    main()
```

###### step7ï¼š æå®šå›ºå®šçš„è·¯çº¿ä¸­é€”å°ç›®æ ‡

åŸºäºstep6çš„é—®é¢˜ï¼Œæˆ‘å°±æŒ‘ä¸€ä¸ªå¤§å›åˆï¼ŒæŠŠachieved_goalå’Œdesired goalä¹‹é—´çš„ç›´çº¿ä¸Šçš„5ä¸ªç­‰è·ç¦»çš„ç‚¹ä½œä¸ºå°å›åˆçš„ç›®æ ‡ï¼Œæœ‰5ä¸ªç›®æ ‡ï¼Œç§»åŠ¨ä½ç§»éƒ½æ˜¯å…¶ä¸­ä¸€ä¸ªå°æ®µï¼Œå°è¯•è®­ç»ƒä½å±‚ç­–ç•¥ã€‚

èƒ½å¤Ÿå¾ˆå¥½çš„æ”¶æ•›ï¼Œä¸”ä¸­é€”æ¯ä¸ªå°ç›®æ ‡éƒ½æœ‰è¾ƒå¹³è¡¡çš„è¦†ç›–åˆ°ï¼š

![image-20250703151233267](img/image-20250703151233267.png)

```python
import datetime
from collections import deque, defaultdict

import numpy
import numpy as np

import my_hi_sac
import my_low_sac
import my_fetchreach_env
import os
import torch
from torch.utils.tensorboard import SummaryWriter

class Config:
    max_episodes = 3000
    pretrain_lo_episodes = 3000
    max_episode_steps = 100
    new_g_interval = 20

def modify_desired_in_state(state:numpy.ndarray, desired:numpy.ndarray):
    assert state.shape[0] ==13  and desired.shape[0] == 3, ""
    new_state = numpy.concat( [ state[0:10], desired] , axis=-1)
    return new_state



def intrinsic_reward(desired:numpy.ndarray, next_state: numpy.ndarray):

    diff = desired - next_state[:3]
    assert diff.shape==(3,), ""
    dist = np.linalg.norm(diff)
    if dist <= 0.05:
        return 1, True, dist
    else:
        return -dist, False, dist

# å…³é”®ä»£ç ï¼Œå–ä¸€æ¡ç›´çº¿ä¸Šçš„5ä¸ªä½ç½®ä½œä¸ºæˆ‘ä»¬çš„å›ºå®šçš„å°ç›®æ ‡
def generate_anchors(env):

    anchors=[]
    num = 5
    while True:
        state, _ = env.reset()
        start_pos = state[:3]
        end_pos = state[10:]
        diff = end_pos - start_pos

        if np.linalg.norm(diff / num) > 0.15:

            step = diff / num
            print(f'step:{step}, {np.linalg.norm(step)}')
            for i in range(num):
                a = start_pos + i * step
                b = a + step
                anchors.append( (a, b) )
                print(f'route:{a}->{b}')
            break

    return anchors
# å…³é”®ä»£ç ï¼Œæ ¹æ®å½“å‰ä½ç½®ï¼Œæ‰¾å¯ä»¥ä½œä¸ºå°ç›®æ ‡çš„anchor
def get_lo_desired(current_state:np.ndarray, anchors):
    idx = 0
    for (a, b) in anchors:
        if np.linalg.norm(a-current_state[:3]) < 0.05:
            return b, idx
        idx += 1
    return None, idx

def pretrain_low_policy(env, lo:my_low_sac.HIRO_LOW_SAC):
    lo_episode_cnt = 0 #ä½å±‚å›åˆä¸ªæ•°ï¼Œæ–¹ä¾¿tbä¸ŠæŠ¥åšæ¨ªåæ ‡
    lo_result=deque(maxlen=100)
    anchors = generate_anchors(env)
    total_steps = 0
    sample_num = defaultdict(int) #è®°å½•æ¯ä¸€ä¸ªlo_desireä¸ºç›®æ ‡çš„å›åˆçš„æ¬¡æ•°
    sample_suc = defaultdict(int) #è®°å½•æ¯ä¸€ä¸ªlo_desireä¸ºç›®æ ‡çš„å›åˆçš„æˆåŠŸæ¬¡æ•°
    for episode in range(1, Config.pretrain_lo_episodes):
        state, _ = env.reset()
        lo_step_cnt = 0 # ç”¨æ¥å†³å®šlo episodeçš„èµ·æ­¢
        lo_desired = None
        lo_done = True

        for i in range(Config.max_episode_steps):  # ä¸€ä¸ªå¤§å›åˆæœ€å¤šä¸ç¯å¢ƒäº¤äº’xxæ¬¡

            if lo_done: # low episode ç»“æŸäº†ï¼Œæˆ–è€…æ²¡æœ‰å¼€å§‹
                lo_desired, anchor_idx = get_lo_desired(state, anchors)  # çœ‹çœ‹æœ‰æ²¡æœ‰åˆé€‚çš„é”šç‚¹ç”¨ä½œä¸‹ä¸€ä¸ªä½å±‚ç›®æ ‡
                if lo_desired is not None:
                    sample_num[anchor_idx] += 1
                    lo.writer.add_scalar('lo/lo_desired', anchor_idx, total_steps)
                    lo_episode_rewards = []  # ä½å±‚ä¸€ä¸ªå›åˆæ¯ä¸ªæ—¶é—´æ­¥çš„å†…éƒ¨å¥–åŠ±
                    lo_done = False  # æ ‡è¯†ä½å±‚å›åˆæ˜¯å¦ç»“æŸ
                    lo_step_cnt = 0 # å¼€å§‹è®¡æ­¥


            if lo_desired is not None:
                state = modify_desired_in_state(state, lo_desired) # ä¿®æ”¹
            # é€‰æ‹©åŠ¨ä½œ
            action = lo.select_action(state)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, env_reward, term, trunc, _ = env.step(action)
            # ç¡®ä¿å†™å…¥bufferçš„æ•°æ®ä¸¥æ ¼ç»Ÿä¸€,å› ä¸ºupdateçš„æ—¶å€™ä¼šç”¨åˆ°ã€‚
            if lo_desired is not None:
                next_state = modify_desired_in_state(next_state, lo_desired)
            done = term or trunc
            lo_step_cnt += 1
            total_steps += 1

            # å¯èƒ½å‡ºç°ä½å±‚å·²ç»å®Œæˆäº†ç›®æ ‡ï¼Œä½†ä½å±‚çš„å›åˆé•¿åº¦è¿˜æ²¡æœ‰åˆ°æ¢æ–°ç›®æ ‡çš„æ—¶å€™ã€‚
            # è¿™ç§æƒ…å†µä¸‹ï¼Œç»§ç»­ä¸ç¯å¢ƒäº¤äº’ï¼Œä½†æ˜¯ä¸å†è®¡ç®—å†…éƒ¨å¥–åŠ±ã€ä¸è®°å½•ä½å±‚çš„æ—¶é—´æ­¥ä¿¡æ¯
            # å¦‚æœå½“å‰lo episodeè¿˜æ²¡æœ‰ç»“æŸï¼Œé‚£ä¹ˆå°±è¦è®¡ç®—å†…éƒ¨å¥–åŠ±ã€ç¡®å®šæ˜¯å¦ç»“æŸã€å­˜å‚¨æ—¶é—´æ­¥
            if not lo_done: #low episode è¿›è¡Œä¸­
                lo_rw, lo_done, dist = intrinsic_reward(lo_desired, next_state)
                lo_done = (lo_done or done or (lo_step_cnt % Config.new_g_interval == 0) ) # ä½å±‚å›åˆæˆªæ–­äº†,lo_doneä¹Ÿå¿…é¡»è®¾ç½®ä¸ºTrue
                # å­˜å‚¨transition
                lo.replay_buffer.push(state, action, lo_rw, next_state, lo_done)

                #print(f"add transition:{state}, {action}, {lo_rw}, {next_state}, {lo_done}")
                # æ›´æ–°ç½‘ç»œå‚æ•°
                lo.update_parameters()
                assert lo_episode_rewards is not None, ""
                lo_episode_rewards.append( lo_rw)
                if lo_done:
                    # å›ºå®šé•¿åº¦çš„lo episodeç»“æŸäº†ï¼Œ ä¸»è¦æ˜¯ä¸ŠæŠ¥æ˜¯å¦æˆåŠŸã€å†…éƒ¨å¥–åŠ±çš„å‡å€¼
                    if lo_rw >= 0:
                        lo_result.append(1)
                        sample_suc[anchor_idx] += 1
                    else:
                        lo_result.append(0)
                    lo.writer.add_scalar('lo/avg_intrinsic_reward', np.mean(lo_episode_rewards), episode)

            # æ›´æ–°çŠ¶æ€
            state = next_state
            if done:
                break

        lo.writer.add_scalar('lo/suc_ratio', np.mean(lo_result), episode)
        if episode % 200 == 0:
            total = 1e-7
            for k, c in sample_num.items():
                total += c
            print(f"samples distribution,total={int(total)}:")
            for k, c in sample_num.items():
                print(f"{k}:{c},{c/total:.2f},{(sample_suc[k] / (c+1e-8)):.2f}")

# ä¸»å‡½æ•°
def main():
    # åˆ›å»ºç¯å¢ƒ
    env = my_fetchreach_env.CustomFetchReachEnv()
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    max_action = float(env.action_space.high[0])
    print(f"state_dim:{state_dim}, action_dim:{action_dim}, max_action:{max_action}")

    writer = SummaryWriter(log_dir=f'logs/HIRO_FetchReach_{datetime.datetime.now().strftime("%m%d_%H%M%S")}')
    # åˆ›å»ºSACä»£ç†
    hi = my_hi_sac.HIRO_HI_SAC(state_dim, 3, 1, writer) # é«˜å±‚ç­–ç•¥è¾“å‡ºçš„æ˜¯g,ç›¸å¯¹äºå½“å‰çš„ä½ç½®çš„xyzåç§»é‡ï¼Œå‡è®¾åç§»é‡æœ€å¤š1ç±³
    lo = my_low_sac.HIRO_LOW_SAC(state_dim, action_dim, max_action, writer)

    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs("checkpoints", exist_ok=True)

    pretrain_low_policy(env, lo)



if __name__ == '__main__':
    main()
```

###### step8ï¼šæå®šå„ç§çº¿è·¯ä¸­çš„ä¸­é€”å°ç›®æ ‡

åŸºäºstep7ï¼Œè¿›ä¸€æ­¥çš„ï¼ŒæŒ‘20ä¸ªå¤§å›åˆï¼Œ

1. å¯¹æ¯ä¸ªå¤§å›åˆï¼ŒæŠŠachieved_goalå’Œdesired goalä¹‹é—´çš„ç›´çº¿ä¸Šçš„5ä¸ªç­‰è·ç¦»çš„ç‚¹ä½œä¸ºå°å›åˆçš„ç›®æ ‡ï¼Œæœ‰5ä¸ªç›®æ ‡ï¼Œç§»åŠ¨ä½ç§»éƒ½æ˜¯å…¶ä¸­ä¸€ä¸ªå°æ®µï¼Œå°è¯•è®­ç»ƒä½å±‚ç­–ç•¥ã€‚
2. 20ä¸ªå¤§å›åˆï¼Œå°±å¾—åˆ°100ä¸ªå°å›åˆ

ç»“æœå¦‚ä¸‹ï¼š

```python
begin a trajectory...
	try sub gaol [1.38792137 0.38960179 0.7105527 ]...
	sub goal [1.38792137 0.38960179 0.7105527 ] reached!
	try sub gaol [1.34094274 0.51510359 0.6351054 ]...
	sub goal [1.34094274 0.51510359 0.6351054 ] reached!
	try sub gaol [1.29396411 0.64060538 0.5596581 ]...
	sub goal [1.29396411 0.64060538 0.5596581 ] reached!
	try sub gaol [1.24698547 0.76610717 0.4842108 ]...
	sub goal [1.24698547 0.76610717 0.4842108 ] reached!
	try sub gaol [1.20000684 0.89160897 0.4087635 ]...
	sub goal [1.20000684 0.89160897 0.4087635 ] missed! #å¥‡æ€ªï¼Œæœ€åä¸€ç«™ä¸ºä»€ä¹ˆå¤§æ¦‚ç‡åˆ°ä¸äº†...
```

![image-20250703165124431](img/image-20250703165124431.png)

åæ¥å‘ç°FetchReachç¯å¢ƒæœ‰bugï¼Œçº¢çƒçš„ä½ç½®å’Œ obs['desired_goal']å¯¹ä¸ä¸Š...

è‡³æ­¤ï¼Œè‡³å°‘è¯æ˜äº†ç”¨æ‰‹æ“çš„SACä»£ç å¯ä»¥å®ç°å°ä½ç§»çš„ä½å±‚ç›®æ ‡çš„goal conditional è¾¾æˆã€‚

ä¸‹ä¸€æ­¥å°±æ˜¯å†»ç»“ä½å±‚æ¨¡å‹ï¼Œè®­ç»ƒé«˜å±‚æ¨¡å‹ï¼Œä½¿å…¶å…·å¤‡è§„åˆ’çš„èƒ½åŠ›ã€‚å¯¹äºFetchReachè¿™æ ·çš„ç®€å•ä»»åŠ¡ï¼Œè¿˜å¯ä»¥ç›´æ¥æ±‚ç©ºé—´ç›´çº¿ä¸Šçš„ç‚¹çš„æ–¹å¼ã€‚

```python
import datetime
import random
from collections import deque, defaultdict

import numpy
import numpy as np

import my_hi_sac
import my_low_sac
import my_fetchreach_env
import os
import torch
from torch.utils.tensorboard import SummaryWriter



class Config:
    max_episodes = 3000
    pretrain_lo_episodes = 3000
    max_episode_steps = 100
    new_g_interval = 20

def modify_desired_in_state(state:numpy.ndarray, desired:numpy.ndarray):
    assert state.shape[0] ==13  and desired.shape[0] == 3, ""
    new_state = numpy.concat( [ state[0:10], desired] , axis=-1)
    return new_state



def intrinsic_reward(desired:numpy.ndarray, next_state: numpy.ndarray):

    diff = desired - next_state[:3]
    assert diff.shape==(3,), ""
    dist = np.linalg.norm(diff)
    if dist <= 0.05:
        return 1, True, dist
    else:
        return -dist, False, dist


def generate_anchors(env, repeat=20):
    anchors=[]
    num = 5
    rp_cnt = 0
    while rp_cnt < repeat:
        state, _ = env.reset()
        start_pos = state[:3]
        end_pos = state[10:]
        diff = end_pos - start_pos

        if np.linalg.norm(diff / num) > 0.15:
            rp_cnt += 1
            step = diff / num
            #print(f'step:{step}, {np.linalg.norm(step)}')
            for i in range(num):
                a = start_pos + i * step
                b = a + step
                anchors.append( (a, b) )
                #print(f'route:{a}->{b}')
    return anchors

def get_lo_desired(current_state:np.ndarray, anchors):
    # ä»åˆ—è¡¨ä¸­çš„æŸä¸ªéšæœºä½ç½®å¼€å§‹å¾€åæŸ¥æ‰¾ï¼Œè¿™æ ·æœ‰åˆ©äºæ‰“æ•£é‡åˆ°ä¸åŒå¤§å›åˆçš„èµ·å§‹ä½ç½®
    start_index = random.randint(0, len(anchors)-1)
    for _ in range(len(anchors)):
        (a, b) = anchors[start_index]
        if np.linalg.norm(a-current_state[:3]) < 0.05:
            return b, start_index
        start_index = (start_index + 1) % len(anchors) #ç»§ç»­æŸ¥æ‰¾
    return None, -1

def show_case(env, lo:my_low_sac.HIRO_LOW_SAC):
    for _ in range(5):
        anchors = generate_anchors(env, 1)

        state, _ = env.reset()
        lo_step_cnt = 0  # ç”¨æ¥å†³å®šlo episodeçš„èµ·æ­¢
        lo_desired = None
        lo_done = True
        print("begin a trajectory...")

        for i in range(Config.max_episode_steps):  # ä¸€ä¸ªå¤§å›åˆæœ€å¤šä¸ç¯å¢ƒäº¤äº’xxæ¬¡

            if lo_done:  # low episode ç»“æŸäº†ï¼Œæˆ–è€…æ²¡æœ‰å¼€å§‹
                lo_desired, anchor_idx = get_lo_desired(state, anchors)  # çœ‹çœ‹æœ‰æ²¡æœ‰åˆé€‚çš„é”šç‚¹ç”¨ä½œä¸‹ä¸€ä¸ªä½å±‚ç›®æ ‡
                if lo_desired is not None:
                    lo_done = False  # æ ‡è¯†ä½å±‚å›åˆæ˜¯å¦ç»“æŸ
                    lo_step_cnt = 0  # å¼€å§‹è®¡æ­¥
                    print(f"\ttry sub gaol {lo_desired}...")

            if lo_desired is not None:
                state = modify_desired_in_state(state, lo_desired)  # ä¿®æ”¹
            # é€‰æ‹©åŠ¨ä½œ
            action = lo.select_action(state)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, env_reward, term, trunc, _ = env.step(action)
            # ç¡®ä¿å†™å…¥bufferçš„æ•°æ®ä¸¥æ ¼ç»Ÿä¸€,å› ä¸ºupdateçš„æ—¶å€™ä¼šç”¨åˆ°ã€‚
            if lo_desired is not None:
                next_state = modify_desired_in_state(next_state, lo_desired)
            done = term or trunc
            lo_step_cnt += 1

            # å¯èƒ½å‡ºç°ä½å±‚å·²ç»å®Œæˆäº†ç›®æ ‡ï¼Œä½†ä½å±‚çš„å›åˆé•¿åº¦è¿˜æ²¡æœ‰åˆ°æ¢æ–°ç›®æ ‡çš„æ—¶å€™ã€‚
            # è¿™ç§æƒ…å†µä¸‹ï¼Œç»§ç»­ä¸ç¯å¢ƒäº¤äº’ï¼Œä½†æ˜¯ä¸å†è®¡ç®—å†…éƒ¨å¥–åŠ±ã€ä¸è®°å½•ä½å±‚çš„æ—¶é—´æ­¥ä¿¡æ¯
            # å¦‚æœå½“å‰lo episodeè¿˜æ²¡æœ‰ç»“æŸï¼Œé‚£ä¹ˆå°±è¦è®¡ç®—å†…éƒ¨å¥–åŠ±ã€ç¡®å®šæ˜¯å¦ç»“æŸã€å­˜å‚¨æ—¶é—´æ­¥
            if not lo_done:  # low episode è¿›è¡Œä¸­
                lo_rw, lo_done, dist = intrinsic_reward(lo_desired, next_state)
                lo_done = (lo_done or done or (lo_step_cnt % Config.new_g_interval == 0))  # ä½å±‚å›åˆæˆªæ–­äº†,lo_doneä¹Ÿå¿…é¡»è®¾ç½®ä¸ºTrue

                if lo_done:
                    # å›ºå®šé•¿åº¦çš„lo episodeç»“æŸäº†ï¼Œ ä¸»è¦æ˜¯ä¸ŠæŠ¥æ˜¯å¦æˆåŠŸã€å†…éƒ¨å¥–åŠ±çš„å‡å€¼
                    if lo_rw >= 0:
                        print(f"\tsub goal {lo_desired} reached!")
                    else:
                        print(f"\tsub goal {lo_desired} missed!")
                        break

            # æ›´æ–°çŠ¶æ€
            state = next_state
            if done:
                break




def pretrain_low_policy(env, lo:my_low_sac.HIRO_LOW_SAC):
    lo_episode_cnt = 0 #ä½å±‚å›åˆä¸ªæ•°ï¼Œæ–¹ä¾¿tbä¸ŠæŠ¥åšæ¨ªåæ ‡
    lo_result=deque(maxlen=100)
    anchors = generate_anchors(env)
    total_steps = 0
    sample_num = defaultdict(int) #è®°å½•æ¯ä¸€ä¸ªlo_desireä¸ºç›®æ ‡çš„å›åˆçš„æ¬¡æ•°
    sample_suc = defaultdict(int) #è®°å½•æ¯ä¸€ä¸ªlo_desireä¸ºç›®æ ‡çš„å›åˆçš„æˆåŠŸæ¬¡æ•°
    for episode in range(1, Config.pretrain_lo_episodes):
        state, _ = env.reset()
        lo_step_cnt = 0 # ç”¨æ¥å†³å®šlo episodeçš„èµ·æ­¢
        lo_desired = None
        lo_done = True

        for i in range(Config.max_episode_steps):  # ä¸€ä¸ªå¤§å›åˆæœ€å¤šä¸ç¯å¢ƒäº¤äº’xxæ¬¡

            if lo_done: # low episode ç»“æŸäº†ï¼Œæˆ–è€…æ²¡æœ‰å¼€å§‹
                lo_desired, anchor_idx = get_lo_desired(state, anchors)  # çœ‹çœ‹æœ‰æ²¡æœ‰åˆé€‚çš„é”šç‚¹ç”¨ä½œä¸‹ä¸€ä¸ªä½å±‚ç›®æ ‡
                if lo_desired is not None:
                    sample_num[anchor_idx] += 1
                    lo.writer.add_scalar('lo/lo_desired', anchor_idx, total_steps)
                    lo_episode_rewards = []  # ä½å±‚ä¸€ä¸ªå›åˆæ¯ä¸ªæ—¶é—´æ­¥çš„å†…éƒ¨å¥–åŠ±
                    lo_done = False  # æ ‡è¯†ä½å±‚å›åˆæ˜¯å¦ç»“æŸ
                    lo_step_cnt = 0 # å¼€å§‹è®¡æ­¥


            if lo_desired is not None:
                state = modify_desired_in_state(state, lo_desired) # ä¿®æ”¹
            # é€‰æ‹©åŠ¨ä½œ
            action = lo.select_action(state)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, env_reward, term, trunc, _ = env.step(action)
            # ç¡®ä¿å†™å…¥bufferçš„æ•°æ®ä¸¥æ ¼ç»Ÿä¸€,å› ä¸ºupdateçš„æ—¶å€™ä¼šç”¨åˆ°ã€‚
            if lo_desired is not None:
                next_state = modify_desired_in_state(next_state, lo_desired)
            done = term or trunc
            lo_step_cnt += 1
            total_steps += 1

            # å¯èƒ½å‡ºç°ä½å±‚å·²ç»å®Œæˆäº†ç›®æ ‡ï¼Œä½†ä½å±‚çš„å›åˆé•¿åº¦è¿˜æ²¡æœ‰åˆ°æ¢æ–°ç›®æ ‡çš„æ—¶å€™ã€‚
            # è¿™ç§æƒ…å†µä¸‹ï¼Œç»§ç»­ä¸ç¯å¢ƒäº¤äº’ï¼Œä½†æ˜¯ä¸å†è®¡ç®—å†…éƒ¨å¥–åŠ±ã€ä¸è®°å½•ä½å±‚çš„æ—¶é—´æ­¥ä¿¡æ¯
            # å¦‚æœå½“å‰lo episodeè¿˜æ²¡æœ‰ç»“æŸï¼Œé‚£ä¹ˆå°±è¦è®¡ç®—å†…éƒ¨å¥–åŠ±ã€ç¡®å®šæ˜¯å¦ç»“æŸã€å­˜å‚¨æ—¶é—´æ­¥
            if not lo_done: #low episode è¿›è¡Œä¸­
                lo_rw, lo_done, dist = intrinsic_reward(lo_desired, next_state)
                lo_done = (lo_done or done or (lo_step_cnt % Config.new_g_interval == 0) ) # ä½å±‚å›åˆæˆªæ–­äº†,lo_doneä¹Ÿå¿…é¡»è®¾ç½®ä¸ºTrue
                # å­˜å‚¨transition
                lo.replay_buffer.push(state, action, lo_rw, next_state, lo_done)

                #print(f"add transition:{state}, {action}, {lo_rw}, {next_state}, {lo_done}")
                # æ›´æ–°ç½‘ç»œå‚æ•°
                lo.update_parameters()
                assert lo_episode_rewards is not None, ""
                lo_episode_rewards.append( lo_rw)
                if lo_done:
                    # å›ºå®šé•¿åº¦çš„lo episodeç»“æŸäº†ï¼Œ ä¸»è¦æ˜¯ä¸ŠæŠ¥æ˜¯å¦æˆåŠŸã€å†…éƒ¨å¥–åŠ±çš„å‡å€¼
                    if lo_rw >= 0:
                        lo_result.append(1)
                        sample_suc[anchor_idx] += 1
                    else:
                        lo_result.append(0)
                    lo.writer.add_scalar('lo/avg_intrinsic_reward', np.mean(lo_episode_rewards), episode)

            # æ›´æ–°çŠ¶æ€
            state = next_state
            if done:
                break

        lo.writer.add_scalar('lo/suc_ratio', np.mean(lo_result), episode)
        if episode % 200 == 0:
            total = 1e-7
            for k, c in sample_num.items():
                total += c
            print(f"\nsamples distribution,total={int(total)}:")
            for k, c in sample_num.items():
                print(f"{k}:\t{c},\t{c/total:.2f},\t{(sample_suc[k]*100 / (c+1e-8)):.1f}%")
    torch.save(lo.actor, './checkpoints/low_sac_actor.pth')
    show_case(env, lo)

# ä¸»å‡½æ•°
def main():
    # åˆ›å»ºç¯å¢ƒ
    env = my_fetchreach_env.CustomFetchReachEnv()
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    max_action = float(env.action_space.high[0])
    print(f"state_dim:{state_dim}, action_dim:{action_dim}, max_action:{max_action}")

    writer = SummaryWriter(log_dir=f'logs/HIRO_FetchReach_{datetime.datetime.now().strftime("%m%d_%H%M%S")}')
    # åˆ›å»ºSACä»£ç†
    hi = my_hi_sac.HIRO_HI_SAC(state_dim, 3, 1, writer) # é«˜å±‚ç­–ç•¥è¾“å‡ºçš„æ˜¯g,ç›¸å¯¹äºå½“å‰çš„ä½ç½®çš„xyzåç§»é‡ï¼Œå‡è®¾åç§»é‡æœ€å¤š1ç±³
    lo = my_low_sac.HIRO_LOW_SAC(state_dim, action_dim, max_action, writer)

    # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs("checkpoints", exist_ok=True)

    pretrain_low_policy(env, lo)



if __name__ == '__main__':
    main()
```