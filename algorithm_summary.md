**各种RL算法的对比记忆**

### 关键的算法

在CleanRL SB3等库里出现过的算法，可以认为是比较重要的DRL算法

| 算法名    | 动作空间            | on/off-policy        | 架构                                                         | 优劣势                                                       | 主要思路                                                     | 探索方法                |
| --------- | ------------------- | -------------------- | :----------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ----------------------- |
| DQN       | 离散 动<br />作空间 | off                  | Q价值网络 / replaybuffer / TD 法                             | 简单、样本效率高、基础性算法                                 | 两个网络：使用main网络进行动作选择和计算当前Q值，使用target网络计算TD目标<br />replay buffer提高样本效率。 | ε-greedy                |
| PQN       | 离散                | on                   | Q价值网络 / 多环境并发                                       | DQN的变种，号称比DQN训练更快                                 | 在DQN的基础上去掉target网络和replay buffer，增加LayNorm层稳定训练，使用并发多环境，变为on-policy | ε-greedy                |
| REINFORCE | 离散 / 连续         | on                   | 策略网络 / MC 方法                                           | MC相比TD，无偏但方差大。基础性算法                           | 仅依赖策略网络（无价值函数） 且使用 蒙特卡洛方法（MC） 进行训练 | 动作概率抽样            |
| A2C       | 离散 / 连续         | on                   | Actor-Critic架构，Advantage函数                              | TD比MC更稳定，方差小但有偏                                   | 优势函数 A(s,a) 替代原始回报，用来衡量动作 *a* 相对于平均水平的优势，通常用TD误差来近似 | 动作概率抽样 / 熵正则化 |
| PPO       | 离散 / 连续         | on                   | Actor-Critic架构，Advantage函数                              | 比A2C有更好的稳定性和样本效率                                | 在A2C的基础上，通过重要性采样 / 值裁剪 / KL散度控制更新步长，提高样本的利用率。<br/>通常包括三个步骤：用多个环境并行收集固定步数的数据，计算GAE，分小批次多次更新。 | 动作概率抽样 / 熵正则化 |
| **RPO**   | 连续                | on                   | 同PPO                                                        | 在 PPO 的框架上增强了探索能力和鲁棒性，更适合机器人控制      | 在PPO的基础上，在更新网络的环节，对策略网络输出的均值加上一个噪音（轨迹收集阶段不加噪音），噪音可以是正态分布或者其他分布 | 动作加噪音              |
| RND       | 离散/连续           | 取决于搭载的基础算法 | 取决于搭载的基础算法                                         | RND 通过衡量状态的新颖性（Novelty）生成内部奖励（Intrinsic Reward），鼓励智能体探索未知状态，解决稀疏奖励问题。 | 在基础RL算法之外，增加两个神经网络：目标网络（固定参数不训练）和预测网络，把一个状态都输入给这两个网络，他们的输出向量的MSE损失作为内部奖励。 | 取决于基础算法          |
| DDPG      | 连续                | off                  | 四个网络Q、μ，Q', μ'，后面两个仅用作生成TD target，并采取soft更新他们 | 结合DQN的技巧（目标网络、Replay Buffer）提升稳定性。对超参数非常敏感不稳定。 | 1.通过replay buffer来消除样本之间的强相关性，使用异策略训练动作价值网络 <br />2.使用延迟更新的target Q network来提供稳定的标注价值（有监督学习中的target）用于TD <br />3.使用了深度学习中常用的mini-batch方式来训练<br />4.μ输出确定的动作值，通过**在动作上添加噪声**（如OU噪声或高斯噪声）实现探索，而非策略本身随机 | 动作加噪音              |
| TD3       | 连续                | off                  | 五个网络Q1、Q2、μ、Q1'、Q2‘                                  | 相比DDPG解决了Q值高估问题、训练更稳定                        | 在DDPG基础上<br />1.引入双Q，解决高估问题。<br />2.延迟更新策略网络<br />3.对目标动作添加**高斯噪声**，再输入目标Q网络，避免Q网络对动作的过拟合 | 动作加噪音              |
| SAC       | 连续                | off                  | 类似TD3还是五个网络，策略网络输出是不确定的动作分布（均值与标准差） | 自动调节探索、在奖励稀疏和高维空间表现更稳定                 | 在TD3的基础上<br />1.策略网络输出是不确定的动作分布，不需要手动加噪声<br />2.引入熵正则和自动学习的熵正则项的参数 | 动作抽样 / 熵正则化     |
| C51       | 离散                | off                  | 在DQN的基础上，把网络输出的每个动作对应的一个标量值，变成了一个分布：51个桥墩描述概率密度函数的值。 | 在理论层面提升了强化学习的表达能力，尤其在复杂、不确定环境中显著优于DQN。但其计算成本较高，若任务简单（如低维控制），传统DQN仍具实用性 | 把DQN输出的每个动作对应的一个标量值，变成了一个分布。然后选择**期望值最大**的动作。C51核心贡献是**将值函数从标量推广到分布**，但动作空间的离散性仍是其本质约束 | ε-greedy / 分布化Q值    |
| PPG       | 离散 / 连续         | on                   | 基于actor-critic架构，actor和critic两部分之间不共享特征提取的CNN或者MLP。actor自带一个辅助的价值头 | 解耦了Actor-Critic更新的时候相互影响，更加稳定；且在辅助阶段可以更充分的利用样本，所以有更高的样本效率。 | PPG的actor和critic两部分之间不共享特征提取的CNN或者MLP：  第一部分：Policy网络（带了一个辅助价值头），可以认为就是一个标准的Actor-Critic网络 第二部分：独立的Critic网络 <br />训练分两阶段：  第一阶段，Policy Phase，用GAE方法分别训练Policy网络和Critic网络 第二阶段，Auxiliary Phase，专门训练Policy网络里的 auxiliary value head和特征提取部分。 | 动作概率抽样 / 熵正则化 |
| HER       | 连续                | off                  | HER是附加套件，通过修改ReplayBuffer进行relabel。可以和任何使用ReplayBuffer的off-policy算法搭配 | 对于奖励稀疏的场景有明显提升                                 | 通常的off-policy方法是训练在确定目标g 下的策略 Pi(state) --> action，而对于HER训练的策略，输入参数不只有state，还有目标g'，也就是HER训练策略 Pi(state, g')-->action，通过深度神经网络的泛化能力，策略网络能学会不同目标下的行为映射。训练收敛后，输入desired goal的时候，策略网络能够指挥agent执行正确的动作，到达desired goal | 取决于基础算法          |
| QDagger   | 离散/连续           | off                  | 取决于基础算法                                               | 重生强化学习，尽量利用已经训练的网络参数或者已经收集的环境交互数据，来加速训练 | 通过蒸馏teacher模型（例如让学生模型输出的动作概率分布尽量逼近teacher的输出）、复用teacher与环境交互得到的样本数据、并逐步减少teacher的参与并最终断奶 | 取决于基础算法          |



### 按场景分类

#### 大语言模型相关的RL

预训练模型仅学习统计规律，缺乏对人类偏好（如安全、有用、无害）的显式对齐。预训练模型通过海量语料学习的本质是**"下一个token预测"的统计建模**，它捕捉的是文本表面的共现规律（如"巴黎-法国"的关联），而非人类期望的**价值对齐**（如事实准确、无害、有用）。

RLHF/DPO通过人类反馈直接优化模型行为，使其更符合实际需求，避免生成低质或有害内容，提升实用性和安全性。

预训练模型的训练过程属于自监督学习，因为正确答案（下一个token）来自数据本身，而非外部标注。这就像人类通过阅读自学语言，而非依赖老师批改作业。后续的SFT或RLHF才引入有监督学习（人类标注数据）进行对齐。这种两阶段设计是大语言模型成功的核心：先通过无监督获得“语言能力”，再通过有监督/强化学习获得“人类偏好”。

| 算法名 | 架构                                                         | 优劣势                                                       | 主要思路                                                     |
| ------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| RLHF   | RewardModel+PPO                                              | 可以使得大模型更好的遵循人类的意图，对人类更有实质性的帮助；更好的生成真实的答案 减少毒性输出；有很好的泛化能力，没有参与微调标注工作的其他人也认为微调后的大模型的输出更符合他们的需要 | step1:使用人工标注的数据有监督学习训练LLM<br />step2：使用对比偏好数据偶监督学习训练一个RM<br />step3：基于自训练+RM，进行强化学习训练，且把每一个token的生成都看作一个bandit回合，即长度为1的回合 |
| GRPO   | RewardModel+REINFORCE                                        | 相比RLHF，去掉了value网络，占用更少的内存（value网络通常也是一个大模型）和算力，要训练的参数也更少了。 | 在RLHF基础上，去掉了value函数网络，改用同一个问题（状态）的一组不同回答（action）的奖励的平均值，作为该状态的value。 |
| DPO    | DPO 的架构没有额外的RL模型（没有RM，没有value网络），整个流程直接用偏好数据，基于现有 LLM 做普通的梯度下降。 | 相比RLHF，简单高效、样本利用率高、收敛快速且稳定；<br />劣势是只能处理偏好对，回答哪一个更好，不能多个回答绝对排序；缺少RL的探索、最大回报等机制，可能会陷入次优解 | 收集用户偏好数据：给定一个问题，收集多个回答，并记录用户更喜欢哪个回答（偏好对）；不用额外训练 reward model，不用 KL 约束。；直接用一个特殊设计的 偏好对数似然目标函数 来 fine-tune LLM，通过普通的有监督梯度下降优化。 |



#### 奖励稀疏场景的算法

| 算法名            | 动作空间    | on/off-policy        | 架构                                                         | 优劣势                                                       | 主要思路                                                     | 探索方法             |
| ----------------- | ----------- | -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------- |
| ICM               | 离散 / 连续 | 取决于搭载的基础算法 | 取决于搭载的基础算法                                         | 奖励稀疏的场景。相比RND，ICM**只关注与Agent动作相关的环境变化**，忽略无关因素（如风吹树叶、光照变化等） | ICM通过**预测环境动态的变化**来生成内在奖励，其核心是：<br />**只关注与Agent动作相关的环境变化**，忽略无关因素（如风吹树叶、光照变化等）。 <br />通过两个子模型实现：  **逆动力学模型（Inverse Model）**：学习从状态变化中反推动作（即“当前状态 + 下一状态 → 动作”）。 **前向动力学模型（Forward Model）**：预测下一状态的特征（即“当前状态 + 动作 → 预测下一状态”）。<br />**内在奖励**定义为前向模型的预测误差（预测状态与实际状态的差异）。误差越大，说明当前状态-动作对越“新奇”。 | 取决于搭载的基础算法 |
| HER               | 连续        | off                  | HER是附加套件，通过修改ReplayBuffer进行relabel。可以和任何使用ReplayBuffer的off-policy算法搭配 | 对于奖励稀疏的场景有明显提升                                 | 通常的off-policy方法是训练在确定目标g 下的策略 Pi(state) --> action，而对于HER训练的策略，输入参数不只有state，还有目标g'，也就是HER训练策略 Pi(state, g')-->action，通过深度神经网络的泛化能力，策略网络能学会不同目标下的行为映射。训练收敛后，输入desired goal的时候，策略网络能够指挥agent执行正确的动作，到达desired goal | 取决于基础算法       |
| RND               | 离散/连续   | 取决于搭载的基础算法 | 取决于搭载的基础算法                                         | RND 通过衡量状态的新颖性（Novelty）生成内部奖励（Intrinsic Reward），鼓励智能体探索未知状态，解决稀疏奖励问题。 | 在基础RL算法之外，增加两个神经网络：目标网络（固定参数不训练）和预测网络，把一个状态都输入给这两个网络，他们的输出向量的MSE损失作为内部奖励。 | 取决于基础算法       |
| hDQN等各种HRL算法 | 离散        | off                  | 两层DQN                                                      | 能处理奖励稀疏的问题，两层架构更具解释性、更好的学习效率、能处理更复杂的任务。劣势就在于复杂性较高、子目标设计困难。 | 分层组织的深度强化学习模块在不同的时间尺度上工作。模型在两个层次上做出决策<br />1.顶层模块（元控制器）接受状态并选择一个新目标<br />2.低层模块（控制器）使用状态和选择的目标来选择动作，直到达到目标或回合终止。然后元控制器选择另一个目标并重复这两个步骤。 | ε-greedy             |

#### 模仿学习和离线RL

| 算法名     | 动作空间  | offline RL？ | 架构                                          | 优劣势                                                       | 主要思路                                                     | 狭义 IL？ |
| ---------- | --------- | ------------ | --------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | --------- |
| GAIL       | 连续/离散 | online       | 判别器+各种基于策略的RL算法                   | 相比典型的逆向RL，跳过了reward函数建模过程，且适合reward稀疏的场景。劣势是样本利用率偏低，必须在线训练持续与环境交互。训练不稳定对参数敏感。 | 使用专家数据和待训练策略的交互数据训练判别器（一个二分类器），利用判别器的输出作为奖励，RL方法训练策略网络。 | N         |
| AIRL       | 连续/离散 | online       | 判别器+各种基于策略的RL算法                   | 相比GAIL，能恢复出reward函数且跨任务可迁移，是真正的逆向RL。缺点是训练开销大，实现复杂。 | 使用专家数据和待训练策略的交互数据训练判别器（一个二分类器），利用判别器的输出作为奖励，RL方法训练策略网络。<br />D的架构要内化reward函数。而GAIL最后判别器的输出概率期望是0.5，不是稳定的Reward | N         |
| TD3+BC     | 连续      | offline      | TD3的架构：五个网络Q1、Q2、μ、Q1'、Q2‘        | 简单好实现，对原有算法侵入极小。离线训练不需要与环境交互，训练效率和样本利用率非常高。劣势就是对离线数据的分布质量要求很高，行为者本身的性能是上限无法突破 | 训练过程完全不需要与环境交互，只使用离线数据。<br />对TD3策略网络的原有loss加上行为克隆的损失项，让策略模仿专家行为 | N         |
| DAgger     | 连续/离散 | online       | 取决于基础RL算法                              | 简单好实现，样本效率高，与reward无关，适合reward稀疏或者缺失的场景。劣势就是对专家有依赖，专家的能力是其性能上限。 | DAgger方法就是：N轮训练，每轮分为 与环境交互、专家标注出现状态的action、有监督学习三步；与环境交互，第一轮是专家完成的，后面减少专家交互的比例（各种衰减方法）；标注训练数据，训练数据里每个状态下应该出现什么动作，都是专家标注的；而训练数据里有哪些状态，主要是学生模型探索出来的；有监督学习，典型的分类问题（离散动作空间）或者回归问题（连续动作空间），训练的目标就是动作逼近专家，与奖励无关，与RL无关 | Y         |
| BCQ        | 连续/离散 | offline      | Actor-Critic架构，外加VAE网络和扰动网络       | 第一个离线RL算法，完全不需要与环境交互。可以有效避免外推误差，样本利用率高，支持连续和离散动作空间。劣势是实现比较复杂，不能应对奖励稀疏的场景，推理效率低，计算复杂 | BCQ通过限制动作落在离线数据的分布内，强制 agent 行为尽量接近批数据中的行为。<br />VAE网络负责输出在状态s下的采样动作，该采样动作在离线数据的分布范围内（通过有监督学习训练VAE获得这个能力），扰动网络（actor）对每个采样得到的动作施加扰动，得到扰动后的动作actions，Q网络对（state，actions）评估价值，取价值最大的动作作为最终动作。 | N         |
| CQL        | 连续/离线 | offline      | Q函数或者Actor-Critic架构，没有额外的网络组件 | 能够有效的抑制离线数据外的动作，减少估值错误，避免灾难性行为，可应用在离散或者连续动作空间，实施简单对原有算法侵入小。劣势是训练开销大，可能由于过度保守而导致收敛慢 | CQL通过对Q函数或者其他Actor-Critic算法中的Q函数的更新加上正则项，惩罚所有可能动作的Q值，防止高估OOD动作，对离线经验数据内的动作进行保护，允许它们Q值较高。这样就达到了让Q值更保守、避免对分布以外的动作的高估。 | N         |
| AggreVaTeD | 连续/离线 | online       | 基于策略的RL                                  | 训练的策略能力可能超过专家，劣势是只能用于基于策略的RL，且依赖专家给出Q值而不只是动作，计算复杂度较高，因为可能需要根据经验信息拟合出Q值 | 利用专家给的信息Q(s,a)，将模仿学习问题转化为一个策略梯度优化问题，从而在深度模型上高效训练。它通过交替地采样当前策略和查询专家，使用梯度下降更新策略，使模型在专家不可用的测试时仍能做出良好决策。步骤上类似类似DAgger | N         |

注：

1. 是否是狭义 IL，我这里的判断标准是：是否用奖励函数，是否用贝尔曼误差、策略梯度等RL方法最大化回报，如果用了，归结为RL而不是 IL
2. 是否是offline RL，我这里的判断标准是：训练过程是否需要与环境交互（不算前面收集经验数据的过程）

#### 层级强化学习

层级强化学习，我觉得它很重要的一个思想就是：让高层策略从更高级的语义、更 粗的粒度、更宏观的视角发出动作，而不陷入环境给出的微观动作和状态观察里，让agent做到胸中有丘壑，而不是云深不知处。 低层策略也很关键，他是上层策略的执行者和子目标的达成者，它是基础。

| 算法名        | 动作空间  | on/off-policy | 架构                                                         | 优劣势                                                       | 主要思路                                                     | 探索方法       |
| ------------- | --------- | ------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------- |
| HIRO          | 离散/连续 | off           | 取决于搭载的基础算法                                         | 设计通用、样本效率高，能应对复杂或奖励系数的场景；劣势：要求状态空间必须是可加减的，使得子目标的语义有限，子目标重新标定的计算量巨大 | 两层策略，使用（阶段性状态 - 起始状态）作为子目标，off-policy方式同时训练两层策略，并使用概率分数高的子目标修正replaybuffer中的经验数据 | 取决于基础算法 |
| hDQN          | 离散      | off           | 两层DQN                                                      | 能处理奖励稀疏的问题，两层架构更具解释性、更好的学习效率、能处理更复杂的任务。劣势就在于复杂性较高、子目标设计困难。 | 分层组织的深度强化学习模块在不同的时间尺度上工作。模型在两个层次上做出决策<br />1.顶层模块（元控制器）接受状态并选择一个新目标<br />2.低层模块（控制器）使用状态和选择的目标来选择动作，直到达到目标或回合终止。然后元控制器选择另一个目标并重复这两个步骤。 | ε-greedy       |
| FuN           | 离散/连续 | on / off      | 两层架构，manager+worker，他们都采取类REINFORCE 的训练方式   | credit assignment更清晰；方向性目标更具泛化能力；结构上具有通用性；劣势：潜在状态空间可解释性较差；训练难度大；manager给出的目标通常是短程的方向，没有构建长程plan。**实现复杂，我搞不定，不推荐** | 非常类似HIRO，但不是可插拔的而是独立的一套算法，高层给出潜在状态的变化方向作为子目标，低层沿着指定的方向进行动作。损失函数中用到余弦相似度来度量方向的一致性。 | action概率采样 |
| Option-Critic | 离散/连续 | on / off      | 两层架构，插件化，高层可以是策略网络或者价值网络，选择option，低层可以是PPO或者TD3这样的算法 | 不需要手工定义选项的目标或子任务，算法可以自动学出有意义的高层动作；适用于离散和连续状态/动作空间，插件化；**损失函数等公式很复杂很费解，我看不懂，不推荐。** | 一个option相当于一个工具，由策略网络、终止函数、Q价值网络构成。高层策略每次选择一个option，根据其策略网络选择动作与环境互动，终止函数决定要不要结束该option，如果终止高层就重新选择option | 取决于基础算法 |