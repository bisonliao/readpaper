**各种RL算法的对比记忆**

### 关键的算法

在CleanRL SB3等库里出现过的算法，可以认为是比较重要的DRL算法

| 算法名    | 动作空间            | on/off-policy        | 架构                                                         | 优劣势                                                       | 主要思路                                                     | 探索方法                |
| --------- | ------------------- | -------------------- | :----------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ----------------------- |
| DQN       | 离散 动<br />作空间 | off                  | Q价值网络 / replaybuffer / TD 法                             | 简单、样本效率高、基础性算法                                 | 两个网络：使用main网络进行动作选择和计算当前Q值，使用target网络计算TD目标<br />replay buffer提高样本效率。 | ε-greedy                |
| PQN       | 离散                | on                   | Q价值网络 / 多环境并发                                       | DQN的变种，号称比DQN训练更快                                 | 在DQN的基础上去掉target网络和replay buffer，增加LayNorm层稳定训练，使用并发多环境，变为on-policy | ε-greedy                |
| REINFORCE | 离散 / 连续         | on                   | 策略网络 / MC 方法                                           | MC相比TD，无偏但方差大。基础性算法                           | 仅依赖策略网络（无价值函数） 且使用 蒙特卡洛方法（MC） 进行训练 | 动作概率抽样            |
| A2C       | 离散 / 连续         | on                   | Actor-Critic架构，Advantage函数                              | TD比MC更稳定，方差小但有偏                                   | 优势函数 A(s,a) 替代原始回报，用来衡量动作 *a* 相对于平均水平的优势，通常用TD误差来近似 | 动作概率抽样 / 熵正则化 |
| PPO       | 离散 / 连续         | on                   | Actor-Critic架构，Advantage函数                              | 比A2C有更好的稳定性和样本效率                                | 在A2C的基础上，通过重要性采样 / 值裁剪 / KL散度控制更新步长，提高样本的利用率。<br/>通常包括三个步骤：用多个环境并行收集固定步数的数据，计算GAE，分小批次多次更新。 | 动作概率抽样 / 熵正则化 |
| **RPO**   | 连续                | on                   | 同PPO                                                        | 在 PPO 的框架上增强了探索能力和鲁棒性，更适合机器人控制      | 在PPO的基础上，在更新网络的环节，对策略网络输出的均值加上一个噪音（轨迹收集阶段不加噪音），噪音可以是正态分布或者其他分布 | 动作加噪音              |
| RND       | 离散/连续           | 取决于搭载的基础算法 | 基于但不限于 Actor-Critic，RND是附加的通用模块               | RND 通过衡量状态的新颖性（Novelty）生成内部奖励（Intrinsic Reward），鼓励智能体探索未知状态，解决稀疏奖励问题。 | 在Actor-Critic架构的基础上，增加两个神经网络：目标网络（固定参数不训练）和预测网络，把一个状态都输入给这两个网络，他们的输出向量的MSE损失作为内部奖励。 | 取决于基础算法          |
| DDPG      | 连续                | off                  | 四个网络Q、μ，Q', μ'，后面两个仅用作生成TD target，并采取soft更新他们 | 结合DQN的技巧（目标网络、Replay Buffer）提升稳定性。对超参数非常敏感不稳定。 | 1.通过replay buffer来消除样本之间的强相关性，使用异策略训练动作价值网络 <br />2.使用延迟更新的target Q network来提供稳定的标注价值（有监督学习中的target）用于TD <br />3.使用了深度学习中常用的mini-batch方式来训练<br />4.μ输出确定的动作值，通过**在动作上添加噪声**（如OU噪声或高斯噪声）实现探索，而非策略本身随机 | 动作加噪音              |
| TD3       | 连续                | off                  | 五个网络Q1、Q2、μ、Q1'、Q2‘                                  | 相比DDPG解决了Q值高估问题、训练更稳定                        | 在DDPG基础上<br />1.引入双Q，解决高估问题。<br />2.延迟更新策略网络<br />3.对目标动作添加**高斯噪声**，再输入目标Q网络，避免Q网络对动作的过拟合 | 动作加噪音              |
| SAC       | 连续                | off                  | 类似TD3还是五个网络，策略网络输出是不确定的动作分布（均值与标准差） | 自动调节探索、在奖励稀疏和高维空间表现更稳定                 | 在TD3的基础上<br />1.策略网络输出是不确定的动作分布，不需要手动加噪声<br />2.引入熵正则和自动学习的熵正则项的参数 | 动作抽样 / 熵正则化     |
| C51       | 离散                | off                  | 在DQN的基础上，把网络输出的每个动作对应的一个标量值，变成了一个分布：51个桥墩描述概率密度函数的值。 | 在理论层面提升了强化学习的表达能力，尤其在复杂、不确定环境中显著优于DQN。但其计算成本较高，若任务简单（如低维控制），传统DQN仍具实用性 | 把DQN输出的每个动作对应的一个标量值，变成了一个分布。然后选择**期望值最大**的动作。C51核心贡献是**将值函数从标量推广到分布**，但动作空间的离散性仍是其本质约束 | ε-greedy / 分布化Q值    |
| PPG       | 离散 / 连续         | on                   | 基于actor-critic架构，actor和critic两部分之间不共享特征提取的CNN或者MLP。actor自带一个辅助的价值头 | 解耦了Actor-Critic更新的时候相互影响，更加稳定；且在辅助阶段可以更充分的利用样本，所以有更高的样本效率。 | PPG的actor和critic两部分之间不共享特征提取的CNN或者MLP：  第一部分：Policy网络（带了一个辅助价值头），可以认为就是一个标准的Actor-Critic网络 第二部分：独立的Critic网络 <br />训练分两阶段：  第一阶段，Policy Phase，用GAE方法分别训练Policy网络和Critic网络 第二阶段，Auxiliary Phase，专门训练Policy网络里的 auxiliary value head和特征提取部分。 | 动作概率抽样 / 熵正则化 |
| HER       | 连续                | off                  | HER是附加套件，通过修改ReplayBuffer进行relabel。可以和任何使用ReplayBuffer的off-policy算法搭配 | 对于奖励稀疏的场景有明显提升                                 | 通常的off-policy方法是训练在确定目标g 下的策略 Pi(state) --> action，而对于HER训练的策略，输入参数不只有state，还有目标g'，也就是HER训练策略 Pi(state, g')-->action，通过深度神经网络的泛化能力，策略网络能学会不同目标下的行为映射。训练收敛后，输入desired goal的时候，策略网络能够指挥agent执行正确的动作，到达desired goal | 取决于基础算法          |
| QDagger   | 离散/连续           | off                  | 取决于基础算法                                               | 重生强化学习，尽量利用已经训练的网络参数或者已经收集的环境交互数据，来加速训练 | 通过蒸馏teacher模型（例如让学生模型输出的动作概率分布尽量逼近teacher的输出）、复用teacher与环境交互得到的样本数据、并逐步减少teacher的参与并最终断奶 | 取决于基础算法          |



### 大语言模型相关的RL





### 其他RL算法