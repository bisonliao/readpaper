HER.md:#### FetchReach
HER.md:- 动作是一个 **4维连续向量**：前三维是 `Δx, Δy, Δz`（末端位置的增量），第4维是 `gripper` 的开合指令（在 FetchReach 中通常会被忽略）。
HER.md:        env = gym.make("FetchReach-v3")
HER.md:    # 启动 4 个 FetchReach 环境并向量化
HER.md:    test_env = gym.make("FetchReach-v3", render_mode="human")  # human 模式可视化
HER.md:    env = gym.make('FetchReach-v3', render_mode=None, reward_type="sparse")
HER.md:6. 我还试了一下[这个开源项目](https://github.com/DLR-RM/rl-baselines3-zoo)运行命令行训练，但是发现各种报错，错误解决不完。 python train.py --algo sac  --env FetchReach-v1
HER.md:        env = gym.make("FetchReach-v3")
HER.md:    # 启动 4 个 FetchReach 环境并向量化
HIRO.md:#### FetchReach任务
HIRO.md:class CustomFetchReachEnv(gym.Env):
HIRO.md:    自定义封装 FetchReach-v3 环境，符合 Gymnasium 接口规范。
HIRO.md:        # 创建原始 FetchReach-v3 环境
HIRO.md:        self._env = gym.make("FetchReach-v3", render_mode=render_mode, max_episode_steps=100)
HIRO.md:    env = my_fetchreach_env.CustomFetchReachEnv()
HIRO.md:    writer = SummaryWriter(log_dir=f'logs/HIRO_FetchReach_{datetime.datetime.now().strftime("%m%d_%H%M%S")}')
HIRO.md:    env = my_fetchreach_env.CustomFetchReachEnv('human')
ICM.md:#### FetchReach 任务
ICM.md:class CustomFetchReachEnv(gym.Env):
ICM.md:    自定义封装 FetchReach-v3 环境，符合 Gymnasium 接口规范。
ICM.md:        # 创建原始 FetchReach-v3 环境
ICM.md:        self._env = gym.make("FetchReach-v3", render_mode=render_mode, max_episode_steps=100)
ICM.md:        # 获取 gripper 位置和目标位置（FetchReach 的 obs 包含这些信息）
ICM.md:    env = CustomFetchReachEnv()
ICM.md:class CustomFetchReachEnv(gym.Env):
ICM.md:    自定义封装 FetchReach-v3 环境，符合 Gymnasium 接口规范。
ICM.md:        # 创建原始 FetchReach-v3 环境
ICM.md:        self._env = gym.make("FetchReach-v3", render_mode=render_mode, max_episode_steps=100)
ICM.md:from CustomFetchReach import CustomFetchReachEnv
ICM.md:env = CustomFetchReachEnv()
ICM.md:test_env = CustomFetchReachEnv(render_mode='human')
ICM.md:torch.save(model, "./CustomFetchReach.pth")
ICM.md:class CustomFetchReachEnv(gym.Env):
ICM.md:    自定义封装 FetchReach-v3 环境，符合 Gymnasium 接口规范。
ICM.md:        # 创建原始 FetchReach-v3 环境
ICM.md:        self._env = gym.make("FetchReach-v3", render_mode=render_mode, max_episode_steps=100)
ICM.md:    env = CustomFetchReachEnv()
ICM.md:只能到40%左右的成功率，让我想起了HER算法的FetchReach任务，也是只有40%成功率
ICM.md:class CustomFetchReachEnv(gym.Env):
ICM.md:    自定义封装 FetchReach-v3 环境，符合 Gymnasium 接口规范。
ICM.md:        # 创建原始 FetchReach-v3 环境
ICM.md:        self._env = gym.make("FetchReach-v3", render_mode=render_mode, max_episode_steps=100)
ICM.md:    envs = SubprocVecEnv([lambda i=i: CustomFetchReachEnv(i+53) for i in range(n_envs)])
ICM.md:2. 然后我修改PPO代码，适配CustomFetchReachEnv环境，先不用ICM，**只使用外部的成功时刻返回的稀疏奖励**。有28%的任务成功率。但这个有偶然性，要多试几次，有时候完全没有成功的。
ICM.md:class CustomFetchReachEnv(gym.Env):
ICM.md:        self._env = gym.make("FetchReach-v3", render_mode=render_mode, max_episode_steps=100)
ICM.md:    env = CustomFetchReachEnv()
ICM.md:    env = CustomFetchReachEnv(render_mode='human')
ICM.md:    env_name="FetchReach-x"
ICM.md:    env = CustomFetchReachEnv()
PPO.md:    #env = CustomFetchReachEnv()
PPO.md:    #env = CustomFetchReachEnv()
RND.md:#### 2、FetchReach任务
RND.md:这个代码是在研究ICM算法的时候，顺便把算法替换为RND跑出来的效果。用到的开源实现是[RLeXplore](https://github.com/RLE-Foundation/RLeXplore)和SB3，我自己对FetchReach进行了二次封装，主要是转换FetchReach的状态返回格式。
RND.md:class CustomFetchReachEnv(gym.Env):
RND.md:    自定义封装 FetchReach-v3 环境，符合 Gymnasium 接口规范。
RND.md:        # 创建原始 FetchReach-v3 环境
RND.md:        self._env = gym.make("FetchReach-v3", render_mode=render_mode, max_episode_steps=100)
RND.md:    envs = SubprocVecEnv([lambda i=i: CustomFetchReachEnv(i+53) for i in range(n_envs)])
