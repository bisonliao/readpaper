### 网友整理的RL论文

| 类别               | 论文题目                                                     | 原文链接                                                     | 视频解读 |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | -------- |
| Value-based        | Playing Atari with Deep Reinforcement Learning (**DQN**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/DQN/Playing Atari with Deep Reinforcement Learning.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/DQN/PDF/Playing Atari with Deep Reinforcement Learning.pdf) | https://arxiv.org/abs/1312.5602                              |          |
|                    | **DRQN**: Deep Recurrent Q-Learning for Partially Observable MDPs [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/DQN/Deep Recurrent Q-Learning for Partially Observable MDPs.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/DQN/PDF/Deep Recurrent Q-Learning for Partially Observable MDPs.pdf) | https://arxiv.org/abs/1507.06527                             |          |
|                    | Dueling Network Architectures for Deep Reinforcement Learning (**Dueling DQN**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/DQN/Dueling Network Architectures for Deep Reinforceme.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/DQN/PDF/Dueling Network Architectures for Deep Reinforceme.pdf) | https://arxiv.org/abs/1511.06581                             |          |
|                    | Deep Reinforcement Learning with Double Q-learning (**Double DQN**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/DQN/Deep Reinforcement Learning with Double Q-learning.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/DQN/PDF/Deep Reinforcement Learning with Double Q-learning.pdf) | https://arxiv.org/abs/1509.06461                             |          |
|                    | **NoisyDQN**                                                 | https://arxiv.org/pdf/1706.10295.pdf                         |          |
|                    | QRDQN                                                        | https://arxiv.org/pdf/1710.10044.pdf                         |          |
|                    | CQL                                                          | https://arxiv.org/pdf/2006.04779.pdf                         |          |
|                    | Prioritized Experience Replay (**PER**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/DQN/Prioritized Experience Replay.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/DQN/PDF/Prioritized Experience Replay.pdf) | https://arxiv.org/abs/1511.05952                             |          |
|                    | Rainbow: Combining Improvements in Deep Reinforcement Learning (**Rainbow**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/DQN/Rainbow_Combining Improvements in Deep Reinforcement Learning.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/DQN/PDF/Rainbow_Combining Improvements in Deep Reinforcement Learning.pdf) | https://arxiv.org/abs/1710.02298                             |          |
|                    | A Distributional Perspective on Reinforcement Learning (**C51**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/A Distributional Perspective on Reinforcement Learning.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/PDF/A Distributional Perspective on Reinforcement Learning.pdf) | https://arxiv.org/abs/1707.06887                             |          |
| Policy -based      | Asynchronous Methods for Deep Reinforcement Learning (**A3C**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/Asynchronous Methods for Deep Reinforcement Learning.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/PDF/Asynchronous Methods for Deep Reinforcement Learning.pdf) | https://arxiv.org/abs/1602.01783                             |          |
|                    | Trust Region Policy Optimization (**TRPO**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/Trust Region Policy Optimization.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/PDF/Trust Region Policy Optimization.pdf) | https://arxiv.org/abs/1502.05477                             |          |
|                    | High-Dimensional Continuous Control Using Generalized Advantage Estimation (**GAE**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/High-Dimensional Continuous Control Using Generalized Advantage Estimation.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/PDF/High-Dimensional Continuous Control Using Generalised Advantage Estimation.pdf) | https://arxiv.org/abs/1506.02438                             |          |
|                    | Proximal Policy Optimization Algorithms (**PPO**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/Proximal Policy Optimization Algorithms.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/PDF/Proximal Policy Optimization Algorithms.pdf) | https://arxiv.org/abs/1707.06347                             |          |
|                    | Emergence of Locomotion Behaviours in Rich Environments (**PPO-Penalty**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/Emergence of Locomotion Behaviours in Rich Environments.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/PDF/Emergence of Locomotion Behaviours in Rich Environments.pdf) | https://arxiv.org/abs/1707.02286                             |          |
|                    | Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (**ACKTP**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/Scalable trust-region method for deep reinforcement learning using Kronecker-factored.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/PDF/Scalable trust-region method for deep reinforcement learning using Kronecker-factored.pdf) | https://arxiv.org/abs/1708.05144                             |          |
|                    | Sample Efficient Actor-Critic with Experience Replay (**ACER**) | https://arxiv.org/abs/1611.01224                             |          |
|                    | Deterministic Policy Gradient Algorithms (**DPG**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/Deterministic Policy Gradient Algorithms.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/PDF/Deterministic Policy Gradient Algorithms.pdf) | http://proceedings.mlr.press/v32/silver14.pdf                |          |
|                    | Continuous Control With Deep Reinforcement Learning (**DDPG**) | https://arxiv.org/abs/1509.02971                             |          |
|                    | Addressing Function Approximation Error in Actor-Critic Methods (**TD3**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/Addressing Function Approximation Error in Actor-Critic Methods.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/PDF/Addressing Function Approximation Error in Actor-Critic Methods.pdf) | https://arxiv.org/abs/1802.09477                             |          |
|                    |                                                              |                                                              |          |
|                    | Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic (**Q-Prop**) | https://arxiv.org/abs/1611.02247                             |          |
|                    | Action-depedent Control Variates for Policy Optimization via Stein’s Identity (**Stein Control Variates**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/Action-depedent Control Variates for Policy Optimization via Stein’s Identity.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/PDF/Action-depedent Control Variates for Policy Optimization via Stein’s Identity.pdf) | https://arxiv.org/abs/1710.11198                             |          |
|                    | The Mirage of Action-Dependent Baselines in Reinforcement Learning [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/The Mirage of Action-Dependent Baselines in Reinforcement Learning.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/PDF/The Mirage of Action-Dependent Baselines in Reinforcement Learning.pdf) | https://arxiv.org/abs/1802.10031                             |          |
|                    | Bridging the Gap Between Value and Policy Based Reinforcement Learning (**PCL**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/Bridging the Gap Between Value and Policy Based Reinforcement Learning.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/PDF/Bridging the Gap Between Value and Policy Based Reinforcement Learning.pdf) | https://arxiv.org/abs/1702.08892                             |          |
| MaxEntropy RL      | Soft Q learning                                              | https://arxiv.org/abs/1702.08165                             |          |
|                    | Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor (**SAC**) [[Markdown\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/Soft Actor-Critic_Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.md) [[PDF\]](https://github.com/datawhalechina/easy-rl/blob/master/papers/Policy_gradient/PDF/Soft Actor-Critic_Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf) | https://arxiv.org/abs/1801.01290                             |          |
| Multi-Agent        | IQL                                                          | https://web.media.mit.edu/~cynthiab/Readings/tan-MAS-reinfLearn.pdf |          |
|                    | VDN                                                          | https://arxiv.org/abs/1706.05296                             |          |
|                    | QTRAN                                                        | http://proceedings.mlr.press/v97/son19a/son19a.pdf           |          |
|                    | QMIX                                                         | https://arxiv.org/abs/1803.11485                             |          |
|                    | Weighted QMIX                                                | https://arxiv.org/abs/2006.10800                             |          |
|                    | COMA                                                         | https://ojs.aaai.org/index.php/AAAI/article/download/11794/11653 |          |
|                    | MAPPO                                                        | https://arxiv.org/abs/2103.01955                             |          |
|                    | MADDPG                                                       |                                                              |          |
| Sparse reward      | Hierarchical DQN                                             | https://arxiv.org/abs/1604.06057                             |          |
|                    | ICM                                                          | https://arxiv.org/pdf/1705.05363.pdf                         |          |
|                    | HER                                                          | https://arxiv.org/pdf/1707.01495.pdf                         |          |
| Imitation Learning | GAIL                                                         | https://arxiv.org/abs/1606.03476                             |          |
|                    | TD3+BC                                                       | https://arxiv.org/pdf/2106.06860.pdf                         |          |
| Model based        | Dyna Q                                                       | https://arxiv.org/abs/1801.06176                             |          |





### AI整理的

#### 价值迭代类：

| 算法               | 论文名称                                                     | ArXiv 链接                                                   |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Q-Learning         | Watkins's Q-learning                                         | https://www.cs.rhul.ac.uk/~chrisw/new-q.pdf（无 arXiv，原始论文） |
| DQN                | Human-level control through deep reinforcement learning      | https://arxiv.org/abs/1312.5602                              |
| Double DQN         | Deep Reinforcement Learning with Double Q-learning           | https://arxiv.org/abs/1509.06461                             |
| Dueling DQN        | Dueling Network Architectures for Deep Reinforcement Learning | https://arxiv.org/abs/1511.06581                             |
| Prioritized Replay | Prioritized Experience Replay                                | https://arxiv.org/abs/1511.05952                             |
| Rainbow DQN        | Rainbow: Combining Improvements in Deep RL                   | https://arxiv.org/abs/1710.02298                             |
| QR-DQN             | Distributional Reinforcement Learning with Quantile Regression | https://arxiv.org/abs/1710.10044                             |

#### 策略迭代类：

| 算法      | 论文名称                                             | ArXiv 链接                                                   |
| --------- | ---------------------------------------------------- | ------------------------------------------------------------ |
| REINFORCE | Simple Statistical Gradient-Following Algorithms     | https://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf（无 arXiv） |
| TRPO      | Trust Region Policy Optimization                     | https://arxiv.org/abs/1502.05477                             |
| PPO       | Proximal Policy Optimization Algorithms              | https://arxiv.org/abs/1707.06347                             |
| A2C / A3C | Asynchronous Methods for Deep Reinforcement Learning | https://arxiv.org/abs/1602.01783                             |

#### AC类：

| 算法 | 论文名称                                                     | ArXiv 链接                       |
| ---- | ------------------------------------------------------------ | -------------------------------- |
| DDPG | Continuous control with deep reinforcement learning          | https://arxiv.org/abs/1509.02971 |
| TD3  | Addressing Function Approximation Error in Actor-Critic Methods | https://arxiv.org/abs/1802.09477 |
| SAC  | Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL        | https://arxiv.org/abs/1801.01290 |

#### 模仿学习/逆强化学习：

| 算法                  | 论文名称                                                     | ArXiv 链接                       |
| --------------------- | ------------------------------------------------------------ | -------------------------------- |
| Behavior Cloning (BC) | 无固定首发，经典总结见：A Survey of Demonstration Learning   | https://arxiv.org/abs/2303.11191 |
| DAgger                | A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning | https://arxiv.org/abs/1011.0686  |
| GAIL                  | Generative Adversarial Imitation Learning                    | https://arxiv.org/abs/1606.03476 |
| AIRL                  | Adversarial Inverse Reinforcement Learning                   | https://arxiv.org/abs/1710.11248 |
| BCQ                   | Batch-Constrained deep Q-learning                            | https://arxiv.org/abs/1812.02900 |

#### 离线学习：

| 算法   | 论文名称                                                     | ArXiv 链接                       |
| ------ | ------------------------------------------------------------ | -------------------------------- |
| BCQ    | Batch-Constrained deep Q-learning                            | https://arxiv.org/abs/1812.02900 |
| TD3+BC | Conservative Offline Deep Reinforcement Learning (含 TD3+BC baseline) | https://arxiv.org/abs/2106.06860 |
| CQL    | Conservative Q-Learning for Offline Reinforcement Learning   | https://arxiv.org/abs/2006.04779 |
| IQL    | Implicit Q-Learning for Offline RL                           | https://arxiv.org/abs/2110.06169 |

#### 层级强化学习

| 算法          | 论文名称                                             | ArXiv 链接                       |
| ------------- | ---------------------------------------------------- | -------------------------------- |
| h-DQN         | Hierarchical Deep Reinforcement Learning             | https://arxiv.org/abs/1604.06057 |
| HIRO          | Data-Efficient Hierarchical Reinforcement Learning   | https://arxiv.org/abs/1805.08296 |
| Option-Critic | The Option-Critic Architecture                       | https://arxiv.org/abs/1609.05140 |
| FuN           | FeUdal Networks for Hierarchical Reinforcement Learn | https://arxiv.org/abs/1703.01161 |
| HAC           | Learning Multi-Level Hierarchies with Hindsight      | https://arxiv.org/abs/1712.00948 |

#### RLHF相关：

| 算法               | 论文名称                                                     | ArXiv 链接                       |
| ------------------ | ------------------------------------------------------------ | -------------------------------- |
| PPO+KL（RLHF基础） | Learning to Summarize with Human Feedback                    | https://arxiv.org/abs/2009.01325 |
| DPO                | Direct Preference Optimization: Your Language Model is Secretly a Reward Model | https://arxiv.org/abs/2305.18290 |



| 序号 | 论文标题                                                     | 作者 / 会议 / 年份              | 关键贡献                                                  |
| ---- | ------------------------------------------------------------ | ------------------------------- | --------------------------------------------------------- |
| 1    | Simple Statistical Gradient-Following Algorithms for Connectionist RL (REINFORCE) | Ronald J. Williams / ML / 1992  | 提出最早的策略梯度方法，为后续基于策略的方法奠定基础      |
| 2    | Policy Gradient Methods for RL with Function Approximation(Actor-Critic) | Sutton et al. / NIPS / 2000     | 引入 baseline 减少梯度方差，提高策略梯度稳定性            |
| 3    | Playing Atari with Deep Reinforcement Learning (DQN)         | Mnih et al. / arXiv / 2013      | 首次使用深度网络成功训练智能体玩 Atari 游戏，深度RL的起点 |
| 4    | Trust Region Policy Optimization (TRPO)                      | Schulman et al. / ICML / 2015   | 提出收敛保障的策略优化方法，引入信赖域思想                |
| 5    | Generalized Advantage Estimation (GAE)                       | Schulman et al. / arXiv / 2015  | 通过优势函数平滑估计，提高策略梯度方法的样本效率          |
| 6    | Asynchronous Methods for Deep RL (A3C)                       | Mnih et al. / ICML / 2016       | 提出多线程并行学习策略，极大提升训练速度与稳定性          |
| 7    | Proximal Policy Optimization Algorithms (PPO)                | Schulman et al. / arXiv / 2017  | 主流策略优化算法，兼顾稳定性与实现简洁性，应用最广泛      |
| 8    | Soft Actor-Critic: Off-Policy Maximum Entropy RL             | Haarnoja et al. / ICML / 2018   | 最大熵策略框架，适用于连续动作空间，鲁棒且高效            |
| 9    | FeUdal Networks for Hierarchical RL (FuN)                    | Vezhnevets et al. / ICML / 2017 | 提出 manager-worker 架构，引入基于方向的子目标表示        |
| 10   | Unifying Count-Based Exploration and Intrinsic Motivation    | Bellemare et al. / NIPS / 2016  | 使用伪计数激励探索，解决稀疏奖励问题，促进主动学习        |