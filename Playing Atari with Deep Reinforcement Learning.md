### Playing Atari with Deep Reinforcement Learning

#### Abstract

提出了第一个深度学习模型，成功地使用强化学习从高维感官输入中直接学习控制策略。

这个模型是一个卷积神经网络模型，使用Q learning变种算法进行训练，输入是原始的像素信息，输出是未来奖励的估计。

我们对7个Atari游戏应用了该模型，在其中6款游戏上超出了以往所有方法，在三款游戏上超出了人类专家。

#### Introduction

直接使用高维感官输入来训练控制agent，例如视觉或者语音，是RL长期以来的挑战。大多数成功的RL都依赖手工的特征工程。这些系统的表现都强依赖特征表示的质量。

近期在深度学习方面的进展，使得直接从感官数据提取高维特征成为可能。自然而然的就会思考：这样的技术是否对需要处理原始数据的RL有帮助。

但是RL从深度学习的角度提出了好几个挑战：

1. 深度学习要求大量的手工标注的训练数据，RL必须从标量奖励中学习，这些信号通常都是稀疏、有噪声、延迟的。action和结果奖励之间的延迟是特别使人气馁的。
2. 深度学习通常假定样本之间是独立分布的，而RL面对的是强相关的数据序列。而且在RL中，当机器学会新的行为后，数据分布会发生变化，这对于假定底层数据分布是固定的深度学习来说是特别成问题的

该论文成功的使用CNN克服了上述挑战。为了缓解数据相关和变化的数据分布问题，我们使用了回放机制，随机采样之前的状态转换，对来自过去大量行为的训练数据的分布起到了平滑的作用。

Atari 2600是一个有挑战的RL测试环境，它给agent输入高维的视觉原始数据，并呈现对人类来说很有挑战的多样且有趣的游戏任务。我们的目标是用单一的CNN网络让agent学习玩尽量多的游戏。我们不给CNN网络提供任何游戏特有的信息或者人工设计的视觉特征，也不能访问到游戏模拟器内部的状态，提供给RL的只有视频输入、奖励信息、终止信号、动作集合，就像人类玩家感知到的一样。并且游戏过程中，网络架构和训练用的超参数保持不变。

#### Background

...此处省略一堆有关强化学习的术语解释和过程解释...

仅仅基于当下的屏幕画面来对游戏局势进行正确的理解不太可能，所以我们考虑输入 画面和动作交替的时间序列并基于该序列来学习游戏策略，这样就形成了马尔可夫决策过程，在这个过程中，每个序列就是一个明确的状态，这样我们就可以对MDP实施标准的RL方法。

...此处省略大量关于贝尔曼方程的解释，教材里讲得比较清楚，就不另外摘抄...

注意：在Q-learning中，target值依赖于要学习的网络参数seta，这个跟深度神经网络里通常的标注是不一样的，DL中标注在开始训练前就确定了。

#### Related Work

双陆棋是RL的一个成功案例，使用模型无关的近似Q-learning的RL算法，用包含一层隐藏层的多层感知机拟合出了状态价值函数。

但是随后在国际象棋、围棋、西洋棋中尝试模仿双陆棋进行RL，都没有那么成功。所以大家一度认为双陆棋是一个特例，双陆棋中掷色子的环节引入的随机有利于探索更多空间，让价值函数特别平滑。

而且，有显示把模型无关的RL与非线性函数拟合 组合在一起，通常导致Q-network发散不收敛。所以一段时间内，RL主要集中在线性函数的拟合，以保证更好的收敛。

最近，把深度神经网络和RL结合在一起的方法又有复兴。深度神经网络被用来估计环境。而且，Q-Learning算法发散的问题，已经被梯度时间差分算法部分的解决了。使用非线性拟合来评估一个固定的策略，或者当使用Q学习的受限变体学习具有线性函数逼近的控制策略时，被证明这些做法是收敛的。但这些方法都没有被扩展到非线性的控制中。

可能和我们最类似的先驱工作，是神经网络拟合Q-Learning（NFQ）。...这里省略大把介绍NFQ的文字...NFQ也已成功应用于使用纯视觉输入的简单现实世界控制任务。不同的是，我们的方法是端到端的应用RL。Q-Learning之前也有结合回放和简单神经网络的做法，但还是从低维的状态开始，而不是原始的视觉信息。

之前也有人用Atari 2600模拟器作为RL平台，通过使用大量的特征来改进结果，并使用拔河hash方法把特征投影到低维空间。例如HyperNEAT 。





