

**A SURVEY OF DEMONSTRATION LEARNING**

### 1、Introduction

 随着 AI 尤其是强化学习（RL）的发展，人们希望通过模仿人类行为训练智能系统。传统 RL 在现实中面临成本高、安全性差、需要大量交互的问题，这让从演示中学习（Demonstration Learning）成为一种更安全高效的替代方案。

传统编程方式无法适应高维或连续动作空间。而通过示范学习，智能体可以自动学习状态-动作的映射（policy），从而规避了人工指定每种情况的繁琐过程。

利用示范学习得到的动态模型（dynamics model）来“模拟”环境，从而生成新的状态转移序列（transitions），而不是真实地与环境交互。如果我们通过专家演示数据学到一个近似准确的环境模型，我们就可以在这个“模拟环境”中运行当前策略进行 rollout（策略执行），得到新的 transition 序列

RL 虽然能达到超人水平，但需要大量与环境交互，安全性差、样本效率低，因此不适合现实世界中如机器人和医疗的任务。

示范学习可利用专家数据跳过试错过程，安全高效地学习策略，并可结合其他方法进一步优化（如 RL、建模或 IRL），成为现实场景的可行方案。

示范学习依赖于专家数据的覆盖度，不足的多样性会导致泛化困难，遇到分布外状态时性能急剧下降，因此如何处理分布偏移是关键难题。

 示范数据的收集也不易，可能存在噪声、不一致或设备误差，因此学习方法需具有一定泛化能力，而非“死记硬背”。

 这一领域自上世纪八十年代起就被视为机器人学习的重要方向，并已广泛应用于各种机器人任务中，研究热度因深度学习而持续上升。

 已有多个综述试图统一该领域的术语与分类方法，但术语仍较混乱，因此本综述统一采用“demonstration learning”这个术语。

本文旨在从数据采集、学习方法、优化流程到评估基准全面综述该领域，并指出目前的挑战与未来研究方向。



![image-20250628125818359](img\image-20250628125818359.png)

### 2、Problem Definition

讲了RL和MDP，这部分已经比较熟悉，不再赘述

1. 行为克隆（Behavior Cloning）：将策略学习建模为监督学习问题，直接拟合专家在每个状态下的动作，适用于离散动作空间（分类）或连续动作空间（回归），但容易因分布外状态导致错误。
2. 逆向强化学习（Inverse Reinforcement Learning, IRL）：从演示中推断奖励函数，使智能体在训练中朝着“专家行为”的方向学习，而无需手动设计复杂的奖励函数。
3. 离线强化学习（Offline RL）：若演示中包含环境奖励，可以将其视作经验数据集，在不与环境交互的前提下，用 RL 方法训练出最大化累计奖励的策略，进而实现对演示策略的改进或泛化。
4. 即使演示中包含次优行为，Offline RL 也能通过估计价值函数选择高质量部分，从而可能超越原始演示策略，但这要求能正确区分好坏行为，是个挑战。
5. Offline RL 的目标是利用演示数据学习价值函数（state-value 或 action-value），再据此优化策略以最大化长期回报。
6. 值函数估计可以支持策略学习时更好地权衡长期收益，比行为克隆能更好应对次优演示和泛化问题。
7. 与监督学习不同，演示数据是时间序列并非独立同分布（non-i.i.d.），这加剧了“分布偏移”问题：策略一旦偏离演示分布，预测误差可能持续累积并放大。Offline RL 方法通常需通过不同机制约束策略偏离，降低泛化风险。

| 方法                         | 主要目标                                       | 数据需求                      | 是否需要奖励信息 | 优势                                             | 劣势                                                         |
| ---------------------------- | ---------------------------------------------- | ----------------------------- | ---------------- | ------------------------------------------------ | ------------------------------------------------------------ |
| 行为克隆（Behavior Cloning） | 模仿专家动作，学习状态到动作的映射关系         | 状态-动作对（不一定覆盖全面） | 否               | 简单高效，易于实现；训练过程快                   | 容易受分布外状态影响，泛化能力弱；无法识别和避免差的演示行为 |
| 逆向强化学习（IRL）          | 从专家行为推断奖励函数，再用 RL 优化策略       | 状态-动作序列                 | 否               | 可自动构建奖励函数，适用于无法手动设计奖励的任务 | 套路较复杂；可能产生多个解释（奖励不唯一）；效率不高         |
| 离线强化学习（Offline RL）   | 直接在静态数据集上进行策略优化以最大化奖励期望 | 状态-动作-奖励-下一状态四元组 | 是               | 可超越专家；能识别次优示范并改进；处理长期回报   | 分布外泛化风险大；学习稳定性依赖策略约束与价值函数估计方法   |

### 3、Demonstration Data Set

#### 3.4 Direct Demonstration（直接示范）
描述：示范由学习代理自身完成，不需要对状态或动作进行映射。

- 3.4.1 Teleoperation（远程操控）  
  人类通过控制器操纵学习代理完成任务，数据由代理内部传感器记录，直接可用。

- 3.4.2 Kinesthetic Teaching（动作示教）  
  教师通过身体接触引导机器人运动（如拉动机械臂关节），机器人用自身传感器记录演示。

  想象一个人亲手“抓住”机器人的手臂，把它移动到正确的位置完成一个任务。这个过程中机器人自身的传感器记录了关节角度、运动轨迹等信息——这些就构成了演示数据。也可以通过语音告诉机器人“现在抬起右臂30度”来进行类似示教。

- 3.4.3 Shadowing（影子模仿）  
  代理通过视觉或追踪传感器实时模仿教师动作，动作由代理执行但需要将教师行为映射为自身控制指令。

  这类似于“跟读”或者镜像模仿。比如一个机器人实时识别人类手势或步态，并让自己的身体同步模仿。这通常用于具有人形结构的机器人，例如双足行走或手臂动作训练。

---

#### 3.5 Indirect Demonstration（间接示范）
描述：示范由外部实体完成，通常需要将示范结果映射到学习代理能理解的状态和动作空间。

- 3.5.1 Observation（观察）  
  代理通过摄像头等外部传感器观看人类执行任务，仅获得视觉信息，需进一步推理动作。

- 3.5.2 Sensors on Teacher（教师佩戴传感器）  
  在教师身上安装传感器收集精确的动作数据，但仍需将这些数据适配到机器人的执行格式。

---

#### 3.6 Data Representation（数据表示方式）
描述：示范数据可以以不同形式进行存储和使用，影响后续学习质量。

- 3.6.1 Raw Features（原始特征）  
  直接使用传感器采集的数据，例如图像、位置信息、速度等，未经加工。

- 3.6.2 Manually Designed Features（手工特征）  
  专家根据任务经验手动提取关键特征，去除冗余，降低维度。

- 3.6.3 Extracted Features（自动特征提取）  
  使用神经网络等方法从原始输入中自动学习表示，适用于高维复杂输入如图像。

- 3.6.4 Time as a Feature（以时间作为特征）  
  假设任务的状态变化依赖于时间，每个时刻对应一个确定动作，省略了显式状态建模。

  这种做法适用于那些环境变化基本不受外部扰动、节奏严格同步的任务，比如工业流水线或机器人舞蹈。你可以把它理解为“到了第3秒就一定要执行动作A”，策略只要根据当前时间决定动作即可。这种方式简化了状态感知，但对环境的时间一致性要求高，稍有延迟或干扰就会导致失效。

---

#### 3.7 Data Set Limitations（数据集的限制）
描述：演示数据往往在覆盖度和质量上存在挑战，是示范学习的一大难题。

- 3.7.1 Incomplete Data（数据覆盖不完整）  
  示范无法涵盖所有可能状态，容易造成泛化失败或遇到分布外状态时出错。

- 3.7.2 Inadequate Data（数据质量不足）  
  示范可能包含噪声、错误、不一致或失败案例，容易误导策略学习，需进行筛选或修正。