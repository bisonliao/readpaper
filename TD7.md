**TD7 算法 (TD3 + 4 Additions)**

### 算法定位：采样效率与稳定性的新标杆

**TD7** (NeurIPS 2023) 并非一个从零开始的全新架构，而是在经典的 **TD3** 算法基础上，精妙地整合了四项针对性改进。它的核心成就证明了：即便是在**低级状态（Low-level states）**任务中，通过学习状态-动作的潜在动力学表征，也能显著提升强化学习的效率和稳定性。

\--------------------------------------------------------------------------------

### 重点解决的问题

• **学习信号弱**：传统 RL 使用非静态且近似的贝尔曼方程作为目标，导致学习信号比监督学习弱得多。

• **表征学习在物理控制中的缺失**：以往表征学习多用于视觉任务，作者认为物理控制任务的难度源于**动力学系统的复杂性**，而非观测空间的大小。

• **算法极度不稳定**：深度 RL 的策略性能在训练中常出现剧烈波动（Dips）。

• **高维输入引发的外推误差**：在引入高维嵌入（Embedding）时，在线 RL 也会出现离线 RL 中常见的“估值过高”问题，导致学习崩溃。

\--------------------------------------------------------------------------------

###  核心机制深度解析 (4 项关键改进)

#### **第一项：SALE (State-Action Learned Embeddings) —— 动力学表征学习**

**【直观理解】**：给算法装上一双“理解物理规律”的眼睛。它不只是生硬地看坐标和速度，而是通过预测“动作会对环境产生什么后果”来理解状态和动作的深层联系。

• **原理解析**：

  ◦ **潜空间建模**：使用编码器 *f*(*s*) 提取状态特征 *z**s*，编码器 *g*(*z**s*,*a*) 提取动作交互特征 *z**s**a*。其目标是让 *z**s**a* 能够预测下一个状态的嵌入表示 *z**s*′。

  ◦ **解耦训练 (Decoupled)**：编码器独立训练，不接收来自 Q 网络或策略网络的梯度。这防止了不稳定的强化学习信号“污染”了已经学好的表征。

  ◦ **AvgL1Norm 归一化**：通过将向量除以其平均绝对值，确保特征尺度恒定，既不会消失也不会爆炸。

  ◦ **价值裁剪 (Value Clipping)**：这是解决“外推误差”的关键。SALE 会跟踪训练集已有的 Q 值范围，强行将目标 Q 值限制在观测到的最大/最小值之间，防止 Q 网络对陌生动作产生盲目乐观的评估。

#### **第二项：Policy Checkpoints —— 策略检查点机制**

**【直观理解】**：给算法买了一份“后悔药”。RL 训练就像爬山，有时候会失足跌落深谷，检查点机制能让算法在“失足”后迅速回滚到之前爬到的最高峰。

• **原理解析**：

  ◦ **胜者为王**：定期固定策略进行多个回合的“考试”（Assessment Episodes）。如果当前策略的表现优于之前的最佳纪录，就存为一个“快照”。

  ◦ **最小化原则**：在评估时取多次试验的**最小值**而非平均值，旨在严厉惩罚那些不稳定的策略，只保留真正稳健的高性能模型。

  ◦ **测试回溯**：在最终评估时，使用表现最好的那个检查点策略，而不是可能已经发生退化的当前策略。

#### **第三项：LAP (Simplified Prioritized Experience Replay) —— 简化优先级回放**

**【直观理解】**：让算法“划重点”。它会自动识别哪些经验是由于自己“没见过”或“理解错”而导致误差大的，然后反复温习这些重点题目。

• **原理解析**：

  ◦ **高效采样**：根据 TD 误差（实际结果与预测的差距）来决定样本被抽取的概率。

  ◦ **鲁棒损失**：使用 **Huber Loss** 替代传统的 MSE，在处理具有极大偏差的异常样本时更加稳定，防止单个坏样本把整个网络带偏。

#### **第四项：Behavior Cloning (BC) Term —— 行为克隆项**

**【直观理解】**：给算法划定“安全区”。在没有老师（环境反馈）的情况下，提醒自己不要做出太离谱、偏离已有经验太远的动作。

• **原理解析**：

  ◦ **离线正则化**：主要用于**离线强化学习**。在更新策略时增加一个约束，让学到的策略不要偏离离线数据集中的行为太远。

  ◦ **通用性**：在线模式下该项权重为 0。这使得 TD7 成为一个通用算法，同一套逻辑既能跑在线交互，也能跑离线数据。

\--------------------------------------------------------------------------------

### 实验效果统计

• **采样效率**：在 OpenAI Gym 基准任务中，TD7 在 **300k 步**时的平均性能甚至超过了 TD3 在 **5M 步**后的最终表现。

• **性能提升幅度**：相比 TD3，在训练早期（300k步）性能提升了 **276.7%**，在训练后期（5M步）提升了 **50.7%**。

• **全方位碾压**：在 MuJoCo 任务中，TD7 显著优于 SAC、TQC 等流行算法；在 D4RL 离线任务中也超越了 CQL 和 IQL 等专业离线算法。

• **计算开销**：虽然增加了编码器，但由于不需要像 TQC 那样使用庞大的集成网络（Ensembles），TD7 的运行速度非常快（如在 HalfCheetah 任务中比 TQC 快一倍以上）。

\--------------------------------------------------------------------------------

### 结论与启发

TD7 成功的核心在于**解耦**：将表征学习与任务学习解耦（SALE），将评估与更新解耦（Checkpoints）。这告诉我们，在处理复杂动力学系统时，增强算法对物理规律的理解力和对自身稳定性的管控力，比单纯增加网络深度或样本数量更有效。